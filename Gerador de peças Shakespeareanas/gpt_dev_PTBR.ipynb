{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "## Construindo um GPT\n",
    "\n",
    "Notebook original por Andrej Karpathy mostrado em [Zero To Hero](https://karpathy.ai/zero-to-hero.html).\n",
    "Adaptado em português por Lucas Parteka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "O6medjfRsLD9",
    "outputId": "ceb16386-d6fe-4dad-abac-bbefe1237666"
   },
   "outputs": [],
   "source": [
    "# Vamos lê-lo e inspecioná-lo\n",
    "with open('entrada.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xWI_VyAsN8F",
    "outputId": "1148d388-ffe9-4c4a-a272-2c5056dcb7a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantidade de caracteres no dataset:  690812\n"
     ]
    }
   ],
   "source": [
    "print(\"quantidade de caracteres no dataset: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pg9IvLwhM6qL"
   },
   "source": [
    "Em inglês, o dataset tinha 1115394 caracteres, foram usados como dataset trechos de obras do Shakespeare em português."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c5V0FvqseE0",
    "outputId": "ecc87331-07fc-4cbc-933b-589412beea52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SANSÃO \n",
      "Por minha palavra, Gregório: não devemos levar desaforo para casa. \n",
      " \n",
      "GREGÓRIO \n",
      "É certo; para não ficarmos desaforados. \n",
      " \n",
      "SANSÃO \n",
      "O que quero dizer é que quando eu fico encolerizado puxo logo da \n",
      "espada. \n",
      " \n",
      "GREGÓRIO \n",
      "Sim, mas se quiseres viver, toma cuidado para não ficares \n",
      "encolarinhado. \n",
      " \n",
      "SANSÃO \n",
      "Quando me irritam, eu ataco prontamente. \n",
      " \n",
      "GREGÓRIO  \n",
      " Mas não te irritas prontamente para atacar. \n",
      " \n",
      "SANSÃO \n",
      "Até um cachorro da casa dos Montecchios me deixa irritado. \n",
      " \n",
      "GREGÓRIO \n",
      "Ficar irritado é pôr-se em movimento, e ser valente é estacar. Logo, \n",
      "se ficares irritado, pôr-te-ás a correr. \n",
      " \n",
      "SANSÃO \n",
      "Um cachorro daquela casa me fará fazer pé firme. Encostar-me-ei na \n",
      "parede contra qualquer homem ou rapariga da casa de Montecchio. \n",
      " \n",
      "GREGÓRIO \n",
      "Isso prova que não passas de um escravo fraco, porque o mais fraco \n",
      "é que se encosta à parede. \n",
      " \n",
      "SANSÃO \n",
      "É certo; é por isso que as mulheres, como vasilhas mais fracas, são \n",
      "sempre encostadas à parede. Por isso, afastarei da parede os hom\n"
     ]
    }
   ],
   "source": [
    "# Vamos ver os primeiros 10000 caracteres\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e-Rbyr8sfM8",
    "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"&'(),-.:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz©ÀÁÂÃÇÉÊÍÓÔÚàáâãçéêíóôõúü—’“”\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "# aqui os caracteres unicos que ocorrem neste texto\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detalhe, apareceram mais caracteres que em ingles que eram 65. A língua portuguesa tem acentuação e letras específicas como \"Ç\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yw1LKNCgwjj1",
    "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 1, 40, 85]\n",
      "E aí\n"
     ]
    }
   ],
   "source": [
    "# cria um mapeamento de caracteres em inteiros\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: pega uma string, retorna uma lista de integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: pega uma lista de inteiros, retorna uma string\n",
    "\n",
    "print(encode(\"E aí\"))\n",
    "print(decode(encode(\"E aí\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJb0OXPwzvqg",
    "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([690812]) torch.int64\n",
      "tensor([32, 14, 27, 32, 70, 28,  1,  0, 29, 54, 57,  1, 52, 48, 53, 47, 40,  1,\n",
      "        55, 40, 51, 40, 61, 57, 40,  8,  1, 20, 57, 44, 46, 86, 57, 48, 54, 11,\n",
      "         1, 53, 81, 54,  1, 43, 44, 61, 44, 52, 54, 58,  1, 51, 44, 61, 40, 57,\n",
      "         1, 43, 44, 58, 40, 45, 54, 57, 54,  1, 55, 40, 57, 40,  1, 42, 40, 58,\n",
      "        40, 10,  1,  0,  1,  0, 20, 31, 18, 20, 75, 31, 22, 28,  1,  0, 72,  1,\n",
      "        42, 44, 57, 59, 54, 12,  1, 55, 40, 57, 40,  1, 53, 81, 54,  1, 45, 48,\n",
      "        42, 40, 57, 52, 54, 58,  1, 43, 44, 58, 40, 45, 54, 57, 40, 43, 54, 58,\n",
      "        10,  1,  0,  1,  0, 32, 14, 27, 32, 70, 28,  1,  0, 28,  1, 56, 60, 44,\n",
      "         1, 56, 60, 44, 57, 54,  1, 43, 48, 65, 44, 57,  1, 83,  1, 56, 60, 44,\n",
      "         1, 56, 60, 40, 53, 43, 54,  1, 44, 60,  1, 45, 48, 42, 54,  1, 44, 53,\n",
      "        42, 54, 51, 44, 57, 48, 65, 40, 43, 54,  1, 55, 60, 63, 54,  1, 51, 54,\n",
      "        46, 54,  1, 43, 40,  1,  0, 44, 58, 55, 40, 43, 40, 10,  1,  0,  1,  0,\n",
      "        20, 31, 18, 20, 75, 31, 22, 28,  1,  0, 32, 48, 52,  8,  1, 52, 40, 58,\n",
      "         1, 58, 44,  1, 56, 60, 48, 58, 44, 57, 44, 58,  1, 61, 48, 61, 44, 57,\n",
      "         8,  1, 59, 54, 52, 40,  1, 42, 60, 48, 43, 40, 43, 54,  1, 55, 40, 57,\n",
      "        40,  1, 53, 81, 54,  1, 45, 48, 42, 40, 57, 44, 58,  1,  0, 44, 53, 42,\n",
      "        54, 51, 40, 57, 48, 53, 47, 40, 43, 54, 10,  1,  0,  1,  0, 32, 14, 27,\n",
      "        32, 70, 28,  1,  0, 30, 60, 40, 53, 43, 54,  1, 52, 44,  1, 48, 57, 57,\n",
      "        48, 59, 40, 52,  8,  1, 44, 60,  1, 40, 59, 40, 42, 54,  1, 55, 57, 54,\n",
      "        53, 59, 40, 52, 44, 53, 59, 44, 10,  1,  0,  1,  0, 20, 31, 18, 20, 75,\n",
      "        31, 22, 28,  1,  1,  0,  1, 26, 40, 58,  1, 53, 81, 54,  1, 59, 44,  1,\n",
      "        48, 57, 57, 48, 59, 40, 58,  1, 55, 57, 54, 53, 59, 40, 52, 44, 53, 59,\n",
      "        44,  1, 55, 40, 57, 40,  1, 40, 59, 40, 42, 40, 57, 10,  1,  0,  1,  0,\n",
      "        32, 14, 27, 32, 70, 28,  1,  0, 14, 59, 83,  1, 60, 52,  1, 42, 40, 42,\n",
      "        47, 54, 57, 57, 54,  1, 43, 40,  1, 42, 40, 58, 40,  1, 43, 54, 58,  1,\n",
      "        26, 54, 53, 59, 44, 42, 42, 47, 48, 54, 58,  1, 52, 44,  1, 43, 44, 48,\n",
      "        63, 40,  1, 48, 57, 57, 48, 59, 40, 43, 54, 10,  1,  0,  1,  0, 20, 31,\n",
      "        18, 20, 75, 31, 22, 28,  1,  0, 19, 48, 42, 40, 57,  1, 48, 57, 57, 48,\n",
      "        59, 40, 43, 54,  1, 83,  1, 55, 87, 57,  9, 58, 44,  1, 44, 52,  1, 52,\n",
      "        54, 61, 48, 52, 44, 53, 59, 54,  8,  1, 44,  1, 58, 44, 57,  1, 61, 40,\n",
      "        51, 44, 53, 59, 44,  1, 83,  1, 44, 58, 59, 40, 42, 40, 57, 10,  1, 25,\n",
      "        54, 46, 54,  8,  1,  0, 58, 44,  1, 45, 48, 42, 40, 57, 44, 58,  1, 48,\n",
      "        57, 57, 48, 59, 40, 43, 54,  8,  1, 55, 87, 57,  9, 59, 44,  9, 79, 58,\n",
      "         1, 40,  1, 42, 54, 57, 57, 44, 57, 10,  1,  0,  1,  0, 32, 14, 27, 32,\n",
      "        70, 28,  1,  0, 34, 52,  1, 42, 40, 42, 47, 54, 57, 57, 54,  1, 43, 40,\n",
      "        56, 60, 44, 51, 40,  1, 42, 40, 58, 40,  1, 52, 44,  1, 45, 40, 57, 79,\n",
      "         1, 45, 40, 65, 44, 57,  1, 55, 83,  1, 45, 48, 57, 52, 44, 10,  1, 18,\n",
      "        53, 42, 54, 58, 59, 40, 57,  9, 52, 44,  9, 44, 48,  1, 53, 40,  1,  0,\n",
      "        55, 40, 57, 44, 43, 44,  1, 42, 54, 53, 59, 57, 40,  1, 56, 60, 40, 51,\n",
      "        56, 60, 44, 57,  1, 47, 54, 52, 44, 52,  1, 54, 60,  1, 57, 40, 55, 40,\n",
      "        57, 48, 46, 40,  1, 43, 40,  1, 42, 40, 58, 40,  1, 43, 44,  1, 26, 54,\n",
      "        53, 59, 44, 42, 42, 47, 48, 54, 10,  1,  0,  1,  0, 20, 31, 18, 20, 75,\n",
      "        31, 22, 28,  1,  0, 22, 58, 58, 54,  1, 55, 57, 54, 61, 40,  1, 56, 60,\n",
      "        44,  1, 53, 81, 54,  1, 55, 40, 58, 58, 40, 58,  1, 43, 44,  1, 60, 52,\n",
      "         1, 44, 58, 42, 57, 40, 61, 54,  1, 45, 57, 40, 42, 54,  8,  1, 55, 54,\n",
      "        57, 56, 60, 44,  1, 54,  1, 52, 40, 48, 58,  1, 45, 57, 40, 42, 54,  1,\n",
      "         0, 83,  1, 56, 60, 44,  1, 58, 44,  1, 44, 53, 42, 54, 58, 59, 40,  1,\n",
      "        78,  1, 55, 40, 57, 44, 43, 44, 10,  1,  0,  1,  0, 32, 14, 27, 32, 70,\n",
      "        28,  1,  0, 72,  1, 42, 44, 57, 59, 54, 12,  1, 83,  1, 55, 54, 57,  1,\n",
      "        48, 58, 58, 54,  1, 56, 60, 44,  1, 40, 58,  1, 52, 60, 51, 47, 44, 57,\n",
      "        44, 58,  8,  1, 42, 54, 52, 54,  1, 61, 40, 58, 48, 51, 47, 40, 58,  1,\n",
      "        52, 40, 48, 58,  1, 45, 57, 40, 42, 40, 58,  8,  1, 58, 81, 54,  1,  0,\n",
      "        58, 44, 52, 55, 57, 44,  1, 44, 53, 42, 54, 58, 59, 40, 43, 40, 58,  1,\n",
      "        78,  1, 55, 40, 57, 44, 43, 44, 10,  1, 29, 54, 57,  1, 48, 58, 58, 54,\n",
      "         8,  1, 40, 45, 40, 58, 59, 40, 57, 44, 48,  1, 43, 40,  1, 55, 40, 57,\n",
      "        44, 43, 44,  1, 54, 58,  1, 47, 54, 52])\n"
     ]
    }
   ],
   "source": [
    "# vamos encodificar todo o dataset de texto e salvar num torch.tensor\n",
    "import torch # Foi usado PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # Os 1000 caracteres vistos acima encodificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "f_WIXqxz0lU5"
   },
   "outputs": [],
   "source": [
    "# Vamos dividir os dados entre treino  e teste\n",
    "n = int(0.9*len(data)) # primeiros 90% serão treino, resto validação\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD5Bj8Y6IAD4",
    "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32, 14, 27, 32, 70, 28,  1,  0, 29])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HXDe8vGJCEn",
    "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quando a entrada é tensor([32]) o alvo: 14\n",
      "quando a entrada é tensor([32, 14]) o alvo: 27\n",
      "quando a entrada é tensor([32, 14, 27]) o alvo: 32\n",
      "quando a entrada é tensor([32, 14, 27, 32]) o alvo: 70\n",
      "quando a entrada é tensor([32, 14, 27, 32, 70]) o alvo: 28\n",
      "quando a entrada é tensor([32, 14, 27, 32, 70, 28]) o alvo: 1\n",
      "quando a entrada é tensor([32, 14, 27, 32, 70, 28,  1]) o alvo: 0\n",
      "quando a entrada é tensor([32, 14, 27, 32, 70, 28,  1,  0]) o alvo: 29\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"quando a entrada é {context} o alvo: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3k1Czf7LuA9",
    "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entradas:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1, 55, 40, 57, 59, 44,  1, 54],\n",
      "        [57, 50,  8,  1, 44,  1, 59, 54],\n",
      "        [ 1, 40,  1, 44, 58, 59, 44,  1],\n",
      "        [52,  1, 44, 51, 44,  1,  0, 16]])\n",
      "alvos:\n",
      "torch.Size([4, 8])\n",
      "tensor([[55, 40, 57, 59, 44,  1, 54, 60],\n",
      "        [50,  8,  1, 44,  1, 59, 54, 43],\n",
      "        [40,  1, 44, 58, 59, 44,  1, 51],\n",
      "        [ 1, 44, 51, 44,  1,  0, 16, 54]])\n",
      "----\n",
      "quando a entrada é [1] o alvo: 55\n",
      "quando a entrada é [1, 55] o alvo: 40\n",
      "quando a entrada é [1, 55, 40] o alvo: 57\n",
      "quando a entrada é [1, 55, 40, 57] o alvo: 59\n",
      "quando a entrada é [1, 55, 40, 57, 59] o alvo: 44\n",
      "quando a entrada é [1, 55, 40, 57, 59, 44] o alvo: 1\n",
      "quando a entrada é [1, 55, 40, 57, 59, 44, 1] o alvo: 54\n",
      "quando a entrada é [1, 55, 40, 57, 59, 44, 1, 54] o alvo: 60\n",
      "quando a entrada é [57] o alvo: 50\n",
      "quando a entrada é [57, 50] o alvo: 8\n",
      "quando a entrada é [57, 50, 8] o alvo: 1\n",
      "quando a entrada é [57, 50, 8, 1] o alvo: 44\n",
      "quando a entrada é [57, 50, 8, 1, 44] o alvo: 1\n",
      "quando a entrada é [57, 50, 8, 1, 44, 1] o alvo: 59\n",
      "quando a entrada é [57, 50, 8, 1, 44, 1, 59] o alvo: 54\n",
      "quando a entrada é [57, 50, 8, 1, 44, 1, 59, 54] o alvo: 43\n",
      "quando a entrada é [1] o alvo: 40\n",
      "quando a entrada é [1, 40] o alvo: 1\n",
      "quando a entrada é [1, 40, 1] o alvo: 44\n",
      "quando a entrada é [1, 40, 1, 44] o alvo: 58\n",
      "quando a entrada é [1, 40, 1, 44, 58] o alvo: 59\n",
      "quando a entrada é [1, 40, 1, 44, 58, 59] o alvo: 44\n",
      "quando a entrada é [1, 40, 1, 44, 58, 59, 44] o alvo: 1\n",
      "quando a entrada é [1, 40, 1, 44, 58, 59, 44, 1] o alvo: 51\n",
      "quando a entrada é [52] o alvo: 1\n",
      "quando a entrada é [52, 1] o alvo: 44\n",
      "quando a entrada é [52, 1, 44] o alvo: 51\n",
      "quando a entrada é [52, 1, 44, 51] o alvo: 44\n",
      "quando a entrada é [52, 1, 44, 51, 44] o alvo: 1\n",
      "quando a entrada é [52, 1, 44, 51, 44, 1] o alvo: 0\n",
      "quando a entrada é [52, 1, 44, 51, 44, 1, 0] o alvo: 16\n",
      "quando a entrada é [52, 1, 44, 51, 44, 1, 0, 16] o alvo: 54\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # Quantas sequencias independentes serão processadas em paralelo?\n",
    "block_size = 8 # Qual é o tamanho máximo de contexto para as predições?\n",
    "\n",
    "def get_batch(split):\n",
    "    # gera um pequeno lote de dados de entrada x e alvos y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('entradas:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('alvos:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # tamanho do lote amostral \n",
    "    for t in range(block_size): # dimensão temporal\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"quando a entrada é {context.tolist()} o alvo: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpyyAeIzQjlO",
    "outputId": "a650f8dc-da81-400b-bc59-0a595487fdb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 55, 40, 57, 59, 44,  1, 54],\n",
      "        [57, 50,  8,  1, 44,  1, 59, 54],\n",
      "        [ 1, 40,  1, 44, 58, 59, 44,  1],\n",
      "        [52,  1, 44, 51, 44,  1,  0, 16]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # nossa entrada para o transformador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nql_1ER53oCf",
    "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 95])\n",
      "tensor(4.9322, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "RsIDIQtQ.?GzN ;âBúuÂÊ-v!LtrYZ\"Áe&bÓÚI k!ÃtkHÔ’ôôãIdHigÉ(eãf—àT,©wDy©Yz'âõââà\"b)\n",
      "FÁ—Ô?Í—;pÊL&ÃTWBNeYE\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # cada token diretamente lê fora das logits para o próximo token de uma tabela de pesquisa\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx e os targets são ambos (B,T) tensors de integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx é vetor (B, T)  de indices no contexto atual \n",
    "        for _ in range(max_new_tokens):\n",
    "            # têm as predições\n",
    "            logits, loss = self(idx)\n",
    "            # foca apenas no ultimo time step\n",
    "            logits = logits[:, -1, :] # vira (B, C)\n",
    "            # aplica softmax para ter probabilidades\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # amostra da distribuição\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append índices amostrados para a sequência que está rodando\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eTyJ8qAaDdiF"
   },
   "outputs": [],
   "source": [
    "# cria um otimizador PyTorch\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs4kI8YdEkQj",
    "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.821751594543457\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100): # aumenta o número de passos para bons resultados...\n",
    "\n",
    "    # amostra um lote de dados\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # avalia a perda\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcVIDWAZEtjN",
    "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Í©pÔÉ ?àzájÍtDühÀzt&'EÂÀÂOoóÚWPqc h(õÚÇÉYÉalcW-õdq.cSÓiilâZnWÊÔáÉTh:OP”GjÔlt!AÂmÃh’Zq tQÍÚIêwÉ;XSõ;HkÉÇUâAmYYà©Ãtêvõ'üLCêôÍHióànçUzú”jBhÓ©ÍnFmüWÉÂTéchXDHÓ“IdD,À—(LSúDCéfIF\n",
      "oüQXrjQRVOÔá'ÔNNô AAçmYÇ:kRéL,h(àXalçiÔ!RgóE“ãPóy\"AõóV“?u(eQ“IÁÂhXgR)ú”Uç &—wxíMóO—zqêCUàYtTVOçiÍíT,á'-ãÁL\n",
      "wç’âbWF&Aõá:wqUz?RU;vÀUzÓÃÂmlé.é©Óêq F Ué©Q'rí©VOsuNLgvüÉk!elamÂmN:?WakHÇ-ôÔq-àMÀWQuSBpRfI\"V'’-Rl-rwjnçUJtêr\n",
      "Q.?IS-ÍbibYÀLgr’SI\n",
      "NVÃupLj?'vz?ÉX©ÁTksRF)ÂWÉOÔHo.ÊÔ! AT\n",
      "D'rq)\n",
      "ôÇséÂNôCw ÁTQz,É ÀQbSaiàBk’E-Dúq.!lÃTãhyGD?é s\"É:â\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XinV8nmAnmKN"
   },
   "source": [
    "## Truque matemático self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tukiH-NbRBhA",
    "outputId": "d981f6d4-ac08-4ec2-8284-82f5fa1e0815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# exemplo ilustrando como multiplicações matriciais podem ser usadas para uma \"agregação baseada nos pesos (weighted aggregation)\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs_E24uRE8kr",
    "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# considere o seguinte exemplo:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, tempo, canais\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "86NuXX0fn7ps"
   },
   "outputs": [],
   "source": [
    "# queremos x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhdOAd6-wXkZ",
    "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# versão 2: usando multiplicação matricial para uma agregação baseada em pesos\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOURrfG-ysoL",
    "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# versão 3: usa Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDarxEWIRMKq",
    "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# versão 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, tempo, canais\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# vamos ver como performa um self-attention de única cabeça\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vT1hdtzXCjgL",
    "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5CvobiQ0pLr"
   },
   "source": [
    "Observação:\n",
    "-  Attention é um **mecanismo de comunicação**. Podem ser visto como nodos em um grafo direcionado apontando um para o outro e agregando informações com uma soma ponderada de todos nodos que apontam para eles, com pesos dependentes dos dados.\n",
    "- Não há noção de espaço. Attention simplesmente age sobre um conjunto de vetores. É por isso que precisamos codificar posicionalmente os tokens.\n",
    "- Cada exemplo entre a dimensão do batch é claramente processado completamente independente e nunca \"conversa\" com os demais\n",
    "- Em um \"encoder\", um bloco de attention apenas deleta a única linha que mascara com `tril`, permitindo todos os tokens se comunicarem. Este bloco aqui é chamado de bloco attention \"decoder\" pois ele tem mascaramento triangular, e é geralmete usado em configurações autoregressivas como modelagem linguística.\n",
    "- \"self-attention\" apenas significa que as chaves e valores são produzidas da mesma fonte que as consultas. Em \"cross-attention\", as consultas ainda são produzidas do x, porém as chaves e valores vem de alguma outra fonte externa (ex. um módulo encoder)\n",
    "- \"Scaled\" attention adicionamente divide `wei` por 1/sqrt(head_size). Isso faz quie quando as entradas Q,K são unidades de variância, wei pode ser unidade de variância também e Softmax permanecerá difuso e não saturará muito. Ilustração abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "4SNbLq5z3oBw"
   },
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nl6I9n9IRTSo",
    "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1tQx7oeRvtc",
    "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLb_odHU3iKM",
    "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0918)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JB82yzt44REI",
    "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mpt8569BB9_f",
    "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # fica muito pico, converge para one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Num7sX9CKOH",
    "outputId": "929ceb78-a639-41d6-aac7-12997b5c93f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variancia\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 de 100 vetores dimensionais\n",
    "x = module(x)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "633T2cmnW1uk",
    "outputId": "7720fa58-0478-4e8a-86a7-502d4cce9443"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean,std de uma característica entre todas entradas do batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LN9cK9BoXCYb",
    "outputId": "6368ece0-600e-417d-8a91-7c1e5d750ba8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.5367e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:].mean(), x[0,:].std() # mean,std of uma única entrada do batch, de suas características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "dRJH6wM_XFfU"
   },
   "outputs": [],
   "source": [
    "# Exemplo de tradução de francês para português:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les réseaux de neurones sont géniaux! <START> as redes neurais são geniais!<END>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcvKeBXoZFOY"
   },
   "source": [
    "### Código finalizado para referência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.213599 M parametros\n",
      "iteração 0: perda do treino 4.7213, perda da validação 4.7206\n",
      "iteração 100: perda do treino 2.7223, perda da validação 2.7430\n",
      "iteração 200: perda do treino 2.5302, perda da validação 2.5608\n",
      "iteração 300: perda do treino 2.4519, perda da validação 2.4609\n",
      "iteração 400: perda do treino 2.3768, perda da validação 2.3939\n",
      "iteração 500: perda do treino 2.3222, perda da validação 2.3299\n",
      "iteração 600: perda do treino 2.2792, perda da validação 2.2819\n",
      "iteração 700: perda do treino 2.2375, perda da validação 2.2374\n",
      "iteração 800: perda do treino 2.2077, perda da validação 2.2171\n",
      "iteração 900: perda do treino 2.1537, perda da validação 2.1607\n",
      "iteração 1000: perda do treino 2.1335, perda da validação 2.1215\n",
      "iteração 1100: perda do treino 2.0962, perda da validação 2.1093\n",
      "iteração 1200: perda do treino 2.0895, perda da validação 2.0677\n",
      "iteração 1300: perda do treino 2.0407, perda da validação 2.0364\n",
      "iteração 1400: perda do treino 2.0268, perda da validação 2.0111\n",
      "iteração 1500: perda do treino 2.0085, perda da validação 2.0024\n",
      "iteração 1600: perda do treino 1.9919, perda da validação 1.9987\n",
      "iteração 1700: perda do treino 1.9684, perda da validação 1.9670\n",
      "iteração 1800: perda do treino 1.9410, perda da validação 1.9522\n",
      "iteração 1900: perda do treino 1.9362, perda da validação 1.9362\n",
      "iteração 2000: perda do treino 1.9231, perda da validação 1.9308\n",
      "iteração 2100: perda do treino 1.8987, perda da validação 1.8964\n",
      "iteração 2200: perda do treino 1.8856, perda da validação 1.8788\n",
      "iteração 2300: perda do treino 1.8757, perda da validação 1.8814\n",
      "iteração 2400: perda do treino 1.8663, perda da validação 1.8639\n",
      "iteração 2500: perda do treino 1.8480, perda da validação 1.8446\n",
      "iteração 2600: perda do treino 1.8279, perda da validação 1.8394\n",
      "iteração 2700: perda do treino 1.8299, perda da validação 1.8255\n",
      "iteração 2800: perda do treino 1.8183, perda da validação 1.8148\n",
      "iteração 2900: perda do treino 1.8089, perda da validação 1.8248\n",
      "iteração 3000: perda do treino 1.7939, perda da validação 1.8031\n",
      "iteração 3100: perda do treino 1.7916, perda da validação 1.7996\n",
      "iteração 3200: perda do treino 1.7795, perda da validação 1.8071\n",
      "iteração 3300: perda do treino 1.7689, perda da validação 1.7844\n",
      "iteração 3400: perda do treino 1.7705, perda da validação 1.7861\n",
      "iteração 3500: perda do treino 1.7658, perda da validação 1.7718\n",
      "iteração 3600: perda do treino 1.7596, perda da validação 1.7682\n",
      "iteração 3700: perda do treino 1.7483, perda da validação 1.7526\n",
      "iteração 3800: perda do treino 1.7293, perda da validação 1.7472\n",
      "iteração 3900: perda do treino 1.7307, perda da validação 1.7468\n",
      "iteração 4000: perda do treino 1.7247, perda da validação 1.7497\n",
      "iteração 4100: perda do treino 1.7307, perda da validação 1.7474\n",
      "iteração 4200: perda do treino 1.7290, perda da validação 1.7366\n",
      "iteração 4300: perda do treino 1.7085, perda da validação 1.7383\n",
      "iteração 4400: perda do treino 1.6906, perda da validação 1.7182\n",
      "iteração 4500: perda do treino 1.6999, perda da validação 1.7317\n",
      "iteração 4600: perda do treino 1.7001, perda da validação 1.7413\n",
      "iteração 4700: perda do treino 1.7027, perda da validação 1.7169\n",
      "iteração 4800: perda do treino 1.6998, perda da validação 1.7074\n",
      "iteração 4900: perda do treino 1.6821, perda da validação 1.7052\n",
      "iteração 5000: perda do treino 1.6776, perda da validação 1.7076\n",
      "iteração 5100: perda do treino 1.6766, perda da validação 1.7038\n",
      "iteração 5200: perda do treino 1.6753, perda da validação 1.6952\n",
      "iteração 5300: perda do treino 1.6613, perda da validação 1.7049\n",
      "iteração 5400: perda do treino 1.6642, perda da validação 1.6932\n",
      "iteração 5500: perda do treino 1.6681, perda da validação 1.6933\n",
      "iteração 5600: perda do treino 1.6617, perda da validação 1.6827\n",
      "iteração 5700: perda do treino 1.6570, perda da validação 1.6946\n",
      "iteração 5800: perda do treino 1.6571, perda da validação 1.6833\n",
      "iteração 5900: perda do treino 1.6501, perda da validação 1.6861\n",
      "iteração 6000: perda do treino 1.6431, perda da validação 1.6794\n",
      "iteração 6100: perda do treino 1.6538, perda da validação 1.6776\n",
      "iteração 6200: perda do treino 1.6434, perda da validação 1.6807\n",
      "iteração 6300: perda do treino 1.6385, perda da validação 1.6741\n",
      "iteração 6400: perda do treino 1.6335, perda da validação 1.6730\n",
      "iteração 6500: perda do treino 1.6368, perda da validação 1.6826\n",
      "iteração 6600: perda do treino 1.6430, perda da validação 1.6829\n",
      "iteração 6700: perda do treino 1.6364, perda da validação 1.6709\n",
      "iteração 6800: perda do treino 1.6196, perda da validação 1.6697\n",
      "iteração 6900: perda do treino 1.6274, perda da validação 1.6741\n",
      "iteração 7000: perda do treino 1.6267, perda da validação 1.6786\n",
      "iteração 7100: perda do treino 1.6083, perda da validação 1.6488\n",
      "iteração 7200: perda do treino 1.6236, perda da validação 1.6655\n",
      "iteração 7300: perda do treino 1.6257, perda da validação 1.6638\n",
      "iteração 7400: perda do treino 1.6133, perda da validação 1.6407\n",
      "iteração 7500: perda do treino 1.6047, perda da validação 1.6433\n",
      "iteração 7600: perda do treino 1.6099, perda da validação 1.6569\n",
      "iteração 7700: perda do treino 1.6153, perda da validação 1.6579\n",
      "iteração 7800: perda do treino 1.6024, perda da validação 1.6508\n",
      "iteração 7900: perda do treino 1.6021, perda da validação 1.6389\n",
      "iteração 8000: perda do treino 1.6151, perda da validação 1.6633\n",
      "iteração 8100: perda do treino 1.5975, perda da validação 1.6326\n",
      "iteração 8200: perda do treino 1.5843, perda da validação 1.6485\n",
      "iteração 8300: perda do treino 1.5864, perda da validação 1.6263\n",
      "iteração 8400: perda do treino 1.6022, perda da validação 1.6346\n",
      "iteração 8500: perda do treino 1.5975, perda da validação 1.6353\n",
      "iteração 8600: perda do treino 1.5902, perda da validação 1.6247\n",
      "iteração 8700: perda do treino 1.5763, perda da validação 1.6255\n",
      "iteração 8800: perda do treino 1.5927, perda da validação 1.6280\n",
      "iteração 8900: perda do treino 1.5792, perda da validação 1.6328\n",
      "iteração 9000: perda do treino 1.5795, perda da validação 1.6293\n",
      "iteração 9100: perda do treino 1.5870, perda da validação 1.6332\n",
      "iteração 9200: perda do treino 1.5763, perda da validação 1.6429\n",
      "iteração 9300: perda do treino 1.5786, perda da validação 1.6186\n",
      "iteração 9400: perda do treino 1.5822, perda da validação 1.6318\n",
      "iteração 9500: perda do treino 1.5775, perda da validação 1.6336\n",
      "iteração 9600: perda do treino 1.5680, perda da validação 1.6187\n",
      "iteração 9700: perda do treino 1.5670, perda da validação 1.6138\n",
      "iteração 9800: perda do treino 1.5765, perda da validação 1.6064\n",
      "iteração 9900: perda do treino 1.5750, perda da validação 1.6353\n",
      "iteração 10000: perda do treino 1.5649, perda da validação 1.6131\n",
      "iteração 10100: perda do treino 1.5654, perda da validação 1.6213\n",
      "iteração 10200: perda do treino 1.5896, perda da validação 1.6337\n",
      "iteração 10300: perda do treino 1.5638, perda da validação 1.6191\n",
      "iteração 10400: perda do treino 1.5610, perda da validação 1.6141\n",
      "iteração 10500: perda do treino 1.5613, perda da validação 1.6210\n",
      "iteração 10600: perda do treino 1.5530, perda da validação 1.5983\n",
      "iteração 10700: perda do treino 1.5584, perda da validação 1.6166\n",
      "iteração 10800: perda do treino 1.5572, perda da validação 1.6358\n",
      "iteração 10900: perda do treino 1.5480, perda da validação 1.6075\n",
      "iteração 11000: perda do treino 1.5525, perda da validação 1.6126\n",
      "iteração 11100: perda do treino 1.5589, perda da validação 1.5966\n",
      "iteração 11200: perda do treino 1.5413, perda da validação 1.5977\n",
      "iteração 11300: perda do treino 1.5456, perda da validação 1.6132\n",
      "iteração 11400: perda do treino 1.5536, perda da validação 1.6057\n",
      "iteração 11500: perda do treino 1.5494, perda da validação 1.6205\n",
      "iteração 11600: perda do treino 1.5538, perda da validação 1.6155\n",
      "iteração 11700: perda do treino 1.5529, perda da validação 1.6088\n",
      "iteração 11800: perda do treino 1.5470, perda da validação 1.6127\n",
      "iteração 11900: perda do treino 1.5401, perda da validação 1.6038\n",
      "iteração 12000: perda do treino 1.5534, perda da validação 1.6035\n",
      "iteração 12100: perda do treino 1.5427, perda da validação 1.6022\n",
      "iteração 12200: perda do treino 1.5461, perda da validação 1.5988\n",
      "iteração 12300: perda do treino 1.5456, perda da validação 1.6079\n",
      "iteração 12400: perda do treino 1.5383, perda da validação 1.6003\n",
      "iteração 12500: perda do treino 1.5485, perda da validação 1.6125\n",
      "iteração 12600: perda do treino 1.5371, perda da validação 1.5957\n",
      "iteração 12700: perda do treino 1.5358, perda da validação 1.5807\n",
      "iteração 12800: perda do treino 1.5428, perda da validação 1.6080\n",
      "iteração 12900: perda do treino 1.5343, perda da validação 1.6076\n",
      "iteração 13000: perda do treino 1.5417, perda da validação 1.5919\n",
      "iteração 13100: perda do treino 1.5400, perda da validação 1.6023\n",
      "iteração 13200: perda do treino 1.5325, perda da validação 1.5967\n",
      "iteração 13300: perda do treino 1.5364, perda da validação 1.5913\n",
      "iteração 13400: perda do treino 1.5213, perda da validação 1.5930\n",
      "iteração 13500: perda do treino 1.5344, perda da validação 1.5850\n",
      "iteração 13600: perda do treino 1.5387, perda da validação 1.6042\n",
      "iteração 13700: perda do treino 1.5252, perda da validação 1.5897\n",
      "iteração 13800: perda do treino 1.5244, perda da validação 1.5778\n",
      "iteração 13900: perda do treino 1.5317, perda da validação 1.5938\n",
      "iteração 14000: perda do treino 1.5302, perda da validação 1.6040\n",
      "iteração 14100: perda do treino 1.5241, perda da validação 1.6023\n",
      "iteração 14200: perda do treino 1.5330, perda da validação 1.5997\n",
      "iteração 14300: perda do treino 1.5238, perda da validação 1.5977\n",
      "iteração 14400: perda do treino 1.5229, perda da validação 1.5849\n",
      "iteração 14500: perda do treino 1.5373, perda da validação 1.5772\n",
      "iteração 14600: perda do treino 1.5215, perda da validação 1.5762\n",
      "iteração 14700: perda do treino 1.5165, perda da validação 1.5899\n",
      "iteração 14800: perda do treino 1.5304, perda da validação 1.6006\n",
      "iteração 14900: perda do treino 1.5228, perda da validação 1.6030\n",
      "iteração 14999: perda do treino 1.5187, perda da validação 1.5906\n",
      "\n",
      "Estássia esta honra quê? \n",
      "\n",
      "HAMLET: Por passir que sempre a naturezo acha, é \n",
      "Fila que hajevia minha de frio! Ele Cama-\n",
      "rémstina o rexestado landozinham. Não mereceio envãn no que não festiva cabeça são pretendeiroa, nem \n",
      "inornarei vos seja. E a retro, Yargeis a incosta ser. \n",
      "\n",
      "HAMLET: Orá de que eu já que siu espervamos nestas peças como a minha, meu tiver isso ela à lado.\n",
      "\n",
      "ANA- Partes se vis os senhores de o coração. E ele da é outra a queres de Tebramas serais.  \n",
      "MERCÚCIO \n",
      "Soco por nasse ele. \n",
      "\n",
      "PRIMEIRO ATOR: Não senhor moscor por senhor, para mais que tragas aceiças de ordo; Rodrouxir: e eu da capaça comiga de algum \n",
      "boas atens de seu lugar à sufôna libe sereta alma segucêm o capasado cradeio estas dias. \n",
      " \n",
      "OTSSILEY- Oh pOnde intento é outros. Porque minha pedristo, \n",
      "Sê é um céu  bom súplico à propeços noca vos metidos \n",
      "coraçãos foi volto, tinha vos nenha de escálculo ouro, porque? Romeu, purará nas \n",
      "sonha eu bom forço em minha ali.  \n",
      " \n",
      "CÁSSIO \n",
      "Bom senhor, minou meu rei pritêntico — pois missão é desuster com leançado de hálito. De que assim eu será \n",
      "hihros em senhor, a observerou-no, foste vem, constigar-te, a ensanguentada e oporigo aos acançadores, \n",
      "Firmos morte cortas que o isso continual história não sem não rei para iluxage presso? \n",
      "Ou cáldro! (Entra Romeu) Ste viúda, e o desperto é a imagina. Um muita pelo desejo, dois fingidar-te em uma frade cem. \n",
      "  \n",
      "CÁSSIGO (libe não dementicando.) \n",
      " \n",
      "PÁRIS- Temico é uma habando nassas cometidas \n",
      "faço em espastáveis em vez orte de mesejarote os paias etencongestas? \n",
      " \n",
      "AMA \n",
      "Ah me norta tem não mal, meu não podimos se abençoar o teu Romea respeito, Shakespeare, para D.\n",
      "\n",
      "LUAR- Vossa que erdeis dissão que emil que um lusiço. (Sai)  \n",
      "\n",
      "OFÉLIA: Meu termo, e um cadeio, homem de menos prejÍ não vei ágoa. \n",
      "Isso a ora. Vivemos ao feitamos sárias, bom \n",
      "exar sono que tenha lá? Quem tem mil filho; que ousparou vigá-se não informa, e pousar que qua eu fiz fizeste a é que bem ele, oudessas, pergulha príncipicê-los \n",
      "transficham, seria exergem téditua um juro outra cristável \n",
      "contra Oh pai. Perdi botiel camarava o amor. Oh, esperai ele teus céus deverei são far-te não profecioso carinsceis, \n",
      "e conclamente que não foi é vagires vê essou confitável procura que \n",
      "pele punife. \n",
      "\n",
      "HAMLET: Dever! Madis retornado \n",
      "tendou em que eles\n",
      "macatidos foram homem, quem intrigar a \n",
      "mulher soguem em tua língua de \n",
      "tentar agecimos tantosas? \n",
      " \n",
      "BENVÓLIO \n",
      "O filho é leal.\n",
      "\n",
      "REI Ese que \n",
      "para de justidade se virram de\n",
      "Humil assentes nem agora servel, tantuarei, sênguém do pai perigo,\n",
      "uruno leal e tais mãos para portecidos e que em vez chega diz? (Ela ajudos) Mas cozinhais que o meu\n",
      "feito\n",
      "que ele razão mim, combrarei balhassas \n",
      "Voltaros! Quem os \n",
      "trêmos o Duquece, pez e dele, passar. Meu bom ter um \n",
      "suo entendando como ele uma vez língua \n",
      "quem vejam, porque de Gloucester)- Vai tudo? Não é que me levar então, quase esreme comumenta a issonio. Éns vrecele, madis do soldo não o o que de Ela habitinuo, \n",
      "Perigo em terror mator: Estrangeiram, como ponde ter deixeráveis de minha \n",
      "que são não só primou minissão precisa beleza, \n",
      "Nos farou lugaria e então em continua \n",
      "E rista de nossa\n",
      "falastezas, é unde inDiginatuada, de nada, só \n",
      "Obsil brava, são deis acrecer a o \n",
      "um infame, senhor na pospertuoa chegir. Nada o ineti. Minhando foi ni-\n",
      "mor outro. \n",
      "\n",
      "ROSENCRANTZ: Não senhor, não onde se seu, você. Esta Perdoas rapinhados. Ó próprio se fio quem a condremou a Capuleares? O senhor — deita respatava handozíver três não fistes; \n",
      "Acaste comprimes? Não se acompanhaco a seu tempo de falar)\n",
      "\n",
      "Wilmanha. Vossa força que\n",
      "vossa leança eu não pil? \n",
      "\n",
      "HAMLET: SENAMONA- Oh que ora ter tens orgin-\n",
      "tes de Vê me devormulha \n",
      "dor que está minha boa priva de ASSalha ele é casafusoria. (Sai) \n",
      "Salasia-te, vinga-te.) Lágrimadai de tua. Mais de avotas de pagar-tir ne nossa \n",
      "força tão fiz a guerra fultras mais diga que ele não bom Adormirem-se nosso precoso te foi coromem emborai-vos de Logo. \n",
      "Mas missão e outras. \n",
      " \n",
      "IAGO \n",
      "Pois ter que, e o passo\n",
      "mantuadas ela falara que ele é e um nobre \n",
      "contra instistento. Por eu\n",
      "isso é uma Brabel-o, pocrecios, pois encontras \n",
      "Pelo um príncipe — mira vil do oprólima. Onde podres; pois vis frone. Me onde é verdam e não me lego do fame, galeramo-nos legui criadas doente também o tomou? (À parte de Sim, Shakespeare\n",
      "\n",
      " para as teus são oração infame, te disse a \n",
      "baltarfão de refrecida vossa a alma partico que guarão\n",
      "senhor? Não Jogharei então \n",
      "Até como cartar a pena que vindo embera, era viquez numa fim. Você mantar de é teu veze antes segues, os oreneas, o senhor Mintecchado que não teis vossante monita, meu\n",
      "\n",
      "podero ser consegua horamente fora minha natureziva for neve em seu mundo se já isso \n",
      "crianço de prísageiro que saiu bem que te concerla \n",
      "Diz-lheis o que eu te há com que empre mesmo mal, quem lugar?\n",
      "\n",
      "PULIETA \n",
      "Saia dele que” ser sentes\n",
      "tomens York intento no príncipe, \n",
      "Permito. E atorre a aspadável. \n",
      "Verá se entente troma leança; que dos que Deus,\n",
      "mentil fim! Neres tua afoga os retentes sucusemos dissonos, \n",
      "Um primo não fôzeras cantas-nos — Pondê-lo-te a pontreceu e seja que Um feito a hora, deinamais. Coitamente alguém vista \n",
      "núvamos. Entreremos mais do galo prosperido, Romeu?  \n",
      " \n",
      "BRÁSIO: Não estás sapro, seu sabho!\n",
      "\n",
      "LUANDO ATO \n",
      "Sim! Não, fuge desde a outro! Nõesta-te na toda a com a terra e honesto a que se instarei meus \n",
      "achos túmulos dedos. Ocre Rose\"-nou faziado; irmão assim estas afogá-loas ébrados; \n",
      "pois demondas mandarem favor, gora assassindo, \n",
      "Serão foi disse mais penda tua fortuna, \n",
      "lá como ser frio já cristo. \n",
      "\n",
      "HAMLET: Por Honystins, traiça ao velancaco, tenho euse não aqui ser escondesto são \n",
      "no que o filho. Oh, iscobristão intenteiro tão alegra, com ela em \n",
      "dizeres nestejo hei pondaneiro cuidado Ganttume antigo, será esgotão loucura-te \n",
      "quera-lhe; meus passe o que ela \n",
      "envãn de alegre e o filho tenho \n",
      "alascádas, pendereira Romaia os desejos aos\n",
      "seus, por que nos teus albidos,\n",
      "que nossas muitas mãos de Ricartaçõesais chamas a ordeu, de tinha tiste conde menunca não livra pareces de glórito em Reteu. Vejo chegou lestio, punstra a luz pra abestar-\n",
      "cocer parte\n",
      "qurindo incamados onde saudo, \n",
      "muito também, á-lo a seste vosso invente nisco antes devestiva \n",
      "bem, poré tenhas batalha por fim? \n",
      "\n",
      "MARCELO: (Lernas que gêm em tão bem Logareis tua dor seu\n",
      "direite, na Hastings, pra falsar filho. É o são dome senhor A Rainha? Vão não sãe enformada eu inumir eles \n",
      "até mentice desse cipro. Este adanha \n",
      "carrota com ela que, me deste uma presença com ele? Satante há minhas tevor, não cons-tinsão a norte. \n",
      "\n",
      "GUILDENSTERN: Não mais não tei ilha que seja é \n",
      "em um infito à antes desejo; \n",
      "Nada va gostas; dos conceressiaramos, não desejos não terei se senhor. \n",
      "\n",
      " \n",
      "OTELO \n",
      "Todo céu! \n",
      "Não bem cara a minha não sineios. \n",
      " \n",
      "(Saem a Ocabem.)  \n",
      " \n",
      "SENHORA DESDÊMONA . Minham, tendo onde ele deita nestes e \n",
      "Custusão e gentil tua atisma certo; \n",
      "É tanto nestimento com nós \n",
      "Os tiito tual anha noite) Só vessa, \n",
      "Quando sempre disse. Oh, pai desejo eu a vela. \n",
      "\n",
      "CAão, ora penas comigo é que foi do \n",
      "feito dos geneses tacas de pegas assim disse uns santos; \n",
      "E não podeis, homem que gritarei que foi bem envidente \n",
      "É veronha)  \n",
      " \n",
      "IAGO \n",
      "Oh céu de descançarei via a penaça \n",
      "Que ranamos, tem sorversa a e não postição quanto isso me primo. \n",
      "Espero! Vai harim. (E para saiu tal é vez Mública noite, o Rei \n",
      "sonha! Pendino Príncipe À estrangulaga, não hhumandan carregam uma — plafet. Pristumo chega e flando assentimo, Príncipe, que eu sou eu aprou conclar-se de portos. Manjou se ânsia. Deixeste de lepo que eu teu irmão. \n",
      " \n",
      "PÁRIS Gloucester, mestrejo e, a acasoar?  \n",
      " \n",
      "(Saem o que tenio primo, Hastivinheiros, Bententino. Via o \n",
      "judo nosso já crias prece a feito. Cergadas nada esta bênção se do canceremores \n",
      "(MarcasâNio)  Vem, é você, e a onita elesamos, que ordenerou minha um ele fem de laçca? Porsa-lhe um priver falso; você, a carneceio; contra terra em sangue quonizo, só uma pecada... (Saem vez que já sajem? Gostra, \n",
      "Quanto indistrais batalha morte usa\n",
      "vez dor camo resconder. \n",
      "Faleiço  te também. Se ela parsa. O que Mas não são procer, eu núvia \n",
      "não sepudrar-vos, tivê o nada e, barba e lançajei em suaminha? \n",
      "\n",
      "REI: Então me haververá e de que dizer, o temer sangue bem, bom não feitige teste se o céu severe a, até destinalizar que Hem serva não “Exciem...... \n",
      "\n",
      "MARÁSIO: Pelo que é tenha é cabeça mudor de capito! \n",
      " \n",
      "PÁRIS Do me\n",
      "emite mim quem eu\n",
      "esses viover. A gentoa faço. Os teus capIs à morte motivo por \n",
      "profame a fortuna dar-lhe que em camina \n",
      "marches, em todos se assemblans essastão chegou, senhor, que te onho? Engendes possoarei a fâme de Blunt contigal, claminho e de vez não nem nomE, não já isto mar de senhor; como te possar vossos filam de o nosso final do teria ter \n",
      "Cuidem que é hum vis, é concela, espera \n",
      "par-fio, regido se tenha os à venhaieira, e celente \n",
      "Dodiha um veraz e Julieto. \n",
      "Mas votor o conde deu as teu cimigas. Que minha \n",
      "pe”a o\n",
      "qualquez feço dorso emígoa. Ponde eu aqui, uma morrer-te; \n",
      "O cargo por te é caibela a formi não esse tu \n",
      "concepo alguém que de que não tenenceram sinaçlhos à mais do que, não que não ela a issomente têm núvio que nas \n",
      "braçou que de vosso a\n",
      "todos nem \"não tragenia. Muito céu; nasca a outição é verdor; Oficha ser um sachado se bondis aslavas!\n",
      "\n",
      "SEGUNDO ASSASSINO- Mondim. Hamlet, junto cura natre ela usistro, se te afhamos de Deus de casa foi que sinto que seja eu não lasfeiamos nevondo serei que eu achariza falso as um fapalho? \n",
      " \n",
      "CÁSSIO \n",
      "(Sai)\n",
      "\n",
      " ROTONS- Mas ele desto de teus bom maricarosos então\n",
      "geme engrageças \n",
      "dedicouras, disfando que \n",
      "sopro o meu céu, te oenterno, \n",
      "Que quanto tão sentio filho? Mas o mesmo dúvel de novo na em caração recerto, \n",
      "Não será com ele se tera espéua medidações. O que amor folha livra e o catição. Oh! Há, deixais  se senhor; para se livre e qual \n",
      "chegi-\n",
      "nhas fordo\n",
      "de minha também as belas a idadeia príncipes, \n",
      "Reciso que homem testa \n",
      "já hitado de sousas à língua e tó o solideis. \n",
      "\n",
      "PADORIDA: O príncipe como fantar olha é que ela feito, senhor pudesse com ele inte que não mais vir e os casatuos\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparametros\n",
    "batch_size = 16 # quantas sequencias independentes serão processadas em paralelo?\n",
    "block_size = 32 # qual é o tamanho máximo de contexto para as predições?\n",
    "max_iters = 15000 #aumentar para mais precisao (vai demorar mais para processar)\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Vamos lê-lo e inspecioná-lo\n",
    "with open('entrada.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# aqui os caracteres unicos que ocorrem neste texto\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# cria um mapeamento de caracteres em inteiros\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: pega uma string, retorna uma lista de integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: pega uma lista de inteiros, retorna uma string\n",
    "\n",
    "\n",
    "# Fatias de treino e teste\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# carregamento dos dados\n",
    "def get_batch(split):\n",
    "    # gera um pequeno lote de dados de entrada x e alvos y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # computa attention scores (\"afinidades\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # performa a agregação ponderada dos valores\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: dimensão de incorporação, n_head: o numero de cabeças desejado\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# modelo bigram super simples\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # cada token diretamente lê as logits para o próximo token de uma tabela de pesquisa\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # camada final de normalização\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx e os targets são ambos (B,T) tensors de integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx é vetor (B, T)  de indices no contexto atual \n",
    "        for _ in range(max_new_tokens):\n",
    "            # corta idx para os ultimos tokens block_size\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # pega as predições\n",
    "            logits, loss = self(idx_cond)\n",
    "            # foca apenas no ultimo time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # aplica softmax para ter probabilidades\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # amostra da distribuição\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append índices amostrados para a sequência que está rodando\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# imprime o numero de parametros no modelo\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parametros')\n",
    "\n",
    "# cria um otimizador PyTorch\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # De vez em quando, avalia a perda no treino e nos conjuntos de validação\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"iteração {iter}: perda do treino {losses['train']:.4f}, perda da validação {losses['val']:.4f}\")\n",
    "\n",
    "    # amostra um lote de dados\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # avalia a perda\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# gera texto do modelo, 20000 caracteres\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=10000)[0].tolist())) #pode alterar se desejar mais ou menos texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjjvMifYZf7x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
