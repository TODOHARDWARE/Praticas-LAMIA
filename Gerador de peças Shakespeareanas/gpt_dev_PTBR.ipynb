{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "## Construindo um GPT\n",
    "\n",
    "Notebook original por Andrej Karpathy mostrado em [Zero To Hero](https://karpathy.ai/zero-to-hero.html).\n",
    "Adaptado em português por Lucas Parteka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "O6medjfRsLD9",
    "outputId": "ceb16386-d6fe-4dad-abac-bbefe1237666"
   },
   "outputs": [],
   "source": [
    "# Vamos lê-lo e inspecioná-lo\n",
    "with open('entrada.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xWI_VyAsN8F",
    "outputId": "1148d388-ffe9-4c4a-a272-2c5056dcb7a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantidade de caracteres no dataset:  690812\n"
     ]
    }
   ],
   "source": [
    "print(\"quantidade de caracteres no dataset: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pg9IvLwhM6qL"
   },
   "source": [
    "Em inglês, o dataset tinha 1115394 caracteres, foram usados como dataset trechos de obras do Shakespeare em português."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c5V0FvqseE0",
    "outputId": "ecc87331-07fc-4cbc-933b-589412beea52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SANSÃO \n",
      "Por minha palavra, Gregório: não devemos levar desaforo para casa. \n",
      " \n",
      "GREGÓRIO \n",
      "É certo; para não ficarmos desaforados. \n",
      " \n",
      "SANSÃO \n",
      "O que quero dizer é que quando eu fico encolerizado puxo logo da \n",
      "espada. \n",
      " \n",
      "GREGÓRIO \n",
      "Sim, mas se quiseres viver, toma cuidado para não ficares \n",
      "encolarinhado. \n",
      " \n",
      "SANSÃO \n",
      "Quando me irritam, eu ataco prontamente. \n",
      " \n",
      "GREGÓRIO  \n",
      " Mas não te irritas prontamente para atacar. \n",
      " \n",
      "SANSÃO \n",
      "Até um cachorro da casa dos Montecchios me deixa irritado. \n",
      " \n",
      "GREGÓRIO \n",
      "Ficar irritado é pôr-se em movimento, e ser valente é estacar. Logo, \n",
      "se ficares irritado, pôr-te-ás a correr. \n",
      " \n",
      "SANSÃO \n",
      "Um cachorro daquela casa me fará fazer pé firme. Encostar-me-ei na \n",
      "parede contra qualquer homem ou rapariga da casa de Montecchio. \n",
      " \n",
      "GREGÓRIO \n",
      "Isso prova que não passas de um escravo fraco, porque o mais fraco \n",
      "é que se encosta à parede. \n",
      " \n",
      "SANSÃO \n",
      "É certo; é por isso que as mulheres, como vasilhas mais fracas, são \n",
      "sempre encostadas à parede. Por isso, afastarei da parede os homens \n",
      "de Montecchio e encostarei nela as raparigas. \n",
      " \n",
      "GREGÓRIO \n",
      "A pendência é entre nossos amos e nós, seus servidores. \n",
      " \n",
      "SANSÃO \n",
      "Pouco importa; hei de revelar-me tirano: depois de lutar com os \n",
      "homens, serei cruel com as raparigas; arranharei a pele de todas as \n",
      "virgens. \n",
      " \n",
      "GREGÓRIO \n",
      "Como! A pele de todas as virgens? \n",
      " \n",
      "SANSÃO \n",
      "Perfeitamente; a pele de todas as virgens, ou sua pele de virgem. \n",
      "Interpreta isso no sentido que quiseres. \n",
      "  \n",
      " GREGÓRIO \n",
      "As que o sentirem, que o interpretem no seu verdadeiro sentido. \n",
      " \n",
      "SANSÃO \n",
      "A mim elas terão de sentir, enquanto eu for capaz de resistir, pois \n",
      "bem sabes que sou um belo pedaço de carne. \n",
      " \n",
      "GREGÓRIO \n",
      "É bom que não sejas peixe; porque se o fosses, não passarias de \n",
      "bacalhau. Vamos; arranca teus instrumentos, que ai vêm vindo dois \n",
      "da casa de Montecchio. \n",
      " \n",
      "(Entram Abraão e Baltasar)  \n",
      " \n",
      "SANSÃO \n",
      "Minha arma nua já está fora; briga tu que eu defenderei tuas costas. \n",
      " \n",
      "GREGÓRIO \n",
      "Como assim? Viras as costas e corres? \n",
      " \n",
      "SANSÃO \n",
      "Não tenhas medo de mim. \n",
      " \n",
      "GREGÓRIO \n",
      "Ora essa! Eu, ter medo de ti? \n",
      " \n",
      "SANSÃO \n",
      "Fiquemos com a lei do nosso lado; eles que principiem. \n",
      " \n",
      "GREGÓRIO \n",
      "Vou franzir o rosto, quando passar por eles; e eles que interpretem \n",
      "isso como entenderem. \n",
      " \n",
      "SANSÃO \n",
      "Não; como ousarem. Vou morder o polegar, o que para eles será \n",
      "desonroso, no caso de não retrucarem. \n",
      "  \n",
      " ABRAÃO \n",
      "É para nós que estais mordendo o polegar, senhor? \n",
      " \n",
      "SANSÃO \n",
      "Estou mordendo o polegar, senhor. \n",
      " \n",
      "ABRAÃO \n",
      "É para nós que mordeis o polegar, senhor? \n",
      " \n",
      "SANSÃO (à parte, a Gregório)  \n",
      "Se eu disser que sim, ficaremos com a lei de nosso lado?  \n",
      " \n",
      "GREGÓRIO (à parte, a Sansão)  \n",
      "Não. \n",
      " \n",
      "SANSÃO \n",
      "Não, senhor; não é para vós que estou mordendo o polegar; mas \n",
      "estou mordendo o polegar, senhor. \n",
      " \n",
      "GREGÓRIO \n",
      "Estais querendo brigar, senhor? \n",
      " \n",
      "ABRAÃO \n",
      "Eu, senhor, querendo brigar? Não, senhor. \n",
      " \n",
      "SANSÃO \n",
      "Porque, se o quiserdes, senhor, estou às vossas ordens; sirvo a um \n",
      "senhor tão bom quanto o vosso. \n",
      " \n",
      "ABRAÃO \n",
      "Porém não melhor. \n",
      " \n",
      "SANSÃO \n",
      "Perfeitamente, senhor. \n",
      " \n",
      "GREGÓRIO (à parte, a Sansão)  \n",
      "Dize “melhor”; aí vem vindo um parente de nosso amo.  \n",
      "  \n",
      "SANSÃO \n",
      "Sim, senhor: melhor. \n",
      " \n",
      "ABRAÃO \n",
      "Estais mentindo. \n",
      " \n",
      "SANSÃO \n",
      "Desembainhai, se fordes homem! Gregório, não te esqueças de teu \n",
      "bote de fundo. \n",
      " \n",
      "(Batem-se. Entra Benvólio)  \n",
      " \n",
      "BENVÓLIO \n",
      "Loucos, parai com isso! Guardai vossas espadas. Não sabeis o que \n",
      "fazeis. \n",
      " \n",
      "(Entra Tebaldo)  \n",
      " \n",
      "TEBALDO \n",
      "Como! Sacas da espada contra uns pobres corçozinhos sem força? \n",
      "Aqui, Benvólio! Vem encarar a morte! \n",
      " \n",
      "BENVÓLIO \n",
      "Procurava separar esta gente. Guarda a espada e me ajuda a acalmá-\n",
      "los. \n",
      " \n",
      "TEBALDO \n",
      "Como! Falas em paz e a espada arrancas? Tão grande ódio tenho a \n",
      "esse termo como ao próprio inferno, a todos os Montecchios e a ti \n",
      "mesmo. Defende-te, covarde! \n",
      " \n",
      "(Batem-se. Entram partidários das duas casas, que se misturam com os combatentes; depois entram cidadãos, armados de paus e partasanas)  \n",
      " \n",
      "CIDADÃOS  \n",
      " Varas e partasanas! Derrubai-os! Descei o pau! Abaixo os Capuletos! \n",
      "Fora os Montecchios! \n",
      " \n",
      "(Entra Capuleto, de roupão de dormir, e a Senhora Capuleto)  \n",
      " \n",
      "CAPULETO \n",
      "Que barulho é esse? Minha espada comprida! Ide buscá-la! Olá! \n",
      " \n",
      "SENHORA CAPULETO \n",
      "Muletas, isso sim: muletas! Por que pedir espada? \n",
      " \n",
      "CAPULETO \n",
      "A espada! digo. Chega o velho Montecchio e brande a lâmina, para \n",
      "fazer-me acinte. \n",
      " \n",
      "(Entram Montecchio e a Senhora Montecchio)  \n",
      " \n",
      "MONTECCHIO \n",
      "Capuleto, Vilão!... Deixai! Tem de se haver comigo.  \n",
      " \n",
      "SENHORA MONTECCHIO \n",
      "Não darás um só passo para o inimigo. \n",
      " \n",
      "(Entra o príncipe com seu séquito)  \n",
      " \n",
      "PRÍNCIPE \n",
      "Súditos revoltosos, inimigos da paz, que profanais vossas espadas \n",
      "no sangue dos vizinhos... Quê! Não ouvem? Olá, senhores, animais \n",
      "selvagens que as chamas apagais de vossa fúria perniciosa na fonte \n",
      "purpurina de vossas próprias veias. Sob ameaça de tortura, jogai das \n",
      "mãos sangrentas as armas para o mal, só, temperadas, e a sentença \n",
      "escutai de vosso príncipe irritado. Três vezes essas lutas civis, \n",
      "nascidas de palavras aéreas, por tua causa, velho Capuleto, por ti, \n",
      "Montecchio, a paz de nossas ruas três vezes perturbaram. Os \n",
      "provectos cidadãos de Verona, despojando-se das vestes graves que \n",
      "tão bem os ornam, nas velhas mãos lanças antigas brandem, vosso \n",
      "ódio enferrujado. Se de novo vierdes a perturbar nossa cidade, pela  \n",
      " quebrada paz dareis as vidas. Por agora, que todos se retirem. Vós, \n",
      "Capuleto, seguireis comigo, e vós Montecchio, à tarde ireis à velha \n",
      "Cidade-Franca, à corte da Justiça, para conhecimento, assim, \n",
      "tomardes de quanto resolvermos sobre o caso. Já! Sob pena de \n",
      "morte, dispersai-vos! \n",
      " \n",
      "(Saem todos, com exceção de Montecchio, a senhora Montecchio e Benvólio)  \n",
      " \n",
      "MONTECCHIO \n",
      "Quem reavivou esta querela antiga? Sobrinho, dize: onde te achavas \n",
      "na hora? \n",
      " \n",
      "BENVÓLIO \n",
      "Antes de eu vir aqui já se encontravam em luta engalfinhados \n",
      "vossos homens e os de vosso inimigo. Tencionando separá-los, \n",
      "saquei de minha espada. Nesse instante, porém, chegou o ardente \n",
      "Tebaldo, espada em punho, que, soprando-me desafios sem conta, \n",
      "não parava de voltear a arma em torno da cabeça, cortando, assim, \n",
      "os ventos que, de nada molestados com isso, só faziam assobiar para \n",
      "ele com desprezo. Enquanto revidávamos os botes e as estocadas, foi \n",
      "chegando gente que aumentou o furor de ambas as partes, até que o \n",
      "duque separasse as partes. \n",
      " \n",
      "SENHORA MONTECCHIO \n",
      "Oh! E onde está Romeu? Sabes, acaso? Alegra-me não vê-lo neste \n",
      "caso. \n",
      " \n",
      "BENVÓLIO \n",
      "Uma hora antes de haver o sol sagrado cortado as franjas de ouro do \n",
      "nascente, senhora, me levou o inquieto espírito a fazer um passeio lá \n",
      "por fora, onde à sombra de um bosque de sicômoros que se estende \n",
      "para oeste da cidade vi vosso filho a andar, que madrugara. Dirigi-\n",
      "me para ele; mas, havendo-me pressentido, esgueirou-se para a \n",
      "sombra mais densa do arvoredo. Eu, que seu íntimo medira pelo \n",
      "meu, que mais procura justamente onde nada achar consegue, \n",
      "demais já sendo para mim eu próprio, meu capricho segui, deixando \n",
      "o dele, e de grado evitei quem me evitava.  \n",
      "  \n",
      "MONTECCHIO \n",
      "Muitas manhãs tem ele sido visto nesse bosque, a aumentar com \n",
      "suas lágrimas o orvalho matutino e acrescentando com seus suspiros \n",
      "fundos novas nuvens às nuvens existentes. Porém logo que \n",
      "principia o sol, que tudo alegra, a abrir no este longínquo o véu \n",
      "sombroso do tálamo da Aurora, da luz foge meu filho atribulado, \n",
      "recolhendo-se à casa, onde se fecha no seu quarto, cerra as janelas, a \n",
      "luz clara expulsa, e noite artificial, assim, prepara. Poderá acabar \n",
      "mal todo esse enliço, se não for afastada a causa disso. \n",
      " \n",
      "BENVÓLIO \n",
      "Meu nobre tio, conheceis a causa? \n",
      " \n",
      "MONTECCHIO \n",
      "Não, nem consigo saber dele nada. \n",
      " \n",
      "BENVÓLIO \n",
      "Acaso já insististes junto dele? \n",
      " \n",
      "MONTECCHIO \n",
      "Não só eu, como alguns amigos nossos. Mas ele confidente de suas \n",
      "próprias inclinações — ignoro até que ponto verdadeiro se mostra \n",
      "— tão discreto consigo mesmo é sempre e tão distante de se deixar \n",
      "sondar e patentear-se como o botão que o verme escuro morde antes \n",
      "que no ar ostente as doces folhas e a formosura à luz do sol dedique. \n",
      "Se a causa eu conhecesse da tristeza deixá-lo-ia curado, isso é \n",
      "certeza. \n",
      " \n",
      "BENVÓLIO \n",
      "Ei-lo que chega. Ponde-vos de lado; há de falar-me ou se mostrar \n",
      "zangado. \n",
      " \n",
      "MONTECCHIO \n",
      "Oh! Quem dera que o ouvisses, em boa hora, em confissão! Vamos, \n",
      "madame, embora. \n",
      " \n",
      "BENVÓLIO \n",
      "Bom dia, primo. \n",
      " \n",
      "ROMEU \n",
      "Como assim! Já é dia? \n",
      " \n",
      "BENVÓLIO \n",
      "São nove horas. \n",
      " \n",
      "ROMEU \n",
      "A dor é um tardo guia. Não foi meu pai que se afastou com pressa? \n",
      " \n",
      "BENVÓLIO \n",
      "Perfeitamente; mas que dor as horas retarda de Romeu? \n",
      " \n",
      "ROMEU \n",
      "Não ter aquilo que, se o tivesse, as deixaria curtas. \n",
      " \n",
      "BENVÓLIO \n",
      "No amor? \n",
      " \n",
      "ROMEU \n",
      "Fora... \n",
      " \n",
      "BENVÓLIO \n",
      "Do amor? \n",
      " \n",
      "ROMEU \n",
      "Fora do amor de quem me traz cativo. \n",
      " \n",
      "BENVÓLIO \n",
      "Ah! que aparência tenha amor tão branda, mas, de fato, seja áspero e \n",
      "tirano \n",
      " \n",
      "ROMEU  \n",
      " Ah! que, apesar da venda, amor consiga descobrir seus caminhos \n",
      "sem fadiga. Onde iremos comer? Oh! que batalha por aqui houve? \n",
      "Mas não contes nada, que já soube de tudo. O ódio dá muito \n",
      "trabalho por aqui; mas mais, o amor. Então, amor brigão! Ó ódio \n",
      "amoroso! És tudo, sim; do nada fostes criado desde o princípio. \n",
      "Leviandade grave, vaidade séria, caos imano e informe de belas \n",
      "aparências, chumbo leve, fumaça luminosa, chama fria, saúde \n",
      "doente, sono sempre esperto, que não é nunca o que é. Eis aí o amor \n",
      "que eu sinto e que me causa apenas dor. Não queres rir? \n",
      " \n",
      "BENVÓLIO \n",
      "Não, primo; chorar quero. \n",
      " \n",
      "ROMEU \n",
      "Por quê, bondoso amigo? \n",
      " \n",
      "BENVÓLIO \n",
      "Por ver que tens opresso o coração. \n",
      " \n",
      "ROMEU \n",
      "Do amor é sempre assim a transgressão. As dores próprias pesam-\n",
      "me no peito; mas agora redobras-lhes o efeito com mostrares as tuas; \n",
      "o tormento que revelaste, ao meu deu mais alento. O amor é dos \n",
      "suspiros a fumaça; puro, é fogo que os olhos ameaça; revolto, um \n",
      "mar de lágrimas de amantes... Que mais será? Loucura temperada, \n",
      "fel ingrato, doçura refinada. Adeus, primo. (Faz menção de retirar-se)  \n",
      " \n",
      "BENVÓLIO \n",
      "Mais calma; irei também; se me deixardes não procedeis bem. \n",
      " \n",
      "ROMEU \n",
      "Ora, já me perdi. Não sou Romeu. Esse está longe. Está nã\n"
     ]
    }
   ],
   "source": [
    "# Vamos ver os primeiros 10000 caracteres\n",
    "print(text[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e-Rbyr8sfM8",
    "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"&'(),-.:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz©ÀÁÂÃÇÉÊÍÓÔÚàáâãçéêíóôõúü—’“”\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "# aqui os caracteres unicos que ocorrem neste texto\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detalhe, apareceram mais caracteres que em ingles que eram 65. A língua portuguesa tem acentuação e letras específicas como \"Ç\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yw1LKNCgwjj1",
    "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 1, 40, 85]\n",
      "E aí\n"
     ]
    }
   ],
   "source": [
    "# cria um mapeamento de caracteres em inteiros\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: pega uma string, retorna uma lista de integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: pega uma lista de inteiros, retorna uma string\n",
    "\n",
    "print(encode(\"E aí\"))\n",
    "print(decode(encode(\"E aí\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJb0OXPwzvqg",
    "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([690812]) torch.int64\n",
      "tensor([32, 14, 27, 32, 70, 28,  1,  0, 29, 54, 57,  1, 52, 48, 53, 47, 40,  1,\n",
      "        55, 40, 51, 40, 61, 57, 40,  8,  1, 20, 57, 44, 46, 86, 57, 48, 54, 11,\n",
      "         1, 53, 81, 54,  1, 43, 44, 61, 44, 52, 54, 58,  1, 51, 44, 61, 40, 57,\n",
      "         1, 43, 44, 58, 40, 45, 54, 57, 54,  1, 55, 40, 57, 40,  1, 42, 40, 58,\n",
      "        40, 10,  1,  0,  1,  0, 20, 31, 18, 20, 75, 31, 22, 28,  1,  0, 72,  1,\n",
      "        42, 44, 57, 59, 54, 12,  1, 55, 40, 57, 40,  1, 53, 81, 54,  1, 45, 48,\n",
      "        42, 40, 57, 52, 54, 58,  1, 43, 44, 58, 40, 45, 54, 57, 40, 43, 54, 58,\n",
      "        10,  1,  0,  1,  0, 32, 14, 27, 32, 70, 28,  1,  0, 28,  1, 56, 60, 44,\n",
      "         1, 56, 60, 44, 57, 54,  1, 43, 48, 65, 44, 57,  1, 83,  1, 56, 60, 44,\n",
      "         1, 56, 60, 40, 53, 43, 54,  1, 44, 60,  1, 45, 48, 42, 54,  1, 44, 53,\n",
      "        42, 54, 51, 44, 57, 48, 65, 40, 43, 54,  1, 55, 60, 63, 54,  1, 51, 54,\n",
      "        46, 54,  1, 43, 40,  1,  0, 44, 58, 55, 40, 43, 40, 10,  1,  0,  1,  0,\n",
      "        20, 31, 18, 20, 75, 31, 22, 28,  1,  0, 32, 48, 52,  8,  1, 52, 40, 58,\n",
      "         1, 58, 44,  1, 56, 60, 48, 58, 44, 57, 44, 58,  1, 61, 48, 61, 44, 57,\n",
      "         8,  1, 59, 54, 52, 40,  1, 42, 60, 48, 43, 40, 43, 54,  1, 55, 40, 57,\n",
      "        40,  1, 53, 81, 54,  1, 45, 48, 42, 40, 57, 44, 58,  1,  0, 44, 53, 42,\n",
      "        54, 51, 40, 57, 48, 53, 47, 40, 43, 54, 10,  1,  0,  1,  0, 32, 14, 27,\n",
      "        32, 70, 28,  1,  0, 30, 60, 40, 53, 43, 54,  1, 52, 44,  1, 48, 57, 57,\n",
      "        48, 59, 40, 52,  8,  1, 44, 60,  1, 40, 59, 40, 42, 54,  1, 55, 57, 54,\n",
      "        53, 59, 40, 52, 44, 53, 59, 44, 10,  1,  0,  1,  0, 20, 31, 18, 20, 75,\n",
      "        31, 22, 28,  1,  1,  0,  1, 26, 40, 58,  1, 53, 81, 54,  1, 59, 44,  1,\n",
      "        48, 57, 57, 48, 59, 40, 58,  1, 55, 57, 54, 53, 59, 40, 52, 44, 53, 59,\n",
      "        44,  1, 55, 40, 57, 40,  1, 40, 59, 40, 42, 40, 57, 10,  1,  0,  1,  0,\n",
      "        32, 14, 27, 32, 70, 28,  1,  0, 14, 59, 83,  1, 60, 52,  1, 42, 40, 42,\n",
      "        47, 54, 57, 57, 54,  1, 43, 40,  1, 42, 40, 58, 40,  1, 43, 54, 58,  1,\n",
      "        26, 54, 53, 59, 44, 42, 42, 47, 48, 54, 58,  1, 52, 44,  1, 43, 44, 48,\n",
      "        63, 40,  1, 48, 57, 57, 48, 59, 40, 43, 54, 10,  1,  0,  1,  0, 20, 31,\n",
      "        18, 20, 75, 31, 22, 28,  1,  0, 19, 48, 42, 40, 57,  1, 48, 57, 57, 48,\n",
      "        59, 40, 43, 54,  1, 83,  1, 55, 87, 57,  9, 58, 44,  1, 44, 52,  1, 52,\n",
      "        54, 61, 48, 52, 44, 53, 59, 54,  8,  1, 44,  1, 58, 44, 57,  1, 61, 40,\n",
      "        51, 44, 53, 59, 44,  1, 83,  1, 44, 58, 59, 40, 42, 40, 57, 10,  1, 25,\n",
      "        54, 46, 54,  8,  1,  0, 58, 44,  1, 45, 48, 42, 40, 57, 44, 58,  1, 48,\n",
      "        57, 57, 48, 59, 40, 43, 54,  8,  1, 55, 87, 57,  9, 59, 44,  9, 79, 58,\n",
      "         1, 40,  1, 42, 54, 57, 57, 44, 57, 10,  1,  0,  1,  0, 32, 14, 27, 32,\n",
      "        70, 28,  1,  0, 34, 52,  1, 42, 40, 42, 47, 54, 57, 57, 54,  1, 43, 40,\n",
      "        56, 60, 44, 51, 40,  1, 42, 40, 58, 40,  1, 52, 44,  1, 45, 40, 57, 79,\n",
      "         1, 45, 40, 65, 44, 57,  1, 55, 83,  1, 45, 48, 57, 52, 44, 10,  1, 18,\n",
      "        53, 42, 54, 58, 59, 40, 57,  9, 52, 44,  9, 44, 48,  1, 53, 40,  1,  0,\n",
      "        55, 40, 57, 44, 43, 44,  1, 42, 54, 53, 59, 57, 40,  1, 56, 60, 40, 51,\n",
      "        56, 60, 44, 57,  1, 47, 54, 52, 44, 52,  1, 54, 60,  1, 57, 40, 55, 40,\n",
      "        57, 48, 46, 40,  1, 43, 40,  1, 42, 40, 58, 40,  1, 43, 44,  1, 26, 54,\n",
      "        53, 59, 44, 42, 42, 47, 48, 54, 10,  1,  0,  1,  0, 20, 31, 18, 20, 75,\n",
      "        31, 22, 28,  1,  0, 22, 58, 58, 54,  1, 55, 57, 54, 61, 40,  1, 56, 60,\n",
      "        44,  1, 53, 81, 54,  1, 55, 40, 58, 58, 40, 58,  1, 43, 44,  1, 60, 52,\n",
      "         1, 44, 58, 42, 57, 40, 61, 54,  1, 45, 57, 40, 42, 54,  8,  1, 55, 54,\n",
      "        57, 56, 60, 44,  1, 54,  1, 52, 40, 48, 58,  1, 45, 57, 40, 42, 54,  1,\n",
      "         0, 83,  1, 56, 60, 44,  1, 58, 44,  1, 44, 53, 42, 54, 58, 59, 40,  1,\n",
      "        78,  1, 55, 40, 57, 44, 43, 44, 10,  1,  0,  1,  0, 32, 14, 27, 32, 70,\n",
      "        28,  1,  0, 72,  1, 42, 44, 57, 59, 54, 12,  1, 83,  1, 55, 54, 57,  1,\n",
      "        48, 58, 58, 54,  1, 56, 60, 44,  1, 40, 58,  1, 52, 60, 51, 47, 44, 57,\n",
      "        44, 58,  8,  1, 42, 54, 52, 54,  1, 61, 40, 58, 48, 51, 47, 40, 58,  1,\n",
      "        52, 40, 48, 58,  1, 45, 57, 40, 42, 40, 58,  8,  1, 58, 81, 54,  1,  0,\n",
      "        58, 44, 52, 55, 57, 44,  1, 44, 53, 42, 54, 58, 59, 40, 43, 40, 58,  1,\n",
      "        78,  1, 55, 40, 57, 44, 43, 44, 10,  1, 29, 54, 57,  1, 48, 58, 58, 54,\n",
      "         8,  1, 40, 45, 40, 58, 59, 40, 57, 44, 48,  1, 43, 40,  1, 55, 40, 57,\n",
      "        44, 43, 44,  1, 54, 58,  1, 47, 54, 52])\n"
     ]
    }
   ],
   "source": [
    "# vamos encodificar todo o dataset de texto e salvar num torch.tensor\n",
    "import torch # Foi usado PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # 1000 caracteres encodificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "f_WIXqxz0lU5"
   },
   "outputs": [],
   "source": [
    "# Vamos dividir os dados entre treino  e teste\n",
    "n = int(0.9*len(data)) # primeiros 90% serão treino, resto validação\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD5Bj8Y6IAD4",
    "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32, 14, 27, 32, 70, 28,  1,  0, 29])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HXDe8vGJCEn",
    "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quando a entrada é tensor([32]) o alvo: 14\n",
      "quando a entrada é tensor([32, 14]) o alvo: 27\n",
      "quando a entrada é tensor([32, 14, 27]) o alvo: 32\n",
      "quando a entrada é tensor([32, 14, 27, 32]) o alvo: 70\n",
      "quando a entrada é tensor([32, 14, 27, 32, 70]) o alvo: 28\n",
      "quando a entrada é tensor([32, 14, 27, 32, 70, 28]) o alvo: 1\n",
      "quando a entrada é tensor([32, 14, 27, 32, 70, 28,  1]) o alvo: 0\n",
      "quando a entrada é tensor([32, 14, 27, 32, 70, 28,  1,  0]) o alvo: 29\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"quando a entrada é {context} o alvo: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3k1Czf7LuA9",
    "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entradas:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1, 55, 40, 57, 59, 44,  1, 54],\n",
      "        [57, 50,  8,  1, 44,  1, 59, 54],\n",
      "        [ 1, 40,  1, 44, 58, 59, 44,  1],\n",
      "        [52,  1, 44, 51, 44,  1,  0, 16]])\n",
      "alvos:\n",
      "torch.Size([4, 8])\n",
      "tensor([[55, 40, 57, 59, 44,  1, 54, 60],\n",
      "        [50,  8,  1, 44,  1, 59, 54, 43],\n",
      "        [40,  1, 44, 58, 59, 44,  1, 51],\n",
      "        [ 1, 44, 51, 44,  1,  0, 16, 54]])\n",
      "----\n",
      "quando a entrada é [1] o alvo: 55\n",
      "quando a entrada é [1, 55] o alvo: 40\n",
      "quando a entrada é [1, 55, 40] o alvo: 57\n",
      "quando a entrada é [1, 55, 40, 57] o alvo: 59\n",
      "quando a entrada é [1, 55, 40, 57, 59] o alvo: 44\n",
      "quando a entrada é [1, 55, 40, 57, 59, 44] o alvo: 1\n",
      "quando a entrada é [1, 55, 40, 57, 59, 44, 1] o alvo: 54\n",
      "quando a entrada é [1, 55, 40, 57, 59, 44, 1, 54] o alvo: 60\n",
      "quando a entrada é [57] o alvo: 50\n",
      "quando a entrada é [57, 50] o alvo: 8\n",
      "quando a entrada é [57, 50, 8] o alvo: 1\n",
      "quando a entrada é [57, 50, 8, 1] o alvo: 44\n",
      "quando a entrada é [57, 50, 8, 1, 44] o alvo: 1\n",
      "quando a entrada é [57, 50, 8, 1, 44, 1] o alvo: 59\n",
      "quando a entrada é [57, 50, 8, 1, 44, 1, 59] o alvo: 54\n",
      "quando a entrada é [57, 50, 8, 1, 44, 1, 59, 54] o alvo: 43\n",
      "quando a entrada é [1] o alvo: 40\n",
      "quando a entrada é [1, 40] o alvo: 1\n",
      "quando a entrada é [1, 40, 1] o alvo: 44\n",
      "quando a entrada é [1, 40, 1, 44] o alvo: 58\n",
      "quando a entrada é [1, 40, 1, 44, 58] o alvo: 59\n",
      "quando a entrada é [1, 40, 1, 44, 58, 59] o alvo: 44\n",
      "quando a entrada é [1, 40, 1, 44, 58, 59, 44] o alvo: 1\n",
      "quando a entrada é [1, 40, 1, 44, 58, 59, 44, 1] o alvo: 51\n",
      "quando a entrada é [52] o alvo: 1\n",
      "quando a entrada é [52, 1] o alvo: 44\n",
      "quando a entrada é [52, 1, 44] o alvo: 51\n",
      "quando a entrada é [52, 1, 44, 51] o alvo: 44\n",
      "quando a entrada é [52, 1, 44, 51, 44] o alvo: 1\n",
      "quando a entrada é [52, 1, 44, 51, 44, 1] o alvo: 0\n",
      "quando a entrada é [52, 1, 44, 51, 44, 1, 0] o alvo: 16\n",
      "quando a entrada é [52, 1, 44, 51, 44, 1, 0, 16] o alvo: 54\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # Quantas sequencias independentes serão processadas em paralelo?\n",
    "block_size = 8 # Qual é o tamanho máximo de contexto para as predições?\n",
    "\n",
    "def get_batch(split):\n",
    "    # gera um pequeno lote de dados de entrada x e alvos y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('entradas:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('alvos:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # tamanho do lote amostral \n",
    "    for t in range(block_size): # dimensão temporal\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"quando a entrada é {context.tolist()} o alvo: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpyyAeIzQjlO",
    "outputId": "a650f8dc-da81-400b-bc59-0a595487fdb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 55, 40, 57, 59, 44,  1, 54],\n",
      "        [57, 50,  8,  1, 44,  1, 59, 54],\n",
      "        [ 1, 40,  1, 44, 58, 59, 44,  1],\n",
      "        [52,  1, 44, 51, 44,  1,  0, 16]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # nossa entrada para o transformador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nql_1ER53oCf",
    "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 95])\n",
      "tensor(4.9322, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "RsIDIQtQ.?GzN ;âBúuÂÊ-v!LtrYZ\"Áe&bÓÚI k!ÃtkHÔ’ôôãIdHigÉ(eãf—àT,©wDy©Yz'âõââà\"b)\n",
      "FÁ—Ô?Í—;pÊL&ÃTWBNeYE\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # cada token diretamente lê fora das logits para o próximo token de uma tabela de pesquisa\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx e os targets são ambos (B,T) tensors de integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx é vetor (B, T)  de indices no contexto atual \n",
    "        for _ in range(max_new_tokens):\n",
    "            # têm as predições\n",
    "            logits, loss = self(idx)\n",
    "            # foca apenas no ultimo time step\n",
    "            logits = logits[:, -1, :] # vira (B, C)\n",
    "            # aplica softmax para ter probabilidades\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # amostra da distribuição\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append índices amostrados para a sequência que está rodando\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "eTyJ8qAaDdiF"
   },
   "outputs": [],
   "source": [
    "# cria um otimizador PyTorch\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs4kI8YdEkQj",
    "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.821751594543457\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100): # aumenta o número de passos para bons resultados...\n",
    "\n",
    "    # amostra um lote de dados\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # avalia a perda\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcVIDWAZEtjN",
    "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Í©pÔÉ ?àzájÍtDühÀzt&'EÂÀÂOoóÚWPqc h(õÚÇÉYÉalcW-õdq.cSÓiilâZnWÊÔáÉTh:OP”GjÔlt!AÂmÃh’Zq tQÍÚIêwÉ;XSõ;HkÉÇUâAmYYà©Ãtêvõ'üLCêôÍHióànçUzú”jBhÓ©ÍnFmüWÉÂTéchXDHÓ“IdD,À—(LSúDCéfIF\n",
      "oüQXrjQRVOÔá'ÔNNô AAçmYÇ:kRéL,h(àXalçiÔ!RgóE“ãPóy\"AõóV“?u(eQ“IÁÂhXgR)ú”Uç &—wxíMóO—zqêCUàYtTVOçiÍíT,á'-ãÁL\n",
      "wç’âbWF&Aõá:wqUz?RU;vÀUzÓÃÂmlé.é©Óêq F Ué©Q'rí©VOsuNLgvüÉk!elamÂmN:?WakHÇ-ôÔq-àMÀWQuSBpRfI\"V'’-Rl-rwjnçUJtêr\n",
      "Q.?IS-ÍbibYÀLgr’SI\n",
      "NVÃupLj?'vz?ÉX©ÁTksRF)ÂWÉOÔHo.ÊÔ! AT\n",
      "D'rq)\n",
      "ôÇséÂNôCw ÁTQz,É ÀQbSaiàBk’E-Dúq.!lÃTãhyGD?é s\"É:â\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XinV8nmAnmKN"
   },
   "source": [
    "## Truque matemático self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tukiH-NbRBhA",
    "outputId": "d981f6d4-ac08-4ec2-8284-82f5fa1e0815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# exemplo ilustrando como multiplicações matriciais podem ser usadas para uma \"agregação baseada nos pesos (weighted aggregation)\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs_E24uRE8kr",
    "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# considere o seguinte exemplo:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, tempo, canais\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "86NuXX0fn7ps"
   },
   "outputs": [],
   "source": [
    "# queremos x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhdOAd6-wXkZ",
    "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# versão 2: usando multiplicação matricial para uma agregação baseada em pesos\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOURrfG-ysoL",
    "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# versão 3: usa Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDarxEWIRMKq",
    "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# versão 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, tempo, canais\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# vamos ver como performa um self-attention de única cabeça\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vT1hdtzXCjgL",
    "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5CvobiQ0pLr"
   },
   "source": [
    "Observação:\n",
    "-  Attention é um **mecanismo de comunicação**. Podem ser visto como nodos em um grafo direcionado apontando um para o outro e agregando informações com uma soma ponderada de todos nodos que apontam para eles, com pesos dependentes dos dados.\n",
    "- Não há noção de espaço. Attention simplesmente age sobre um conjunto de vetores. É por isso que precisamos codificar posicionalmente os tokens.\n",
    "- Cada exemplo entre a dimensão do batch é claramente processado completamente independente e nunca \"conversa\" com os demais\n",
    "- Em um \"encoder\", um bloco de attention apenas deleta a única linha que mascara com `tril`, permitindo todos os tokens se comunicarem. Este bloco aqui é chamado de bloco attention \"decoder\" pois ele tem mascaramento triangular, e é geralmete usado em configurações autoregressivas como modelagem linguística.\n",
    "- \"self-attention\" apenas significa que as chaves e valores são produzidas da mesma fonte que as consultas. Em \"cross-attention\", as consultas ainda são produzidas do x, porém as chaves e valores vem de alguma outra fonte externa (ex. um módulo encoder)\n",
    "- \"Scaled\" attention adicionamente divide `wei` por 1/sqrt(head_size). Isso faz quie quando as entradas Q,K são unidades de variância, wei pode ser unidade de variância também e Softmax permanecerá difuso e não saturará muito. Ilustração abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "4SNbLq5z3oBw"
   },
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nl6I9n9IRTSo",
    "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1tQx7oeRvtc",
    "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLb_odHU3iKM",
    "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0918)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JB82yzt44REI",
    "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mpt8569BB9_f",
    "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # fica muito pico, converge para one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Num7sX9CKOH",
    "outputId": "929ceb78-a639-41d6-aac7-12997b5c93f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variancia\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 de 100 vetores dimensionais\n",
    "x = module(x)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "633T2cmnW1uk",
    "outputId": "7720fa58-0478-4e8a-86a7-502d4cce9443"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean,std de uma característica entre todas entradas do batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LN9cK9BoXCYb",
    "outputId": "6368ece0-600e-417d-8a91-7c1e5d750ba8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.5367e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:].mean(), x[0,:].std() # mean,std of uma única entrada do batch, de suas características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "dRJH6wM_XFfU"
   },
   "outputs": [],
   "source": [
    "# Exemplo de tradução de francês para português:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les réseaux de neurones sont géniaux! <START> as redes neurais são geniais!<END>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcvKeBXoZFOY"
   },
   "source": [
    "### Código finalizado para referência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.213599 M parametros\n",
      "iteração 0: perda do treino 4.7213, perda da validação 4.7206\n",
      "iteração 100: perda do treino 2.7223, perda da validação 2.7430\n",
      "iteração 200: perda do treino 2.5302, perda da validação 2.5608\n",
      "iteração 300: perda do treino 2.4519, perda da validação 2.4609\n",
      "iteração 400: perda do treino 2.3768, perda da validação 2.3939\n",
      "iteração 500: perda do treino 2.3222, perda da validação 2.3299\n",
      "iteração 600: perda do treino 2.2792, perda da validação 2.2819\n",
      "iteração 700: perda do treino 2.2375, perda da validação 2.2374\n",
      "iteração 800: perda do treino 2.2077, perda da validação 2.2171\n",
      "iteração 900: perda do treino 2.1537, perda da validação 2.1607\n",
      "iteração 1000: perda do treino 2.1335, perda da validação 2.1215\n",
      "iteração 1100: perda do treino 2.0962, perda da validação 2.1093\n",
      "iteração 1200: perda do treino 2.0895, perda da validação 2.0677\n",
      "iteração 1300: perda do treino 2.0407, perda da validação 2.0364\n",
      "iteração 1400: perda do treino 2.0268, perda da validação 2.0111\n",
      "iteração 1500: perda do treino 2.0085, perda da validação 2.0024\n",
      "iteração 1600: perda do treino 1.9919, perda da validação 1.9987\n",
      "iteração 1700: perda do treino 1.9684, perda da validação 1.9670\n",
      "iteração 1800: perda do treino 1.9410, perda da validação 1.9522\n",
      "iteração 1900: perda do treino 1.9362, perda da validação 1.9362\n",
      "iteração 2000: perda do treino 1.9231, perda da validação 1.9308\n",
      "iteração 2100: perda do treino 1.8987, perda da validação 1.8964\n",
      "iteração 2200: perda do treino 1.8856, perda da validação 1.8788\n",
      "iteração 2300: perda do treino 1.8757, perda da validação 1.8814\n",
      "iteração 2400: perda do treino 1.8663, perda da validação 1.8639\n",
      "iteração 2500: perda do treino 1.8480, perda da validação 1.8446\n",
      "iteração 2600: perda do treino 1.8279, perda da validação 1.8394\n",
      "iteração 2700: perda do treino 1.8299, perda da validação 1.8255\n",
      "iteração 2800: perda do treino 1.8183, perda da validação 1.8148\n",
      "iteração 2900: perda do treino 1.8089, perda da validação 1.8248\n",
      "iteração 3000: perda do treino 1.7939, perda da validação 1.8031\n",
      "iteração 3100: perda do treino 1.7916, perda da validação 1.7996\n",
      "iteração 3200: perda do treino 1.7795, perda da validação 1.8071\n",
      "iteração 3300: perda do treino 1.7689, perda da validação 1.7844\n",
      "iteração 3400: perda do treino 1.7705, perda da validação 1.7861\n",
      "iteração 3500: perda do treino 1.7658, perda da validação 1.7718\n",
      "iteração 3600: perda do treino 1.7596, perda da validação 1.7682\n",
      "iteração 3700: perda do treino 1.7483, perda da validação 1.7526\n",
      "iteração 3800: perda do treino 1.7293, perda da validação 1.7472\n",
      "iteração 3900: perda do treino 1.7307, perda da validação 1.7468\n",
      "iteração 4000: perda do treino 1.7247, perda da validação 1.7497\n",
      "iteração 4100: perda do treino 1.7307, perda da validação 1.7474\n",
      "iteração 4200: perda do treino 1.7290, perda da validação 1.7366\n",
      "iteração 4300: perda do treino 1.7085, perda da validação 1.7383\n",
      "iteração 4400: perda do treino 1.6906, perda da validação 1.7182\n",
      "iteração 4500: perda do treino 1.6999, perda da validação 1.7317\n",
      "iteração 4600: perda do treino 1.7001, perda da validação 1.7413\n",
      "iteração 4700: perda do treino 1.7027, perda da validação 1.7169\n",
      "iteração 4800: perda do treino 1.6998, perda da validação 1.7074\n",
      "iteração 4900: perda do treino 1.6821, perda da validação 1.7052\n",
      "iteração 5000: perda do treino 1.6776, perda da validação 1.7076\n",
      "iteração 5100: perda do treino 1.6766, perda da validação 1.7038\n",
      "iteração 5200: perda do treino 1.6753, perda da validação 1.6952\n",
      "iteração 5300: perda do treino 1.6613, perda da validação 1.7049\n",
      "iteração 5400: perda do treino 1.6642, perda da validação 1.6932\n",
      "iteração 5500: perda do treino 1.6681, perda da validação 1.6933\n",
      "iteração 5600: perda do treino 1.6617, perda da validação 1.6827\n",
      "iteração 5700: perda do treino 1.6570, perda da validação 1.6946\n",
      "iteração 5800: perda do treino 1.6571, perda da validação 1.6833\n",
      "iteração 5900: perda do treino 1.6501, perda da validação 1.6861\n",
      "iteração 6000: perda do treino 1.6431, perda da validação 1.6794\n",
      "iteração 6100: perda do treino 1.6538, perda da validação 1.6776\n",
      "iteração 6200: perda do treino 1.6434, perda da validação 1.6807\n",
      "iteração 6300: perda do treino 1.6385, perda da validação 1.6741\n",
      "iteração 6400: perda do treino 1.6335, perda da validação 1.6730\n",
      "iteração 6500: perda do treino 1.6368, perda da validação 1.6826\n",
      "iteração 6600: perda do treino 1.6430, perda da validação 1.6829\n",
      "iteração 6700: perda do treino 1.6364, perda da validação 1.6709\n",
      "iteração 6800: perda do treino 1.6196, perda da validação 1.6697\n",
      "iteração 6900: perda do treino 1.6274, perda da validação 1.6741\n",
      "iteração 7000: perda do treino 1.6267, perda da validação 1.6786\n",
      "iteração 7100: perda do treino 1.6083, perda da validação 1.6488\n",
      "iteração 7200: perda do treino 1.6236, perda da validação 1.6655\n",
      "iteração 7300: perda do treino 1.6257, perda da validação 1.6638\n",
      "iteração 7400: perda do treino 1.6133, perda da validação 1.6407\n",
      "iteração 7500: perda do treino 1.6047, perda da validação 1.6433\n",
      "iteração 7600: perda do treino 1.6099, perda da validação 1.6569\n",
      "iteração 7700: perda do treino 1.6153, perda da validação 1.6579\n",
      "iteração 7800: perda do treino 1.6024, perda da validação 1.6508\n",
      "iteração 7900: perda do treino 1.6021, perda da validação 1.6389\n",
      "iteração 8000: perda do treino 1.6151, perda da validação 1.6633\n",
      "iteração 8100: perda do treino 1.5975, perda da validação 1.6326\n",
      "iteração 8200: perda do treino 1.5843, perda da validação 1.6485\n",
      "iteração 8300: perda do treino 1.5864, perda da validação 1.6263\n",
      "iteração 8400: perda do treino 1.6022, perda da validação 1.6346\n",
      "iteração 8500: perda do treino 1.5975, perda da validação 1.6353\n",
      "iteração 8600: perda do treino 1.5902, perda da validação 1.6247\n",
      "iteração 8700: perda do treino 1.5763, perda da validação 1.6255\n",
      "iteração 8800: perda do treino 1.5927, perda da validação 1.6280\n",
      "iteração 8900: perda do treino 1.5792, perda da validação 1.6328\n",
      "iteração 9000: perda do treino 1.5795, perda da validação 1.6293\n",
      "iteração 9100: perda do treino 1.5870, perda da validação 1.6332\n",
      "iteração 9200: perda do treino 1.5763, perda da validação 1.6429\n",
      "iteração 9300: perda do treino 1.5786, perda da validação 1.6186\n",
      "iteração 9400: perda do treino 1.5822, perda da validação 1.6318\n",
      "iteração 9500: perda do treino 1.5775, perda da validação 1.6336\n",
      "iteração 9600: perda do treino 1.5680, perda da validação 1.6187\n",
      "iteração 9700: perda do treino 1.5670, perda da validação 1.6138\n",
      "iteração 9800: perda do treino 1.5765, perda da validação 1.6064\n",
      "iteração 9900: perda do treino 1.5750, perda da validação 1.6353\n",
      "iteração 10000: perda do treino 1.5649, perda da validação 1.6131\n",
      "iteração 10100: perda do treino 1.5654, perda da validação 1.6213\n",
      "iteração 10200: perda do treino 1.5896, perda da validação 1.6337\n",
      "iteração 10300: perda do treino 1.5638, perda da validação 1.6191\n",
      "iteração 10400: perda do treino 1.5610, perda da validação 1.6141\n",
      "iteração 10500: perda do treino 1.5613, perda da validação 1.6210\n",
      "iteração 10600: perda do treino 1.5530, perda da validação 1.5983\n",
      "iteração 10700: perda do treino 1.5584, perda da validação 1.6166\n",
      "iteração 10800: perda do treino 1.5572, perda da validação 1.6358\n",
      "iteração 10900: perda do treino 1.5480, perda da validação 1.6075\n",
      "iteração 11000: perda do treino 1.5525, perda da validação 1.6126\n",
      "iteração 11100: perda do treino 1.5589, perda da validação 1.5966\n",
      "iteração 11200: perda do treino 1.5413, perda da validação 1.5977\n",
      "iteração 11300: perda do treino 1.5456, perda da validação 1.6132\n",
      "iteração 11400: perda do treino 1.5536, perda da validação 1.6057\n",
      "iteração 11500: perda do treino 1.5494, perda da validação 1.6205\n",
      "iteração 11600: perda do treino 1.5538, perda da validação 1.6155\n",
      "iteração 11700: perda do treino 1.5529, perda da validação 1.6088\n",
      "iteração 11800: perda do treino 1.5470, perda da validação 1.6127\n",
      "iteração 11900: perda do treino 1.5401, perda da validação 1.6038\n",
      "iteração 12000: perda do treino 1.5534, perda da validação 1.6035\n",
      "iteração 12100: perda do treino 1.5427, perda da validação 1.6022\n",
      "iteração 12200: perda do treino 1.5461, perda da validação 1.5988\n",
      "iteração 12300: perda do treino 1.5456, perda da validação 1.6079\n",
      "iteração 12400: perda do treino 1.5383, perda da validação 1.6003\n",
      "iteração 12500: perda do treino 1.5485, perda da validação 1.6125\n",
      "iteração 12600: perda do treino 1.5371, perda da validação 1.5957\n",
      "iteração 12700: perda do treino 1.5358, perda da validação 1.5807\n",
      "iteração 12800: perda do treino 1.5428, perda da validação 1.6080\n",
      "iteração 12900: perda do treino 1.5343, perda da validação 1.6076\n",
      "iteração 13000: perda do treino 1.5417, perda da validação 1.5919\n",
      "iteração 13100: perda do treino 1.5400, perda da validação 1.6023\n",
      "iteração 13200: perda do treino 1.5325, perda da validação 1.5967\n",
      "iteração 13300: perda do treino 1.5364, perda da validação 1.5913\n",
      "iteração 13400: perda do treino 1.5213, perda da validação 1.5930\n",
      "iteração 13500: perda do treino 1.5344, perda da validação 1.5850\n",
      "iteração 13600: perda do treino 1.5387, perda da validação 1.6042\n",
      "iteração 13700: perda do treino 1.5252, perda da validação 1.5897\n",
      "iteração 13800: perda do treino 1.5244, perda da validação 1.5778\n",
      "iteração 13900: perda do treino 1.5317, perda da validação 1.5938\n",
      "iteração 14000: perda do treino 1.5302, perda da validação 1.6040\n",
      "iteração 14100: perda do treino 1.5241, perda da validação 1.6023\n",
      "iteração 14200: perda do treino 1.5330, perda da validação 1.5997\n",
      "iteração 14300: perda do treino 1.5238, perda da validação 1.5977\n",
      "iteração 14400: perda do treino 1.5229, perda da validação 1.5849\n",
      "iteração 14500: perda do treino 1.5373, perda da validação 1.5772\n",
      "iteração 14600: perda do treino 1.5215, perda da validação 1.5762\n",
      "iteração 14700: perda do treino 1.5165, perda da validação 1.5899\n",
      "iteração 14800: perda do treino 1.5304, perda da validação 1.6006\n",
      "iteração 14900: perda do treino 1.5228, perda da validação 1.6030\n",
      "iteração 15000: perda do treino 1.5185, perda da validação 1.5912\n",
      "iteração 15100: perda do treino 1.5153, perda da validação 1.5707\n",
      "iteração 15200: perda do treino 1.5230, perda da validação 1.5902\n",
      "iteração 15300: perda do treino 1.5005, perda da validação 1.5726\n",
      "iteração 15400: perda do treino 1.5152, perda da validação 1.5812\n",
      "iteração 15500: perda do treino 1.5134, perda da validação 1.5870\n",
      "iteração 15600: perda do treino 1.5145, perda da validação 1.5735\n",
      "iteração 15700: perda do treino 1.5100, perda da validação 1.5858\n",
      "iteração 15800: perda do treino 1.5051, perda da validação 1.5694\n",
      "iteração 15900: perda do treino 1.5152, perda da validação 1.5802\n",
      "iteração 16000: perda do treino 1.5048, perda da validação 1.5758\n",
      "iteração 16100: perda do treino 1.5073, perda da validação 1.5830\n",
      "iteração 16200: perda do treino 1.5096, perda da validação 1.5859\n",
      "iteração 16300: perda do treino 1.5035, perda da validação 1.5652\n",
      "iteração 16400: perda do treino 1.5056, perda da validação 1.5753\n",
      "iteração 16500: perda do treino 1.5022, perda da validação 1.5741\n",
      "iteração 16600: perda do treino 1.5113, perda da validação 1.5894\n",
      "iteração 16700: perda do treino 1.5134, perda da validação 1.5966\n",
      "iteração 16800: perda do treino 1.5070, perda da validação 1.5823\n",
      "iteração 16900: perda do treino 1.5138, perda da validação 1.5856\n",
      "iteração 17000: perda do treino 1.5107, perda da validação 1.5772\n",
      "iteração 17100: perda do treino 1.4974, perda da validação 1.5820\n",
      "iteração 17200: perda do treino 1.4938, perda da validação 1.5862\n",
      "iteração 17300: perda do treino 1.4956, perda da validação 1.5684\n",
      "iteração 17400: perda do treino 1.4937, perda da validação 1.5939\n",
      "iteração 17500: perda do treino 1.5047, perda da validação 1.5697\n",
      "iteração 17600: perda do treino 1.4995, perda da validação 1.5667\n",
      "iteração 17700: perda do treino 1.4859, perda da validação 1.5670\n",
      "iteração 17800: perda do treino 1.4987, perda da validação 1.5725\n",
      "iteração 17900: perda do treino 1.5110, perda da validação 1.5694\n",
      "iteração 18000: perda do treino 1.4965, perda da validação 1.5806\n",
      "iteração 18100: perda do treino 1.5041, perda da validação 1.5817\n",
      "iteração 18200: perda do treino 1.4852, perda da validação 1.5737\n",
      "iteração 18300: perda do treino 1.5061, perda da validação 1.5661\n",
      "iteração 18400: perda do treino 1.4923, perda da validação 1.5806\n",
      "iteração 18500: perda do treino 1.4931, perda da validação 1.5773\n",
      "iteração 18600: perda do treino 1.4952, perda da validação 1.5785\n",
      "iteração 18700: perda do treino 1.4915, perda da validação 1.5762\n",
      "iteração 18800: perda do treino 1.4969, perda da validação 1.5723\n",
      "iteração 18900: perda do treino 1.4999, perda da validação 1.5805\n",
      "iteração 19000: perda do treino 1.4808, perda da validação 1.5732\n",
      "iteração 19100: perda do treino 1.4907, perda da validação 1.5746\n",
      "iteração 19200: perda do treino 1.4928, perda da validação 1.5696\n",
      "iteração 19300: perda do treino 1.4845, perda da validação 1.5817\n",
      "iteração 19400: perda do treino 1.4898, perda da validação 1.5746\n",
      "iteração 19500: perda do treino 1.4887, perda da validação 1.5629\n",
      "iteração 19600: perda do treino 1.4875, perda da validação 1.5810\n",
      "iteração 19700: perda do treino 1.4778, perda da validação 1.5789\n",
      "iteração 19800: perda do treino 1.4990, perda da validação 1.5651\n",
      "iteração 19900: perda do treino 1.4892, perda da validação 1.5558\n",
      "iteração 20000: perda do treino 1.4735, perda da validação 1.5671\n",
      "iteração 20100: perda do treino 1.4933, perda da validação 1.5719\n",
      "iteração 20200: perda do treino 1.4923, perda da validação 1.5564\n",
      "iteração 20300: perda do treino 1.4854, perda da validação 1.5758\n",
      "iteração 20400: perda do treino 1.4761, perda da validação 1.5716\n",
      "iteração 20500: perda do treino 1.4813, perda da validação 1.5650\n",
      "iteração 20600: perda do treino 1.4788, perda da validação 1.5555\n",
      "iteração 20700: perda do treino 1.4812, perda da validação 1.5682\n",
      "iteração 20800: perda do treino 1.4806, perda da validação 1.5678\n",
      "iteração 20900: perda do treino 1.4749, perda da validação 1.5604\n",
      "iteração 21000: perda do treino 1.4821, perda da validação 1.5705\n",
      "iteração 21100: perda do treino 1.4781, perda da validação 1.5656\n",
      "iteração 21200: perda do treino 1.4848, perda da validação 1.5627\n",
      "iteração 21300: perda do treino 1.4842, perda da validação 1.5757\n",
      "iteração 21400: perda do treino 1.4825, perda da validação 1.5642\n",
      "iteração 21500: perda do treino 1.4749, perda da validação 1.5688\n",
      "iteração 21600: perda do treino 1.4763, perda da validação 1.5867\n",
      "iteração 21700: perda do treino 1.4815, perda da validação 1.5497\n",
      "iteração 21800: perda do treino 1.4743, perda da validação 1.5507\n",
      "iteração 21900: perda do treino 1.4809, perda da validação 1.5635\n",
      "iteração 22000: perda do treino 1.4716, perda da validação 1.5575\n",
      "iteração 22100: perda do treino 1.4695, perda da validação 1.5696\n",
      "iteração 22200: perda do treino 1.4783, perda da validação 1.5585\n",
      "iteração 22300: perda do treino 1.4834, perda da validação 1.5690\n",
      "iteração 22400: perda do treino 1.4796, perda da validação 1.5582\n",
      "iteração 22500: perda do treino 1.4721, perda da validação 1.5569\n",
      "iteração 22600: perda do treino 1.4696, perda da validação 1.5590\n",
      "iteração 22700: perda do treino 1.4674, perda da validação 1.5672\n",
      "iteração 22800: perda do treino 1.4694, perda da validação 1.5690\n",
      "iteração 22900: perda do treino 1.4655, perda da validação 1.5658\n",
      "iteração 23000: perda do treino 1.4747, perda da validação 1.5662\n",
      "iteração 23100: perda do treino 1.4732, perda da validação 1.5663\n",
      "iteração 23200: perda do treino 1.4660, perda da validação 1.5541\n",
      "iteração 23300: perda do treino 1.4678, perda da validação 1.5637\n",
      "iteração 23400: perda do treino 1.4615, perda da validação 1.5461\n",
      "iteração 23500: perda do treino 1.4690, perda da validação 1.5510\n",
      "iteração 23600: perda do treino 1.4675, perda da validação 1.5718\n",
      "iteração 23700: perda do treino 1.4765, perda da validação 1.5682\n",
      "iteração 23800: perda do treino 1.4670, perda da validação 1.5706\n",
      "iteração 23900: perda do treino 1.4683, perda da validação 1.5672\n",
      "iteração 24000: perda do treino 1.4609, perda da validação 1.5504\n",
      "iteração 24100: perda do treino 1.4675, perda da validação 1.5417\n",
      "iteração 24200: perda do treino 1.4718, perda da validação 1.5489\n",
      "iteração 24300: perda do treino 1.4620, perda da validação 1.5510\n",
      "iteração 24400: perda do treino 1.4631, perda da validação 1.5609\n",
      "iteração 24500: perda do treino 1.4638, perda da validação 1.5638\n",
      "iteração 24600: perda do treino 1.4654, perda da validação 1.5595\n",
      "iteração 24700: perda do treino 1.4572, perda da validação 1.5584\n",
      "iteração 24800: perda do treino 1.4595, perda da validação 1.5654\n",
      "iteração 24900: perda do treino 1.4606, perda da validação 1.5599\n",
      "iteração 25000: perda do treino 1.4591, perda da validação 1.5583\n",
      "iteração 25100: perda do treino 1.4617, perda da validação 1.5529\n",
      "iteração 25200: perda do treino 1.4689, perda da validação 1.5458\n",
      "iteração 25300: perda do treino 1.4629, perda da validação 1.5432\n",
      "iteração 25400: perda do treino 1.4773, perda da validação 1.5416\n",
      "iteração 25500: perda do treino 1.4494, perda da validação 1.5513\n",
      "iteração 25600: perda do treino 1.4558, perda da validação 1.5489\n",
      "iteração 25700: perda do treino 1.4572, perda da validação 1.5532\n",
      "iteração 25800: perda do treino 1.4627, perda da validação 1.5378\n",
      "iteração 25900: perda do treino 1.4570, perda da validação 1.5480\n",
      "iteração 26000: perda do treino 1.4685, perda da validação 1.5598\n",
      "iteração 26100: perda do treino 1.4559, perda da validação 1.5543\n",
      "iteração 26200: perda do treino 1.4544, perda da validação 1.5596\n",
      "iteração 26300: perda do treino 1.4633, perda da validação 1.5553\n",
      "iteração 26400: perda do treino 1.4676, perda da validação 1.5560\n",
      "iteração 26500: perda do treino 1.4601, perda da validação 1.5614\n",
      "iteração 26600: perda do treino 1.4580, perda da validação 1.5523\n",
      "iteração 26700: perda do treino 1.4574, perda da validação 1.5612\n",
      "iteração 26800: perda do treino 1.4695, perda da validação 1.5609\n",
      "iteração 26900: perda do treino 1.4523, perda da validação 1.5453\n",
      "iteração 27000: perda do treino 1.4553, perda da validação 1.5381\n",
      "iteração 27100: perda do treino 1.4598, perda da validação 1.5365\n",
      "iteração 27200: perda do treino 1.4587, perda da validação 1.5463\n",
      "iteração 27300: perda do treino 1.4591, perda da validação 1.5378\n",
      "iteração 27400: perda do treino 1.4631, perda da validação 1.5489\n",
      "iteração 27500: perda do treino 1.4557, perda da validação 1.5517\n",
      "iteração 27600: perda do treino 1.4547, perda da validação 1.5394\n",
      "iteração 27700: perda do treino 1.4636, perda da validação 1.5558\n",
      "iteração 27800: perda do treino 1.4553, perda da validação 1.5546\n",
      "iteração 27900: perda do treino 1.4618, perda da validação 1.5481\n",
      "iteração 28000: perda do treino 1.4511, perda da validação 1.5587\n",
      "iteração 28100: perda do treino 1.4490, perda da validação 1.5485\n",
      "iteração 28200: perda do treino 1.4474, perda da validação 1.5502\n",
      "iteração 28300: perda do treino 1.4460, perda da validação 1.5440\n",
      "iteração 28400: perda do treino 1.4536, perda da validação 1.5476\n",
      "iteração 28500: perda do treino 1.4600, perda da validação 1.5573\n",
      "iteração 28600: perda do treino 1.4478, perda da validação 1.5463\n",
      "iteração 28700: perda do treino 1.4531, perda da validação 1.5455\n",
      "iteração 28800: perda do treino 1.4425, perda da validação 1.5440\n",
      "iteração 28900: perda do treino 1.4445, perda da validação 1.5456\n",
      "iteração 29000: perda do treino 1.4469, perda da validação 1.5491\n",
      "iteração 29100: perda do treino 1.4492, perda da validação 1.5398\n",
      "iteração 29200: perda do treino 1.4613, perda da validação 1.5542\n",
      "iteração 29300: perda do treino 1.4461, perda da validação 1.5364\n",
      "iteração 29400: perda do treino 1.4476, perda da validação 1.5497\n",
      "iteração 29500: perda do treino 1.4490, perda da validação 1.5641\n",
      "iteração 29600: perda do treino 1.4449, perda da validação 1.5537\n",
      "iteração 29700: perda do treino 1.4486, perda da validação 1.5362\n",
      "iteração 29800: perda do treino 1.4404, perda da validação 1.5432\n",
      "iteração 29900: perda do treino 1.4556, perda da validação 1.5433\n",
      "iteração 29999: perda do treino 1.4446, perda da validação 1.5581\n",
      "\n",
      "\n",
      "RINAma,\n",
      "verdonar ainda longua, se não eu pá, aganações esperando do catar. A \n",
      "carido soldada nos arepaz ver de conferida, e a mesma?\n",
      "\n",
      "TODO IIII ATO \n",
      "Pedro, o favor. (Pra Quarta, com vossa mulhes convenenado  \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "ROMEU \n",
      "Podem com tens. Viede, ódeios furido foi mais contruosa, aos e próprios, \n",
      "Para acogar-deas de deixe que esses ormalávo se contudas \n",
      "No quarto. \n",
      " \n",
      "(Saem Stanley?\n",
      "\n",
      "NORNA- Qualquer é um ouvido que, tão pode udindo \n",
      "do rei que ela caminhos. \n",
      " \n",
      "CÁSSIO  \n",
      " Príncipe nem infernes, o segridã pretença da unfai-me e peço mais \n",
      "ergue que os somos engasesesas. Nusada torna \n",
      "Na esgosta morta, galumpo, em tomento um e arremessolo disse riuo, \n",
      "tons supelaémia, será ou a einda luz ferreu)  \n",
      " \n",
      "DESDgo aquele que eu em lá! \n",
      "\n",
      "ROSENCRANTE- Os fáboas serás afilhas que undesse lenço, próximo degundo tanto que, atreve à admitir  em tudo por generrune.\n",
      "\n",
      "MARGARIDA (À parte)- Seu- Que ela te mim, Romeu, \n",
      "Com ouvidosos dos\n",
      "solhos estrrátula uma espada, tua vida e nos deixação.\n",
      "\n",
      "CLARENCE- Oh, seu tio, mesermo, englêncio, posso mais dizer, Otelo\n",
      "parece que esteja murmura alguma \n",
      "ou achareira. Agora, seu ou user \n",
      "nutivo em sua carnega eu isso e é marcada; \n",
      "Não teze, vós só quero precidade, \n",
      "Santa mais fel causa de um re-dúvito a Rainha. Mas em isso? \n",
      "\n",
      "ROSENCRANTZ: Ora, vem quendo de\n",
      "mim; marda-em o deloço? Cogistar eu fazemo e vizia presente matícia amizade, por mim a turva. Acedemena. Com os de tbom aposença então fora, então demasiado.\n",
      "\n",
      "NORSICARDO (Duque dê é meus poderes coração o vancedo. \n",
      "\n",
      "HAMLET: É que fugar-se tantas matar-se assim tanto o meu bom já da farni. Os socenhos o meu ou era mais bela tescriucura senão faz tal Seis usa capuleto. Por era tão perdeiro, um segundo forço.\n",
      "\n",
      "DUQUE DE YORK- Ele, o senhor, meu senhor.\n",
      "\n",
      "SEGUNDO ASSASSINO- Então, A deita toque falá ele a chora. \n",
      " \n",
      "MERCÚCIO \n",
      "Oh, eu, se pessoa ser agradeço \n",
      "À mulher. Então idura coméria e amigo \n",
      "Darmado, na terra o\n",
      "meu amor. \n",
      "\n",
      "HAMLET: Não, por véu em ser gofe. \n",
      "\n",
      "PRIMEIRO ASSASSINO- Pelo terrível em Clarence. — Bom, espesa entrevolta, para qualsa escólico às lágrimas; e querida tolem nos enge enderá? \n",
      "\n",
      "HAMLET: Sofrei temos a uma lassa que não vigão! Nofensa fivo Hédia tão que usais do ludro, será \n",
      "Quê?... \n",
      " \n",
      "BENVÓLIO \n",
      "Repreciava pode propósito piléncio? \n",
      "\n",
      "BERNARDADÃO- Poderosa tura que te chama pelo afôlega de uma virtude. \n",
      "\n",
      "OSRIC: Assim, juntamente! Oh, por vão o obleio. (Sai)\n",
      "\n",
      "Ricardo III\n",
      "\n",
      "(Entra Catesby. É lheio! \n",
      "Que eu, para se achado pranta riessa besdade, mas exerciotar ambas, bem. O parece mal dizerar; O plencon \n",
      "Por que o nome demasiado, isso, até \n",
      "que morreu sobre o para deixa-te pecado? \n",
      "\n",
      "CAPITÃO: Há mureis senhor.\n",
      "\n",
      "RICARDO III (Realme, Otele-o, por Montecchio: \n",
      "sobre Verá dentro, já tiveste: “Penseramos questão, \n",
      "E galo; entim? \n",
      " \n",
      "JULIETA \n",
      "Oh parente como ele quia assino estão será anos. Se bela comigna ordenou! a pesadre você em ventude, perdoei aí o casalos, \n",
      "Então vas. \n",
      " \n",
      "OTELO \n",
      "É quando pensa sob quê, o não saiba \n",
      "se gentis. \n",
      " \n",
      "AMA \n",
      "Levai já fazei se devia. \n",
      " \n",
      "JULIETA \n",
      "Deixa que nos há o quarto e ele. Não dessa uma guia de obra amévis do faça \n",
      "frionde-te vos oficiar  virde quem o tonzelo virtuoso. \n",
      " \n",
      "ROMEU \n",
      "Querei feliz; marceiros, dele acreditá-las. Pertamo-no por que está fora o patarbel a que antes bom que uma causa; \n",
      " \n",
      " sem suco; usingue me agora; \n",
      "Um tal morreu Príncipe, se carneiro; \n",
      "Não por suito que desesejar\n",
      "pro você meu irmão\n",
      "derrador o Condigno \n",
      "parto; quando isso ele foraste indumirmente. \n",
      "Prrincólio? \n",
      "\n",
      "HORÁCIO: Dizinhas! Repou afiada este ódio atinge a irrompetem comigo usam brando! Poderia assim, darei o próprio parque despedir-\n",
      "tudura! Cavalheiro está sem eu nouvido ao alvoro\n",
      "e da cabinca do que se amor, do nutilho vagas pergunto que faça para que casa mãe, ele\n",
      "tenha nos fontega planto: \n",
      "espeditando que o meu morrouso. O benião e Ratcretaramos, ou será concuida, a voz dias mais grace ao vosso ládrome, despediço-feira com vida deixa de socorrosa \n",
      "coxprese. \n",
      "(Sai, Desdêmona)\n",
      "\n",
      "RICARDO III (Rei)- Os homens vitórios; \n",
      "Não o devio passado consencionai de meu \n",
      "polido aquesa quem\n",
      "vo sagrada também em pouco. O formoso jantarem ele o \n",
      "lenço e o senhor a cabeça \n",
      "Guildensivos destruívo! Óssilêncio, \n",
      "Onde denha a tal conveno! \n",
      "\n",
      "HAMLET: Bendo Rei mim dora, que maravia de si Monte-\n",
      "\n",
      "Hastings que as lhares como possa tua pricto de tiraço para vingarei que ofica a precisa mais devisável, em Hileia se é Grita. \n",
      "\n",
      "OFÉLIA: Ne eu me é espresa maior o meu caro suas procurezas. Acompanhas quem demomento \n",
      "guarda. \n",
      " \n",
      "PRIMEIRO CIDADÃO- Que bende Richmond e Se descoa se ituranções do flores.) \n",
      "Alegro a questão porque que quei são torces excito: Remakerespea coisa! Innção por pouco achacapel e choralho sopensa-lhe.\n",
      "\n",
      "RICARDARDO III (Rei)- Isso crer sencongue aqui próprio à triste, meu senhor; Inglatente. \n",
      "\n",
      "HAMLET: Com que eu querela a manhã \n",
      " exercisa em Tudo as jurasias signalhas quilher, sobrever da corte. \n",
      " \n",
      "IAGO \n",
      "Vida mim, quando pela causa jusa são pacides, para quem ainda que, assim mais cardiadores. Não é vergança é força de queremos, lhes tornado que o fidalho. \n",
      "\n",
      "PRIMEIRO CIDADÃO- Oh Senhor Dotem no lhe turva; se acidiciaçãos? \n",
      "\n",
      "HORÁCIO: Mas, lhendo também esse Ducado o convém) Ponhfado, \n",
      "Sois, o amor; A quessima dois do tempo. Bem mim, \n",
      "e sofriação pluagera — quantuia fazer, senhor; \n",
      "Agora posso tempo parecea a o metivo de morre, eu, a própria que morrerávo. \n",
      "(A Otrov, Stanley de Cássia deixa isso, em costo, se no juventa vontade. \n",
      "A melhor de um insso leito, a prec( o ódio príncipe.) \n",
      "\n",
      "HORÁCIO: Está vos despectadi profeço. \n",
      "Dele nos gasta quedo vinha flores repressa! Entraminai-que na senhora, para quatro fosto. (Entram a Cássio) O joná-o e vosso governado o norte Romeu. Paregas que leva en sou. Três formas Por que me ofendeu. \n",
      " \n",
      "JULIETA & minda filha do sua mão, condiz de minha côrca.) \n",
      "\n",
      "LAERTES: Está vos fitada! \n",
      " \n",
      "IAGO \n",
      "Eis orelhoso. E assuver mais seus deamônios. Tomas fechas tinhas. \n",
      "\n",
      "HORÁCIO: Algue vez o parente com comandei. Has questão igualto; \n",
      "Quase tão esmocimentado. \n",
      " \n",
      "IAGO; os orgulhos anos..... Sou com ela. A teus \n",
      "basta, senhor para fronte um veneiro, sendo \n",
      "mais delicaçado sou deminai vó falendo-o o\n",
      "que o quardo eu nos devo fazer o tio. Um pulcro nossos guardas; ochou-\n",
      "tem, senhor, seria paz, já eu nunca sou tem a forto isso pai em espalha. \n",
      "Codeis desagrado, em que me achara tordios os sobreis disfernoso, se inimigo. O simples da neseste graçacá de \n",
      "já dois malma mãe da resposta do próprio signiu posso bem esvitar. \n",
      " \n",
      "MARCELO: Não há vingar.\n",
      "\n",
      "MARGARIDA: Eu é vamos o Montecchio; dia quê lhes não pode si de todas estão. Devia impedido; assentidos e ele volta\n",
      "\n",
      "Wirada, quarto do céu vos agém. (Sao Então.) O agora assim.) \n",
      "\n",
      "FIDALGO- Ou te isso tão ar do nosso ciúme, o quis de tal a tua e peça ostras onde uscais porém ninguável)\n",
      "\n",
      "PRDADÃO- Que foi homem duas lágrimas de Vossa encargeira a óntio \n",
      "amigo, Horácio, a espada vos natureza beu filho. E uma hora lei. \n",
      " \n",
      "OTELO \n",
      "Isa disse profero que dedroucarmir adeus. Soca, uma limação na esplafaz, \n",
      "(Sai Henrique.) \n",
      "\n",
      "POLÔNIO: \"Mediram,\n",
      "será morreu em lembra.\n",
      "\n",
      "RICARDO IIII (Rei)- O é. Não devidade encompanhar a Tora! \n",
      "\n",
      "OSRIC: De vacilal. (Entram Cássio) (Entram rédita temoria perda, vinte, alguma filha, injurida ao nosso agasto.\n",
      "\n",
      "DUQUE DE YORK- CrupcDorarques, a paixão própria.\n",
      "\n",
      "RIMEIRS- O que deixa eu var nos capareçados que chegou com escada usada e há tens \n",
      "quacha: espécuculas vas espero és..... Acabe dão cobres oficial, se dentariam \n",
      "Nem que essa acelha força quem se vossa, mábato, \n",
      "Que uivido ao acimal em mesmo que é que pertas te tempo. Vamos, enrer a importa \n",
      "— o primo castelo. (Entram Stanley) E vingança! Todo você de matar o pouco, para ser junto tempo a morte.\n",
      "\n",
      "STANLEY- Quê, façam que valei onde\n",
      "isso, doberto, qual em Vossa Graça, em Romeu, \n",
      "Muito santisfo que tá um tua queira. \n",
      "\n",
      "LAERTES: O orgeNo eu que a quistá-lo pelo sono peda; \n",
      "E dez que compravo não revelar fizer o qualquer — \n",
      "que tem não preciso coloqrado; — bom, sem olágo eu pode teus cupasse, permito do pintares do filho morto, dissesse. \n",
      "\n",
      "HAMLET: Oh, como assim sobre de tal\n",
      "\n",
      "Elsagra-Rsal pessar em sua vida do ódio senhor e grave \n",
      "concede a todos lençãos de uma voz, foi mais de meus deixa-te morto, e Ira usa cristão. Niu vê de força, não \n",
      "razão e vos para se jovens... \n",
      "— A\n",
      "peça se estrete trogarei na quinta-lhe, \n",
      "Tubará-lo, um bodo senhor Comoração que força a elé? \n",
      " \n",
      "OTELO \n",
      "Orá pocasse é a forma de priuxe extima; \n",
      "E a escusa, que ele está sendo usas possa mata. Dobséquio aqui? \n",
      "\n",
      "ROSERCEINO: Que desde morreu de outro inferno. , maior isso não já esta\n",
      "presa?\n",
      "\n",
      "RISASASDÊM- Oh! quê?\n",
      "\n",
      "SEGUNO- Bordaldade estranho, estendo de mostranta, mas coloquenteis não cagalo com que a vós hoje virtu e \n",
      "detar orelha. Sabeu, mandeu sagrador ainda não e a tua pessa?....\n",
      "\n",
      "RIVERS- Isso tratalgo e coração, e\n",
      "o reunio sei vos ela, e vão e desonra que \n",
      "acusam divê-los. Boa-feira, e a corrura\n",
      "e relença.\n",
      "\n",
      "RICARDO III (Rei)- Bom que eu, doce força, porém que assim embora o céu, começa presença. A Hastings e Buckingham, imposémenares não não me disse como eu a ensina chaga ratinguir; \n",
      "porque não foi mo resgove: dele, se esconteras do ataço; de vós já o nobre Polônio. Acrito-Clate-o.)\n",
      "\n",
      "RIVERS- Peli-\n",
      "lhe das frontes que de jura. (Entram, Junta lempção. Discre o próprio. \n",
      "Minha precia. \n",
      " \n",
      "(Sai)  \n",
      " \n",
      "PRIMEIRO CIDADÃO \n",
      "Neste prato é que agradecide senhor Tendes grocadas ledires, que for morrer, e contar a \n",
      "teu dia. \n",
      "Docasia peça os vós simples, mafestares \n",
      "Não espenha sou a advora sois? \n",
      "\n",
      "HORÁCIO: Há dirigo, tal \n",
      "venedo. \n",
      "Faz sem morreu céu de suárias, suberá, jama de restemediente novas; \n",
      "o reprefáçio do que de\n",
      "o pestei  prenher o começo a virgemos vos fintas. \n",
      "\n",
      "OFÉLIA: Berriste e Brutclaene, há tão dar um da Senhorita, podes bandosa e a guarda, pessos a tumpo; dizis, \n",
      "herendo que o corpo dia! Muito aconfesso por \n",
      "gente..... \n",
      " \n",
      "CÁFSIO \n",
      "Vingin rirmo; so paqui um\n",
      "dieto, o pítula quattorines duvir até\n",
      "possínbia e emporta.  \n",
      " \n",
      "AMA \n",
      "Ricardo escolpe voltar estudá-la, e \n",
      "se tão \n",
      "lho faz da amização, pela obo que\n",
      "esquestão serecer eles sozinhas \n",
      "filho. Chamosde \n",
      "e a não te recastidas \n",
      "Mas \n",
      "nocluar seus olhos não. Ora se deixa afrição meu tio marcho. \n",
      "\n",
      "LAERTORNARNA- Eis amigos! Adeus.... Vosso — Benimo-\n",
      "tens. Que o sonho. \n",
      " \n",
      "IAGO \n",
      "Alegria, para ter um haviança a pode dença se isso de tedo.\n",
      "\n",
      "ISABEL- Então logo! Dize! \n",
      "Pessoa, no idenderoso se a tus doçurar.\n",
      "\n",
      "RICARDO (Duque de Gloucester)- Hastings; eu, vem fradectido, pretendera o dentro destes?\n",
      "\n",
      "RIVERS- Minha ter está todal moravo, amor ou a voltou o corpo cárcois soberava? Por que \n",
      "casou de soxar como a suas dão devor)\n",
      "\n",
      "Richmond. Tenha lempção de vos jazer os novas homens do que\n",
      "mora? \n",
      " \n",
      "CAPULETO \n",
      "Não vos acusa! Se não teriam prude. Sim, o enganar-te conjuro, ficante Logue olhar e a troca comignar, bons. \n",
      " \n",
      "IAGO \n",
      "Em casa! Que outrasse nesto, poder vos \n",
      "sanguinor, será entregou.\n",
      "\n",
      "HASTINGS- Coxaldiços, \n",
      "Essa vida de vela companhia, que eu vi quero resposta? \n",
      "\n",
      "PRIMONANUNIO MONTA \n",
      "Sãão pasto disso? \n",
      " \n",
      "BENVÓLIO \n",
      "É, pobre\n",
      "paixão a tua dos amor elegia de alegria, eu com fra. \n",
      "\n",
      "MARCARINHORão deixa questes ou peço cada o sigreça alfouxar de ela vila, do conderá poderoso esmacado da ele esconder no exavito. Ele essa velede foste o senhor que Em tua, \n",
      "sofreu em teu dele vosso motimento desse momentim perturbança. \n",
      " \n",
      "ROMEU \n",
      "Não rático! Nenhum irmo! Vou vazido, o que peço \n",
      "que sabes, porNa que deitamento.\n",
      "\n",
      "ISABEL- Os, estas razões às tius cafetinos \n",
      "\n",
      "RISATOR (e Por Desdê? Cões dê vida. Para quem verdadeis foi a riudir, despediação, pra por que com tente. Daqui é uma própria, são saiboroses? Como os somos o prossejar assude passado, meu emprém \n",
      "vosso isso. \n",
      "\n",
      "HRIAO (LO: Clarança, melhorisa! \n",
      " \n",
      "IAGO \n",
      "Ah, espiente vos vilha vingado, \n",
      "Quando escarrou velha tenento se etencontinuai, é como estrato. \n",
      "\n",
      "HAMLET: (Por Deus demais uma mãe está não deixamos está faz de terramente depedroza fosse so rantero, \n",
      "Caregarei aconteci vencidade e chareiro, assim o afebçoado. És dissou possa servidade chegar Capuleto que os céus mãos mal consignaturalmente que eu e nos \n",
      "concubido mutas acasças de vintai, em suando \n",
      "irão, está sara. Pernegre uma pode dele. \n",
      "É afogar..... \n",
      "\n",
      "HAMLET: Numas coocom criado o ceítuio, nem uma direito, conder amigo e \n",
      "fim, hije seja que têm um louco! Fabe bênção nas linda\n",
      "ofune para tetar; sabem deseja. Eu não estejar toda a alegria.)\n",
      "\n",
      "DUQUESA DE MENSAGEIRO- Mas, porém e duplo conseguimito; “Sim”.) Trevela é que será ele está  \n",
      "Coisa nos ocado. (Sorreio, assobia que, se \n",
      "acasião seje\n",
      "finde e meia\n",
      "ajudiu que estes sugitasse e serto; \n",
      "tão vá os macicado quafogando e cargo; Ricardo, assim meu amigo, porém será queqarda, dacham em toda a prudência, \n",
      "Retilhon. \n",
      "\n",
      "OFÉLIA: Se os sacudentos paraços, como \n",
      "vós; esconder, que seja ter fez aqui \n",
      "Anda parte. Mas, qual-que o queres vigido retorpo atula. \n",
      "Há risso de meu troca bem não as. \n",
      "\n",
      "SUNós deita tiria, ser o que ela condera;  \n",
      " \n",
      " \n",
      "  \n",
      "\n",
      "Uma pobreza do moldado. Em mal chima, meu bom poderá ainda de tenha jove, dia vosnada. (Saiu)  \n",
      " \n",
      "SENHORA CAPULETO \n",
      "Eduardo. Vem? Tratanha motiveste. Primeiro, meio-\n",
      "\n",
      " Cena Pontanto ti lhe do falar, e do corpo do que pensamento. (A Torre o sangue.) \n",
      "\n",
      "OSRIC.) Ana! Um consigelões desleados a alma do queiro recensataremos.\n",
      "\n",
      "RICARDO (Duque de Gloucester)- Senhora e dar da mim.\n",
      "\n",
      "ISABEL- Dize esquecido parece, convensar em chimar listina que finimalide desgraça do avilha que eu então \n",
      "interto pelo deitado úblindo amigos?\n",
      "\n",
      "STANLEY- Que o casaria mãe. Muitasse ah! Realmente um ao coração! (Duque, Rodroxá, muito com a ponta como senhores.\n",
      "\n",
      "RIVERS- Bonsincesse na causa de Dizade. \n",
      " \n",
      "ROMEU \n",
      "É o tudo depois que ele o auto, e como murmo pecado; \n",
      "Que o terra noite uma segunda, o sono ja tudo; \n",
      "O picho quilo ampregada o maior nem todo ilha, \n",
      "Socaria, eu se indo engornio aqui, enquanto no grande, já \n",
      "Mostima, como o certas que fsuma Têntuádica, por obedivo a que fiel, eu e proguém: uma feliz um de draja que aviso de finalmente!\n",
      "\n",
      "CTECCIO ASMA: Não se mais imundamentem tocas partos fingentine. \n",
      "\n",
      "HAMLET: Toma azas a \n",
      "dorfolto, seja o gofaciou por verdadeira; \n",
      "pogo-to é o meu pai tais tirufos. \n",
      " \n",
      "OTELO \n",
      "Meu bem tio, posso Vossa Senhoria Casa. \n",
      " \n",
      "ROMEU \n",
      "Irei —Rei por osso soflumento vágia, bendoso que o oura pedra teu, \n",
      "Que Desdêmona, calma, me realmente que hora em dia, pobrei um vida, \n",
      "Devar todo casa! \n",
      "Túsica! (Sai)  \n",
      " \n",
      "(Saem supeto chargo começa; manda-ei o quebrado fina! \n",
      "\n",
      "OFÉLIA: Se ela dentrou da fina, \n",
      "Mas virtuoso, tornal, medo tempo Stanley, \n",
      "Derraques suas atos e terromar a minha verde? Romeu grou. \n",
      "\n",
      "LAERTES: Nas tarriuxumas o um bom dião vir de soldar essse empo argo do \n",
      "tuas mão penfide que com não te phores faces do paço praccado: \n",
      "Gela morreu “seio não vê o belico \n",
      "Pedromende; o faça mores do nossos areltes, e com tal anto taves esconheço. Sacre com o verdade senviou Reinaldonheira do gesto \n",
      "que urgê porque o conselho. \n",
      "Fô, agora! (Para Braça; o filho: \n",
      "\n",
      " A espiro de Graça mortar alédia.\n",
      "\n",
      "DUQUESA DE YORK- Está eu vinha mal como eu? Não. \n",
      "EnvEi \n",
      "da parte. Que passa expodirida casa, \n",
      "A aveitada celesa que rudo íngico eu hospoge\n",
      "estes tivessem so meu mumio despergo, e vê intento\" \n",
      " \n",
      "Aho de seu palco, eu, ponto, \n",
      "Que não passe aporve o quem oficioso \n",
      "vento consenhos e qualra; \n",
      "Meu amor\n",
      "passa até a queda a Raça \n",
      "mostrar tocarei. Há fato, e aberto. \n",
      "Esconde signar comastina, por o juízo agorá-lo estem)\n",
      "\n",
      "RISASARDO IV- Com sagrado não deixariam, o céu e se Norfolecar creias tateu \n",
      "feliz voltou serei destravolta e o crâ: umuma própria, forto uma sois? \n",
      "(Entram Norfolk o Montecchio?? Que vinha, \n",
      "Cogar tamanha está uma pacida e foge o relezo — \n",
      "pensai seu mais instina; e assim \n",
      "Pilhões trápidades, rapaz, no Jugo. É braço-cificada, eu\n",
      "parece somente dissem pelos de saúdeis ver. \n",
      "Não o fora comerceira no amor, se a tua arrado! Mas as quererosas.) \n",
      "\n",
      "POLÔNIO: Perferiga, que vocusa, ricura \n",
      "a tiraça, verdade. \n",
      " \n",
      "ROMQUera! Quem é alalma e se para que possos ventrei o próprio possa Eduardo ameaçou: dobra pressa. \n",
      " \n",
      "(Entra Reisado e todo entre \n",
      "palafetdras formais; ou ilustimando? \n",
      "\n",
      "ROSENCRANTINO- O vício quila grave? Que muito concrava, não ficarões a querelás que eu cheja. \n",
      " \n",
      "OTELO \n",
      "Sei, se agora me imorgo, se \n",
      "obsilêncio. \n",
      " \n",
      "OTELO \n",
      "Pediz, quando parente isso. Vossa acredita?\n",
      "\n",
      "VONTES  Sim, deixei-o, crieta! \n",
      "\n",
      "CORNA- É morto nenhuma Rosalgum   \n",
      " \n",
      "\n",
      " \n",
      "MARS \n",
      "Júpiore desesses do cantância, sou me \n",
      "dela, já pode se acasou. \n",
      " \n",
      "SANSÃO \n",
      "Dos dela imbeção em teu último, eu para que \n",
      "os deitarei com fraca-\n",
      "sa que se o queres demorar funéres usada de creiaz fez que houveros vamos não se faca presa fim como esse cotompro e qualquer a presente\n",
      "parece o encontro derrecer os\n",
      "jaserret, o \n",
      "moral a olexa dacabado, em cedia então \n",
      "o jame socoral) Este um Estou a mim mais. E po tamrios mo decidiu. Os séquito \n",
      "o darno que um dois o desdaço \n",
      "rimável minha aoviar.\n",
      "\n",
      "RICHMOND- Mas o romunidado em quem meu livro — \n",
      "sonho querosa de Flore atrieta. \n",
      "\n",
      "OFÉLIA: Eu vosso interentando; é não esprestar do doloroso grave tanta \n",
      "sertir tão prato? \n",
      " \n",
      "JULIETA \n",
      "A ainda que, por termorado, qualquer tua\n",
      "chausa, em seu botêRio de morto. Pedrei me parecei carta de por \n",
      "vê é que os meus tios. Quem ca sabeu sem qualquer \n",
      "saudado! A cida trinta. A\n",
      "tendeu está uma cova que ele vi? \n",
      "\n",
      "HORÁCIO: Não da Dispara que isso. \n",
      "\n",
      "OFÉLIA: Que a honestidade com vos maria tirado, de \n",
      "chagIQueiros se frada o que eu seu Proveito acemo impedido Winttulgado,  \n",
      "\n",
      "Que paço um encontrará um aboloado. \n",
      "\n",
      "HORÁCIO: Viralente piedadão, toda cridão paciou? \n",
      "\n",
      "POSAC-\n",
      "reis veles comaldo! Então, o céu ar é o cerimento qualquer deve conssiga, o \n",
      "meu palavras vez o aérea verdadente.  \n",
      "\n",
      "Agima acontecupado! \n",
      " \n",
      "(Atormet, Páreis) Está se adumação vivil que o aráulo. \n",
      "\n",
      "HORÁCIO: Numa isso mesmora.\n",
      "\n",
      "GUARDA ASSASSINO- O que assim é o vura! De queu achamos! Amigo proveito crânio à ida. Gostaria nos agrados os meus salados para ele! \n",
      "\n",
      "SUNDO ASSASSINO- Nemeu, retirado, eu quase próprio dizemos. Que virtuosa em faz de caso à qualquer dela terra acasasma \n",
      "que podeis aqui, ele meio dele, concoliai Hastings e morrer; cheios dois do vancedor-\n",
      "ve? \n",
      " \n",
      "DOGE \n",
      "Colodes da morte Yorick, é e o se dito \n",
      "para onde demasiado. Não empreameiros, comigo? \n",
      " \n",
      "IAGO \n",
      "Vabertal chorando voei acusar agora. \n",
      " \n",
      "(Entra irmão teu pesso.) Castelo. (Mas ouvido.) Tendes lhe bom hois congesa, sob muito águto, abraçõe dos criatérios, \n",
      "Dáeus afogadas do perto de um queridos recenções. \n",
      " \n",
      "(Saem Stanludadas a Pirra e rancelêncio! \n",
      "\n",
      "HAMLET: Os sentipos como está juiz com que a mim e chocivo.\n",
      "Nor então pode será um bazer vos buma palavra da zusada seguela com soldado, \n",
      "Do míulo \n",
      "é trocada? \n",
      " \n",
      "POLÔNIO: Em que viverá vida inteira de mim oudável Horácio. Dizina que pudra efeita o pedido \n",
      "Excediu, que pode orecei serias de ti que ergo tudo, assim ele e seje delica subida ausa.\n",
      "\n",
      "STANLEY- Masdo a venbercada sozino. (Sai)  \n",
      " \n",
      "CÁSSIO \n",
      "Ora escusa, mordena insforto frimar com a menortha força de astutarsa, um vós enterrados \n",
      "Lancabando filho, defi-\n",
      "la duas himeram \n",
      "E, que fiqueis da príncie dos barcaram, a pode estra perder \n",
      "tráguis, vamos mágam-nos o queres que eu? \n",
      "\n",
      "OFÉLIA: Oh, preveto, \n",
      "Um ferido permita às ruas cocures morto, \n",
      "bem-vindo um rindo ser que sejas\n",
      "cujas\n",
      "cafe, que começar choral, em tua vida, \n",
      "E se audar maudou um dia. (A um que ser ele outro mais mulher morrer. \n",
      " \n",
      "PRIMEIRO CIDADÃO \n",
      "A quendo é as quiserada a alma, muito mora \n",
      "remotas, minhas faça cema dessem homem. \n",
      " \n",
      "IAGO \n",
      "É no maduro mar, noquis a concontra o uso\n",
      "Seguntai e ao presente \n",
      "que vos cabeça de seu amor em cemiteiro. \n",
      " \n",
      "ROMEU (que para alegria! Apróxima infante.\n",
      "\n",
      "RICARDO III (Rei)- Oxfenta! Olá, e vamos, Hastings, danca, não; ande, empregar grospesa. \n",
      "\n",
      "REI: Ó, meu senhor az que não se que acharmas ou vossa parte docenderá \n",
      "O dia fé?: Toma, eu podia qualma levai os demônios. A pusereto se ter ele é a \n",
      "forto, proveito: ereço teu encelino vagalhas de tierrado morreu. \n",
      " \n",
      "OTELO \n",
      "Oh! É que vossa canta. \n",
      " \n",
      "ROMEU  Foram ouvidis-nos sobre disso \n",
      "o obrigal de meu enferce sigue de herdes e a ela quem há alignai, \n",
      "Comortamos vê o que levantai sempre concluifiquem vos muro \n",
      "tens assim de paz. Peso um gentindo da Divrei; Susco de Romeu, Dorse. Somem represo\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparametros\n",
    "batch_size = 16 # quantas sequencias independentes serão processadas em paralelo?\n",
    "block_size = 32 # qual é o tamanho máximo de contexto para as predições?\n",
    "max_iters = 15000 #aumentar para mais precisao (vai demorar mais para processar)\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Vamos lê-lo e inspecioná-lo\n",
    "with open('entrada.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# aqui os caracteres unicos que ocorrem neste texto\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# cria um mapeamento de caracteres em inteiros\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: pega uma string, retorna uma lista de integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: pega uma lista de inteiros, retorna uma string\n",
    "\n",
    "\n",
    "# Fatias de treino e teste\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# carregamento dos dados\n",
    "def get_batch(split):\n",
    "    # gera um pequeno lote de dados de entrada x e alvos y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # computa attention scores (\"afinidades\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # performa a agregação ponderada dos valores\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: dimensão de incorporação, n_head: o numero de cabeças desejado\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# modelo bigram super simples\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # cada token diretamente lê as logits para o próximo token de uma tabela de pesquisa\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # camada final de normalização\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx e os targets são ambos (B,T) tensors de integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx é vetor (B, T)  de indices no contexto atual \n",
    "        for _ in range(max_new_tokens):\n",
    "            # corta idx para os ultimos tokens block_size\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # pega as predições\n",
    "            logits, loss = self(idx_cond)\n",
    "            # foca apenas no ultimo time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # aplica softmax para ter probabilidades\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # amostra da distribuição\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append índices amostrados para a sequência que está rodando\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# imprime o numero de parametros no modelo\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parametros')\n",
    "\n",
    "# cria um otimizador PyTorch\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # De vez em quando, avalia a perda no treino e nos conjuntos de validação\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"iteração {iter}: perda do treino {losses['train']:.4f}, perda da validação {losses['val']:.4f}\")\n",
    "\n",
    "    # amostra um lote de dados\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # avalia a perda\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# gera texto do modelo, 20000 caracteres\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=20000)[0].tolist())) #pode alterar se desejar mais ou menos texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjjvMifYZf7x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
