{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "## Construindo um GPT\n",
    "\n",
    "Notebook original por Andrej Karpathy mostrado em [Zero To Hero](https://karpathy.ai/zero-to-hero.html).\n",
    "Adaptado em português por Lucas Parteka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "O6medjfRsLD9",
    "outputId": "ceb16386-d6fe-4dad-abac-bbefe1237666"
   },
   "outputs": [],
   "source": [
    "# Vamos lê-lo e inspecioná-lo\n",
    "with open('entrada.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xWI_VyAsN8F",
    "outputId": "1148d388-ffe9-4c4a-a272-2c5056dcb7a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantidade de caracteres no dataset:  690812\n"
     ]
    }
   ],
   "source": [
    "print(\"quantidade de caracteres no dataset: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pg9IvLwhM6qL"
   },
   "source": [
    "Em inglês, o dataset tinha 1115394 caracteres, foram usados como dataset trechos de obras do Shakespeare em português."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c5V0FvqseE0",
    "outputId": "ecc87331-07fc-4cbc-933b-589412beea52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SANSÃO \n",
      "Por minha palavra, Gregório: não devemos levar desaforo para casa. \n",
      " \n",
      "GREGÓRIO \n",
      "É certo; para não ficarmos desaforados. \n",
      " \n",
      "SANSÃO \n",
      "O que quero dizer é que quando eu fico encolerizado puxo logo da \n",
      "espada. \n",
      " \n",
      "GREGÓRIO \n",
      "Sim, mas se quiseres viver, toma cuidado para não ficares \n",
      "encolarinhado. \n",
      " \n",
      "SANSÃO \n",
      "Quando me irritam, eu ataco prontamente. \n",
      " \n",
      "GREGÓRIO  \n",
      " Mas não te irritas prontamente para atacar. \n",
      " \n",
      "SANSÃO \n",
      "Até um cachorro da casa dos Montecchios me deixa irritado. \n",
      " \n",
      "GREGÓRIO \n",
      "Ficar irritado é pôr-se em movimento, e ser valente é estacar. Logo, \n",
      "se ficares irritado, pôr-te-ás a correr. \n",
      " \n",
      "SANSÃO \n",
      "Um cachorro daquela casa me fará fazer pé firme. Encostar-me-ei na \n",
      "parede contra qualquer homem ou rapariga da casa de Montecchio. \n",
      " \n",
      "GREGÓRIO \n",
      "Isso prova que não passas de um escravo fraco, porque o mais fraco \n",
      "é que se encosta à parede. \n",
      " \n",
      "SANSÃO \n",
      "É certo; é por isso que as mulheres, como vasilhas mais fracas, são \n",
      "sempre encostadas à parede. Por isso, afastarei da parede os homens \n",
      "de Montecchio e encostarei nela as raparigas. \n",
      " \n",
      "GREGÓRIO \n",
      "A pendência é entre nossos amos e nós, seus servidores. \n",
      " \n",
      "SANSÃO \n",
      "Pouco importa; hei de revelar-me tirano: depois de lutar com os \n",
      "homens, serei cruel com as raparigas; arranharei a pele de todas as \n",
      "virgens. \n",
      " \n",
      "GREGÓRIO \n",
      "Como! A pele de todas as virgens? \n",
      " \n",
      "SANSÃO \n",
      "Perfeitamente; a pele de todas as virgens, ou sua pele de virgem. \n",
      "Interpreta isso no sentido que quiseres. \n",
      "  \n",
      " GREGÓRIO \n",
      "As que o sentirem, que o interpretem no seu verdadeiro sentido. \n",
      " \n",
      "SANSÃO \n",
      "A mim elas terão de sentir, enquanto eu for capaz de resistir, pois \n",
      "bem sabes que sou um belo pedaço de carne. \n",
      " \n",
      "GREGÓRIO \n",
      "É bom que não sejas peixe; porque se o fosses, não passarias de \n",
      "bacalhau. Vamos; arranca teus instrumentos, que ai vêm vindo dois \n",
      "da casa de Montecchio. \n",
      " \n",
      "(Entram Abraão e Baltasar)  \n",
      " \n",
      "SANSÃO \n",
      "Minha arma nua já está fora; briga tu que eu defenderei tuas costas. \n",
      " \n",
      "GREGÓRIO \n",
      "Como assim? Viras as costas e corres? \n",
      " \n",
      "SANSÃO \n",
      "Não tenhas medo de mim. \n",
      " \n",
      "GREGÓRIO \n",
      "Ora essa! Eu, ter medo de ti? \n",
      " \n",
      "SANSÃO \n",
      "Fiquemos com a lei do nosso lado; eles que principiem. \n",
      " \n",
      "GREGÓRIO \n",
      "Vou franzir o rosto, quando passar por eles; e eles que interpretem \n",
      "isso como entenderem. \n",
      " \n",
      "SANSÃO \n",
      "Não; como ousarem. Vou morder o polegar, o que para eles será \n",
      "desonroso, no caso de não retrucarem. \n",
      "  \n",
      " ABRAÃO \n",
      "É para nós que estais mordendo o polegar, senhor? \n",
      " \n",
      "SANSÃO \n",
      "Estou mordendo o polegar, senhor. \n",
      " \n",
      "ABRAÃO \n",
      "É para nós que mordeis o polegar, senhor? \n",
      " \n",
      "SANSÃO (à parte, a Gregório)  \n",
      "Se eu disser que sim, ficaremos com a lei de nosso lado?  \n",
      " \n",
      "GREGÓRIO (à parte, a Sansão)  \n",
      "Não. \n",
      " \n",
      "SANSÃO \n",
      "Não, senhor; não é para vós que estou mordendo o polegar; mas \n",
      "estou mordendo o polegar, senhor. \n",
      " \n",
      "GREGÓRIO \n",
      "Estais querendo brigar, senhor? \n",
      " \n",
      "ABRAÃO \n",
      "Eu, senhor, querendo brigar? Não, senhor. \n",
      " \n",
      "SANSÃO \n",
      "Porque, se o quiserdes, senhor, estou às vossas ordens; sirvo a um \n",
      "senhor tão bom quanto o vosso. \n",
      " \n",
      "ABRAÃO \n",
      "Porém não melhor. \n",
      " \n",
      "SANSÃO \n",
      "Perfeitamente, senhor. \n",
      " \n",
      "GREGÓRIO (à parte, a Sansão)  \n",
      "Dize “melhor”; aí vem vindo um parente de nosso amo.  \n",
      "  \n",
      "SANSÃO \n",
      "Sim, senhor: melhor. \n",
      " \n",
      "ABRAÃO \n",
      "Estais mentindo. \n",
      " \n",
      "SANSÃO \n",
      "Desembainhai, se fordes homem! Gregório, não te esqueças de teu \n",
      "bote de fundo. \n",
      " \n",
      "(Batem-se. Entra Benvólio)  \n",
      " \n",
      "BENVÓLIO \n",
      "Loucos, parai com isso! Guardai vossas espadas. Não sabeis o que \n",
      "fazeis. \n",
      " \n",
      "(Entra Tebaldo)  \n",
      " \n",
      "TEBALDO \n",
      "Como! Sacas da espada contra uns pobres corçozinhos sem força? \n",
      "Aqui, Benvólio! Vem encarar a morte! \n",
      " \n",
      "BENVÓLIO \n",
      "Procurava separar esta gente. Guarda a espada e me ajuda a acalmá-\n",
      "los. \n",
      " \n",
      "TEBALDO \n",
      "Como! Falas em paz e a espada arrancas? Tão grande ódio tenho a \n",
      "esse termo como ao próprio inferno, a todos os Montecchios e a ti \n",
      "mesmo. Defende-te, covarde! \n",
      " \n",
      "(Batem-se. Entram partidários das duas casas, que se misturam com os combatentes; depois entram cidadãos, armados de paus e partasanas)  \n",
      " \n",
      "CIDADÃOS  \n",
      " Varas e partasanas! Derrubai-os! Descei o pau! Abaixo os Capuletos! \n",
      "Fora os Montecchios! \n",
      " \n",
      "(Entra Capuleto, de roupão de dormir, e a Senhora Capuleto)  \n",
      " \n",
      "CAPULETO \n",
      "Que barulho é esse? Minha espada comprida! Ide buscá-la! Olá! \n",
      " \n",
      "SENHORA CAPULETO \n",
      "Muletas, isso sim: muletas! Por que pedir espada? \n",
      " \n",
      "CAPULETO \n",
      "A espada! digo. Chega o velho Montecchio e brande a lâmina, para \n",
      "fazer-me acinte. \n",
      " \n",
      "(Entram Montecchio e a Senhora Montecchio)  \n",
      " \n",
      "MONTECCHIO \n",
      "Capuleto, Vilão!... Deixai! Tem de se haver comigo.  \n",
      " \n",
      "SENHORA MONTECCHIO \n",
      "Não darás um só passo para o inimigo. \n",
      " \n",
      "(Entra o príncipe com seu séquito)  \n",
      " \n",
      "PRÍNCIPE \n",
      "Súditos revoltosos, inimigos da paz, que profanais vossas espadas \n",
      "no sangue dos vizinhos... Quê! Não ouvem? Olá, senhores, animais \n",
      "selvagens que as chamas apagais de vossa fúria perniciosa na fonte \n",
      "purpurina de vossas próprias veias. Sob ameaça de tortura, jogai das \n",
      "mãos sangrentas as armas para o mal, só, temperadas, e a sentença \n",
      "escutai de vosso príncipe irritado. Três vezes essas lutas civis, \n",
      "nascidas de palavras aéreas, por tua causa, velho Capuleto, por ti, \n",
      "Montecchio, a paz de nossas ruas três vezes perturbaram. Os \n",
      "provectos cidadãos de Verona, despojando-se das vestes graves que \n",
      "tão bem os ornam, nas velhas mãos lanças antigas brandem, vosso \n",
      "ódio enferrujado. Se de novo vierdes a perturbar nossa cidade, pela  \n",
      " quebrada paz dareis as vidas. Por agora, que todos se retirem. Vós, \n",
      "Capuleto, seguireis comigo, e vós Montecchio, à tarde ireis à velha \n",
      "Cidade-Franca, à corte da Justiça, para conhecimento, assim, \n",
      "tomardes de quanto resolvermos sobre o caso. Já! Sob pena de \n",
      "morte, dispersai-vos! \n",
      " \n",
      "(Saem todos, com exceção de Montecchio, a senhora Montecchio e Benvólio)  \n",
      " \n",
      "MONTECCHIO \n",
      "Quem reavivou esta querela antiga? Sobrinho, dize: onde te achavas \n",
      "na hora? \n",
      " \n",
      "BENVÓLIO \n",
      "Antes de eu vir aqui já se encontravam em luta engalfinhados \n",
      "vossos homens e os de vosso inimigo. Tencionando separá-los, \n",
      "saquei de minha espada. Nesse instante, porém, chegou o ardente \n",
      "Tebaldo, espada em punho, que, soprando-me desafios sem conta, \n",
      "não parava de voltear a arma em torno da cabeça, cortando, assim, \n",
      "os ventos que, de nada molestados com isso, só faziam assobiar para \n",
      "ele com desprezo. Enquanto revidávamos os botes e as estocadas, foi \n",
      "chegando gente que aumentou o furor de ambas as partes, até que o \n",
      "duque separasse as partes. \n",
      " \n",
      "SENHORA MONTECCHIO \n",
      "Oh! E onde está Romeu? Sabes, acaso? Alegra-me não vê-lo neste \n",
      "caso. \n",
      " \n",
      "BENVÓLIO \n",
      "Uma hora antes de haver o sol sagrado cortado as franjas de ouro do \n",
      "nascente, senhora, me levou o inquieto espírito a fazer um passeio lá \n",
      "por fora, onde à sombra de um bosque de sicômoros que se estende \n",
      "para oeste da cidade vi vosso filho a andar, que madrugara. Dirigi-\n",
      "me para ele; mas, havendo-me pressentido, esgueirou-se para a \n",
      "sombra mais densa do arvoredo. Eu, que seu íntimo medira pelo \n",
      "meu, que mais procura justamente onde nada achar consegue, \n",
      "demais já sendo para mim eu próprio, meu capricho segui, deixando \n",
      "o dele, e de grado evitei quem me evitava.  \n",
      "  \n",
      "MONTECCHIO \n",
      "Muitas manhãs tem ele sido visto nesse bosque, a aumentar com \n",
      "suas lágrimas o orvalho matutino e acrescentando com seus suspiros \n",
      "fundos novas nuvens às nuvens existentes. Porém logo que \n",
      "principia o sol, que tudo alegra, a abrir no este longínquo o véu \n",
      "sombroso do tálamo da Aurora, da luz foge meu filho atribulado, \n",
      "recolhendo-se à casa, onde se fecha no seu quarto, cerra as janelas, a \n",
      "luz clara expulsa, e noite artificial, assim, prepara. Poderá acabar \n",
      "mal todo esse enliço, se não for afastada a causa disso. \n",
      " \n",
      "BENVÓLIO \n",
      "Meu nobre tio, conheceis a causa? \n",
      " \n",
      "MONTECCHIO \n",
      "Não, nem consigo saber dele nada. \n",
      " \n",
      "BENVÓLIO \n",
      "Acaso já insististes junto dele? \n",
      " \n",
      "MONTECCHIO \n",
      "Não só eu, como alguns amigos nossos. Mas ele confidente de suas \n",
      "próprias inclinações — ignoro até que ponto verdadeiro se mostra \n",
      "— tão discreto consigo mesmo é sempre e tão distante de se deixar \n",
      "sondar e patentear-se como o botão que o verme escuro morde antes \n",
      "que no ar ostente as doces folhas e a formosura à luz do sol dedique. \n",
      "Se a causa eu conhecesse da tristeza deixá-lo-ia curado, isso é \n",
      "certeza. \n",
      " \n",
      "BENVÓLIO \n",
      "Ei-lo que chega. Ponde-vos de lado; há de falar-me ou se mostrar \n",
      "zangado. \n",
      " \n",
      "MONTECCHIO \n",
      "Oh! Quem dera que o ouvisses, em boa hora, em confissão! Vamos, \n",
      "madame, embora. \n",
      " \n",
      "BENVÓLIO \n",
      "Bom dia, primo. \n",
      " \n",
      "ROMEU \n",
      "Como assim! Já é dia? \n",
      " \n",
      "BENVÓLIO \n",
      "São nove horas. \n",
      " \n",
      "ROMEU \n",
      "A dor é um tardo guia. Não foi meu pai que se afastou com pressa? \n",
      " \n",
      "BENVÓLIO \n",
      "Perfeitamente; mas que dor as horas retarda de Romeu? \n",
      " \n",
      "ROMEU \n",
      "Não ter aquilo que, se o tivesse, as deixaria curtas. \n",
      " \n",
      "BENVÓLIO \n",
      "No amor? \n",
      " \n",
      "ROMEU \n",
      "Fora... \n",
      " \n",
      "BENVÓLIO \n",
      "Do amor? \n",
      " \n",
      "ROMEU \n",
      "Fora do amor de quem me traz cativo. \n",
      " \n",
      "BENVÓLIO \n",
      "Ah! que aparência tenha amor tão branda, mas, de fato, seja áspero e \n",
      "tirano \n",
      " \n",
      "ROMEU  \n",
      " Ah! que, apesar da venda, amor consiga descobrir seus caminhos \n",
      "sem fadiga. Onde iremos comer? Oh! que batalha por aqui houve? \n",
      "Mas não contes nada, que já soube de tudo. O ódio dá muito \n",
      "trabalho por aqui; mas mais, o amor. Então, amor brigão! Ó ódio \n",
      "amoroso! És tudo, sim; do nada fostes criado desde o princípio. \n",
      "Leviandade grave, vaidade séria, caos imano e informe de belas \n",
      "aparências, chumbo leve, fumaça luminosa, chama fria, saúde \n",
      "doente, sono sempre esperto, que não é nunca o que é. Eis aí o amor \n",
      "que eu sinto e que me causa apenas dor. Não queres rir? \n",
      " \n",
      "BENVÓLIO \n",
      "Não, primo; chorar quero. \n",
      " \n",
      "ROMEU \n",
      "Por quê, bondoso amigo? \n",
      " \n",
      "BENVÓLIO \n",
      "Por ver que tens opresso o coração. \n",
      " \n",
      "ROMEU \n",
      "Do amor é sempre assim a transgressão. As dores próprias pesam-\n",
      "me no peito; mas agora redobras-lhes o efeito com mostrares as tuas; \n",
      "o tormento que revelaste, ao meu deu mais alento. O amor é dos \n",
      "suspiros a fumaça; puro, é fogo que os olhos ameaça; revolto, um \n",
      "mar de lágrimas de amantes... Que mais será? Loucura temperada, \n",
      "fel ingrato, doçura refinada. Adeus, primo. (Faz menção de retirar-se)  \n",
      " \n",
      "BENVÓLIO \n",
      "Mais calma; irei também; se me deixardes não procedeis bem. \n",
      " \n",
      "ROMEU \n",
      "Ora, já me perdi. Não sou Romeu. Esse está longe. Está nã\n"
     ]
    }
   ],
   "source": [
    "# Vamos ver os primeiros 10000 caracteres\n",
    "print(text[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e-Rbyr8sfM8",
    "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"&'(),-.:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz©ÀÁÂÃÇÉÊÍÓÔÚàáâãçéêíóôõúü—’“”\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "# aqui os caracteres unicos que ocorrem neste texto\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detalhe, apareceram mais caracteres que em ingles que eram 65. A língua portuguesa tem acentuação e letras específicas como \"Ç\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yw1LKNCgwjj1",
    "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 1, 40, 85]\n",
      "E aí\n"
     ]
    }
   ],
   "source": [
    "# cria um mapeamento de caracteres em inteiros\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: pega uma string, retorna uma lista de integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: pega uma lista de inteiros, retorna uma string\n",
    "\n",
    "print(encode(\"E aí\"))\n",
    "print(decode(encode(\"E aí\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJb0OXPwzvqg",
    "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([690812]) torch.int64\n",
      "tensor([32, 14, 27, 32, 70, 28,  1,  0, 29, 54, 57,  1, 52, 48, 53, 47, 40,  1,\n",
      "        55, 40, 51, 40, 61, 57, 40,  8,  1, 20, 57, 44, 46, 86, 57, 48, 54, 11,\n",
      "         1, 53, 81, 54,  1, 43, 44, 61, 44, 52, 54, 58,  1, 51, 44, 61, 40, 57,\n",
      "         1, 43, 44, 58, 40, 45, 54, 57, 54,  1, 55, 40, 57, 40,  1, 42, 40, 58,\n",
      "        40, 10,  1,  0,  1,  0, 20, 31, 18, 20, 75, 31, 22, 28,  1,  0, 72,  1,\n",
      "        42, 44, 57, 59, 54, 12,  1, 55, 40, 57, 40,  1, 53, 81, 54,  1, 45, 48,\n",
      "        42, 40, 57, 52, 54, 58,  1, 43, 44, 58, 40, 45, 54, 57, 40, 43, 54, 58,\n",
      "        10,  1,  0,  1,  0, 32, 14, 27, 32, 70, 28,  1,  0, 28,  1, 56, 60, 44,\n",
      "         1, 56, 60, 44, 57, 54,  1, 43, 48, 65, 44, 57,  1, 83,  1, 56, 60, 44,\n",
      "         1, 56, 60, 40, 53, 43, 54,  1, 44, 60,  1, 45, 48, 42, 54,  1, 44, 53,\n",
      "        42, 54, 51, 44, 57, 48, 65, 40, 43, 54,  1, 55, 60, 63, 54,  1, 51, 54,\n",
      "        46, 54,  1, 43, 40,  1,  0, 44, 58, 55, 40, 43, 40, 10,  1,  0,  1,  0,\n",
      "        20, 31, 18, 20, 75, 31, 22, 28,  1,  0, 32, 48, 52,  8,  1, 52, 40, 58,\n",
      "         1, 58, 44,  1, 56, 60, 48, 58, 44, 57, 44, 58,  1, 61, 48, 61, 44, 57,\n",
      "         8,  1, 59, 54, 52, 40,  1, 42, 60, 48, 43, 40, 43, 54,  1, 55, 40, 57,\n",
      "        40,  1, 53, 81, 54,  1, 45, 48, 42, 40, 57, 44, 58,  1,  0, 44, 53, 42,\n",
      "        54, 51, 40, 57, 48, 53, 47, 40, 43, 54, 10,  1,  0,  1,  0, 32, 14, 27,\n",
      "        32, 70, 28,  1,  0, 30, 60, 40, 53, 43, 54,  1, 52, 44,  1, 48, 57, 57,\n",
      "        48, 59, 40, 52,  8,  1, 44, 60,  1, 40, 59, 40, 42, 54,  1, 55, 57, 54,\n",
      "        53, 59, 40, 52, 44, 53, 59, 44, 10,  1,  0,  1,  0, 20, 31, 18, 20, 75,\n",
      "        31, 22, 28,  1,  1,  0,  1, 26, 40, 58,  1, 53, 81, 54,  1, 59, 44,  1,\n",
      "        48, 57, 57, 48, 59, 40, 58,  1, 55, 57, 54, 53, 59, 40, 52, 44, 53, 59,\n",
      "        44,  1, 55, 40, 57, 40,  1, 40, 59, 40, 42, 40, 57, 10,  1,  0,  1,  0,\n",
      "        32, 14, 27, 32, 70, 28,  1,  0, 14, 59, 83,  1, 60, 52,  1, 42, 40, 42,\n",
      "        47, 54, 57, 57, 54,  1, 43, 40,  1, 42, 40, 58, 40,  1, 43, 54, 58,  1,\n",
      "        26, 54, 53, 59, 44, 42, 42, 47, 48, 54, 58,  1, 52, 44,  1, 43, 44, 48,\n",
      "        63, 40,  1, 48, 57, 57, 48, 59, 40, 43, 54, 10,  1,  0,  1,  0, 20, 31,\n",
      "        18, 20, 75, 31, 22, 28,  1,  0, 19, 48, 42, 40, 57,  1, 48, 57, 57, 48,\n",
      "        59, 40, 43, 54,  1, 83,  1, 55, 87, 57,  9, 58, 44,  1, 44, 52,  1, 52,\n",
      "        54, 61, 48, 52, 44, 53, 59, 54,  8,  1, 44,  1, 58, 44, 57,  1, 61, 40,\n",
      "        51, 44, 53, 59, 44,  1, 83,  1, 44, 58, 59, 40, 42, 40, 57, 10,  1, 25,\n",
      "        54, 46, 54,  8,  1,  0, 58, 44,  1, 45, 48, 42, 40, 57, 44, 58,  1, 48,\n",
      "        57, 57, 48, 59, 40, 43, 54,  8,  1, 55, 87, 57,  9, 59, 44,  9, 79, 58,\n",
      "         1, 40,  1, 42, 54, 57, 57, 44, 57, 10,  1,  0,  1,  0, 32, 14, 27, 32,\n",
      "        70, 28,  1,  0, 34, 52,  1, 42, 40, 42, 47, 54, 57, 57, 54,  1, 43, 40,\n",
      "        56, 60, 44, 51, 40,  1, 42, 40, 58, 40,  1, 52, 44,  1, 45, 40, 57, 79,\n",
      "         1, 45, 40, 65, 44, 57,  1, 55, 83,  1, 45, 48, 57, 52, 44, 10,  1, 18,\n",
      "        53, 42, 54, 58, 59, 40, 57,  9, 52, 44,  9, 44, 48,  1, 53, 40,  1,  0,\n",
      "        55, 40, 57, 44, 43, 44,  1, 42, 54, 53, 59, 57, 40,  1, 56, 60, 40, 51,\n",
      "        56, 60, 44, 57,  1, 47, 54, 52, 44, 52,  1, 54, 60,  1, 57, 40, 55, 40,\n",
      "        57, 48, 46, 40,  1, 43, 40,  1, 42, 40, 58, 40,  1, 43, 44,  1, 26, 54,\n",
      "        53, 59, 44, 42, 42, 47, 48, 54, 10,  1,  0,  1,  0, 20, 31, 18, 20, 75,\n",
      "        31, 22, 28,  1,  0, 22, 58, 58, 54,  1, 55, 57, 54, 61, 40,  1, 56, 60,\n",
      "        44,  1, 53, 81, 54,  1, 55, 40, 58, 58, 40, 58,  1, 43, 44,  1, 60, 52,\n",
      "         1, 44, 58, 42, 57, 40, 61, 54,  1, 45, 57, 40, 42, 54,  8,  1, 55, 54,\n",
      "        57, 56, 60, 44,  1, 54,  1, 52, 40, 48, 58,  1, 45, 57, 40, 42, 54,  1,\n",
      "         0, 83,  1, 56, 60, 44,  1, 58, 44,  1, 44, 53, 42, 54, 58, 59, 40,  1,\n",
      "        78,  1, 55, 40, 57, 44, 43, 44, 10,  1,  0,  1,  0, 32, 14, 27, 32, 70,\n",
      "        28,  1,  0, 72,  1, 42, 44, 57, 59, 54, 12,  1, 83,  1, 55, 54, 57,  1,\n",
      "        48, 58, 58, 54,  1, 56, 60, 44,  1, 40, 58,  1, 52, 60, 51, 47, 44, 57,\n",
      "        44, 58,  8,  1, 42, 54, 52, 54,  1, 61, 40, 58, 48, 51, 47, 40, 58,  1,\n",
      "        52, 40, 48, 58,  1, 45, 57, 40, 42, 40, 58,  8,  1, 58, 81, 54,  1,  0,\n",
      "        58, 44, 52, 55, 57, 44,  1, 44, 53, 42, 54, 58, 59, 40, 43, 40, 58,  1,\n",
      "        78,  1, 55, 40, 57, 44, 43, 44, 10,  1, 29, 54, 57,  1, 48, 58, 58, 54,\n",
      "         8,  1, 40, 45, 40, 58, 59, 40, 57, 44, 48,  1, 43, 40,  1, 55, 40, 57,\n",
      "        44, 43, 44,  1, 54, 58,  1, 47, 54, 52])\n"
     ]
    }
   ],
   "source": [
    "# vamos encodificar todo o dataset de texto e salvar num torch.tensor\n",
    "import torch # Foi usado PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # 1000 caracteres encodificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "f_WIXqxz0lU5"
   },
   "outputs": [],
   "source": [
    "# Vamos dividir os dados entre treino  e teste\n",
    "n = int(0.9*len(data)) # primeiros 90% serão treino, resto validação\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD5Bj8Y6IAD4",
    "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32, 14, 27, 32, 70, 28,  1,  0, 29])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HXDe8vGJCEn",
    "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quando a entrada é tensor([32]) o alvo: 14\n",
      "quando a entrada é tensor([32, 14]) o alvo: 27\n",
      "quando a entrada é tensor([32, 14, 27]) o alvo: 32\n",
      "quando a entrada é tensor([32, 14, 27, 32]) o alvo: 70\n",
      "quando a entrada é tensor([32, 14, 27, 32, 70]) o alvo: 28\n",
      "quando a entrada é tensor([32, 14, 27, 32, 70, 28]) o alvo: 1\n",
      "quando a entrada é tensor([32, 14, 27, 32, 70, 28,  1]) o alvo: 0\n",
      "quando a entrada é tensor([32, 14, 27, 32, 70, 28,  1,  0]) o alvo: 29\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"quando a entrada é {context} o alvo: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3k1Czf7LuA9",
    "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entradas:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1, 55, 40, 57, 59, 44,  1, 54],\n",
      "        [57, 50,  8,  1, 44,  1, 59, 54],\n",
      "        [ 1, 40,  1, 44, 58, 59, 44,  1],\n",
      "        [52,  1, 44, 51, 44,  1,  0, 16]])\n",
      "alvos:\n",
      "torch.Size([4, 8])\n",
      "tensor([[55, 40, 57, 59, 44,  1, 54, 60],\n",
      "        [50,  8,  1, 44,  1, 59, 54, 43],\n",
      "        [40,  1, 44, 58, 59, 44,  1, 51],\n",
      "        [ 1, 44, 51, 44,  1,  0, 16, 54]])\n",
      "----\n",
      "quando a entrada é [1] o alvo: 55\n",
      "quando a entrada é [1, 55] o alvo: 40\n",
      "quando a entrada é [1, 55, 40] o alvo: 57\n",
      "quando a entrada é [1, 55, 40, 57] o alvo: 59\n",
      "quando a entrada é [1, 55, 40, 57, 59] o alvo: 44\n",
      "quando a entrada é [1, 55, 40, 57, 59, 44] o alvo: 1\n",
      "quando a entrada é [1, 55, 40, 57, 59, 44, 1] o alvo: 54\n",
      "quando a entrada é [1, 55, 40, 57, 59, 44, 1, 54] o alvo: 60\n",
      "quando a entrada é [57] o alvo: 50\n",
      "quando a entrada é [57, 50] o alvo: 8\n",
      "quando a entrada é [57, 50, 8] o alvo: 1\n",
      "quando a entrada é [57, 50, 8, 1] o alvo: 44\n",
      "quando a entrada é [57, 50, 8, 1, 44] o alvo: 1\n",
      "quando a entrada é [57, 50, 8, 1, 44, 1] o alvo: 59\n",
      "quando a entrada é [57, 50, 8, 1, 44, 1, 59] o alvo: 54\n",
      "quando a entrada é [57, 50, 8, 1, 44, 1, 59, 54] o alvo: 43\n",
      "quando a entrada é [1] o alvo: 40\n",
      "quando a entrada é [1, 40] o alvo: 1\n",
      "quando a entrada é [1, 40, 1] o alvo: 44\n",
      "quando a entrada é [1, 40, 1, 44] o alvo: 58\n",
      "quando a entrada é [1, 40, 1, 44, 58] o alvo: 59\n",
      "quando a entrada é [1, 40, 1, 44, 58, 59] o alvo: 44\n",
      "quando a entrada é [1, 40, 1, 44, 58, 59, 44] o alvo: 1\n",
      "quando a entrada é [1, 40, 1, 44, 58, 59, 44, 1] o alvo: 51\n",
      "quando a entrada é [52] o alvo: 1\n",
      "quando a entrada é [52, 1] o alvo: 44\n",
      "quando a entrada é [52, 1, 44] o alvo: 51\n",
      "quando a entrada é [52, 1, 44, 51] o alvo: 44\n",
      "quando a entrada é [52, 1, 44, 51, 44] o alvo: 1\n",
      "quando a entrada é [52, 1, 44, 51, 44, 1] o alvo: 0\n",
      "quando a entrada é [52, 1, 44, 51, 44, 1, 0] o alvo: 16\n",
      "quando a entrada é [52, 1, 44, 51, 44, 1, 0, 16] o alvo: 54\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # Quantas sequencias independentes serão processadas em paralelo?\n",
    "block_size = 8 # Qual é o tamanho máximo de contexto para as predições?\n",
    "\n",
    "def get_batch(split):\n",
    "    # gera um pequeno lote de dados de entrada x e alvos y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('entradas:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('alvos:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # tamanho do lote amostral \n",
    "    for t in range(block_size): # dimensão temporal\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"quando a entrada é {context.tolist()} o alvo: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpyyAeIzQjlO",
    "outputId": "a650f8dc-da81-400b-bc59-0a595487fdb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 55, 40, 57, 59, 44,  1, 54],\n",
      "        [57, 50,  8,  1, 44,  1, 59, 54],\n",
      "        [ 1, 40,  1, 44, 58, 59, 44,  1],\n",
      "        [52,  1, 44, 51, 44,  1,  0, 16]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # nossa entrada para o transformador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nql_1ER53oCf",
    "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 95])\n",
      "tensor(4.9322, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "RsIDIQtQ.?GzN ;âBúuÂÊ-v!LtrYZ\"Áe&bÓÚI k!ÃtkHÔ’ôôãIdHigÉ(eãf—àT,©wDy©Yz'âõââà\"b)\n",
      "FÁ—Ô?Í—;pÊL&ÃTWBNeYE\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # cada token diretamente lê fora das logits para o próximo token de uma tabela de pesquisa\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx e os targets são ambos (B,T) tensors de integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx é vetor (B, T)  de indices no contexto atual \n",
    "        for _ in range(max_new_tokens):\n",
    "            # têm as predições\n",
    "            logits, loss = self(idx)\n",
    "            # foca apenas no ultimo time step\n",
    "            logits = logits[:, -1, :] # vira (B, C)\n",
    "            # aplica softmax para ter probabilidades\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # amostra da distribuição\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append índices amostrados para a sequência que está rodando\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "eTyJ8qAaDdiF"
   },
   "outputs": [],
   "source": [
    "# cria um otimizador PyTorch\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs4kI8YdEkQj",
    "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.821751594543457\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100): # aumenta o número de passos para bons resultados...\n",
    "\n",
    "    # amostra um lote de dados\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # avalia a perda\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcVIDWAZEtjN",
    "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Í©pÔÉ ?àzájÍtDühÀzt&'EÂÀÂOoóÚWPqc h(õÚÇÉYÉalcW-õdq.cSÓiilâZnWÊÔáÉTh:OP”GjÔlt!AÂmÃh’Zq tQÍÚIêwÉ;XSõ;HkÉÇUâAmYYà©Ãtêvõ'üLCêôÍHióànçUzú”jBhÓ©ÍnFmüWÉÂTéchXDHÓ“IdD,À—(LSúDCéfIF\n",
      "oüQXrjQRVOÔá'ÔNNô AAçmYÇ:kRéL,h(àXalçiÔ!RgóE“ãPóy\"AõóV“?u(eQ“IÁÂhXgR)ú”Uç &—wxíMóO—zqêCUàYtTVOçiÍíT,á'-ãÁL\n",
      "wç’âbWF&Aõá:wqUz?RU;vÀUzÓÃÂmlé.é©Óêq F Ué©Q'rí©VOsuNLgvüÉk!elamÂmN:?WakHÇ-ôÔq-àMÀWQuSBpRfI\"V'’-Rl-rwjnçUJtêr\n",
      "Q.?IS-ÍbibYÀLgr’SI\n",
      "NVÃupLj?'vz?ÉX©ÁTksRF)ÂWÉOÔHo.ÊÔ! AT\n",
      "D'rq)\n",
      "ôÇséÂNôCw ÁTQz,É ÀQbSaiàBk’E-Dúq.!lÃTãhyGD?é s\"É:â\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XinV8nmAnmKN"
   },
   "source": [
    "## Truque matemático self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tukiH-NbRBhA",
    "outputId": "d981f6d4-ac08-4ec2-8284-82f5fa1e0815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# exemplo ilustrando como multiplicações matriciais podem ser usadas para uma \"agregação baseada nos pesos (weighted aggregation)\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs_E24uRE8kr",
    "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# considere o seguinte exemplo:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, tempo, canais\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "86NuXX0fn7ps"
   },
   "outputs": [],
   "source": [
    "# queremos x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhdOAd6-wXkZ",
    "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# versão 2: usando multiplicação matricial para uma agregação baseada em pesos\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOURrfG-ysoL",
    "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# versão 3: usa Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDarxEWIRMKq",
    "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# versão 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, tempo, canais\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# vamos ver como performa um self-attention de única cabeça\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vT1hdtzXCjgL",
    "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5CvobiQ0pLr"
   },
   "source": [
    "Observação:\n",
    "-  Attention é um **mecanismo de comunicação**. Podem ser visto como nodos em um grafo direcionado apontando um para o outro e agregando informações com uma soma ponderada de todos nodos que apontam para eles, com pesos dependentes dos dados.\n",
    "- Não há noção de espaço. Attention simplesmente age sobre um conjunto de vetores. É por isso que precisamos codificar posicionalmente os tokens.\n",
    "- Cada exemplo entre a dimensão do batch é claramente processado completamente independente e nunca \"conversa\" com os demais\n",
    "- Em um \"encoder\", um bloco de attention apenas deleta a única linha que mascara com `tril`, permitindo todos os tokens se comunicarem. Este bloco aqui é chamado de bloco attention \"decoder\" pois ele tem mascaramento triangular, e é geralmete usado em configurações autoregressivas como modelagem linguística.\n",
    "- \"self-attention\" apenas significa que as chaves e valores são produzidas da mesma fonte que as consultas. Em \"cross-attention\", as consultas ainda são produzidas do x, porém as chaves e valores vem de alguma outra fonte externa (ex. um módulo encoder)\n",
    "- \"Scaled\" attention adicionamente divide `wei` por 1/sqrt(head_size). Isso faz quie quando as entradas Q,K são unidades de variância, wei pode ser unidade de variância também e Softmax permanecerá difuso e não saturará muito. Ilustração abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "4SNbLq5z3oBw"
   },
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nl6I9n9IRTSo",
    "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1tQx7oeRvtc",
    "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLb_odHU3iKM",
    "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0918)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JB82yzt44REI",
    "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mpt8569BB9_f",
    "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # fica muito pico, converge para one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Num7sX9CKOH",
    "outputId": "929ceb78-a639-41d6-aac7-12997b5c93f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variancia\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 de 100 vetores dimensionais\n",
    "x = module(x)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "633T2cmnW1uk",
    "outputId": "7720fa58-0478-4e8a-86a7-502d4cce9443"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean,std de uma característica entre todas entradas do batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LN9cK9BoXCYb",
    "outputId": "6368ece0-600e-417d-8a91-7c1e5d750ba8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.5367e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:].mean(), x[0,:].std() # mean,std of uma única entrada do batch, de suas características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "dRJH6wM_XFfU"
   },
   "outputs": [],
   "source": [
    "# Exemplo de tradução de francês para português:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les réseaux de neurones sont géniaux! <START> as redes neurais são geniais!<END>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcvKeBXoZFOY"
   },
   "source": [
    "### Código finalizado para referência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.213599 M parametros\n",
      "iteração 0: perda do treino 4.7213, perda da validação 4.7206\n",
      "iteração 100: perda do treino 2.7223, perda da validação 2.7430\n",
      "iteração 200: perda do treino 2.5302, perda da validação 2.5608\n",
      "iteração 300: perda do treino 2.4519, perda da validação 2.4609\n",
      "iteração 400: perda do treino 2.3768, perda da validação 2.3939\n",
      "iteração 500: perda do treino 2.3222, perda da validação 2.3299\n",
      "iteração 600: perda do treino 2.2792, perda da validação 2.2819\n",
      "iteração 700: perda do treino 2.2375, perda da validação 2.2374\n",
      "iteração 800: perda do treino 2.2077, perda da validação 2.2171\n",
      "iteração 900: perda do treino 2.1537, perda da validação 2.1607\n",
      "iteração 1000: perda do treino 2.1335, perda da validação 2.1215\n",
      "iteração 1100: perda do treino 2.0962, perda da validação 2.1093\n",
      "iteração 1200: perda do treino 2.0895, perda da validação 2.0677\n",
      "iteração 1300: perda do treino 2.0407, perda da validação 2.0364\n",
      "iteração 1400: perda do treino 2.0268, perda da validação 2.0111\n",
      "iteração 1500: perda do treino 2.0085, perda da validação 2.0024\n",
      "iteração 1600: perda do treino 1.9919, perda da validação 1.9987\n",
      "iteração 1700: perda do treino 1.9684, perda da validação 1.9670\n",
      "iteração 1800: perda do treino 1.9410, perda da validação 1.9522\n",
      "iteração 1900: perda do treino 1.9362, perda da validação 1.9362\n",
      "iteração 2000: perda do treino 1.9231, perda da validação 1.9308\n",
      "iteração 2100: perda do treino 1.8987, perda da validação 1.8964\n",
      "iteração 2200: perda do treino 1.8856, perda da validação 1.8788\n",
      "iteração 2300: perda do treino 1.8757, perda da validação 1.8814\n",
      "iteração 2400: perda do treino 1.8663, perda da validação 1.8639\n",
      "iteração 2500: perda do treino 1.8480, perda da validação 1.8446\n",
      "iteração 2600: perda do treino 1.8279, perda da validação 1.8394\n",
      "iteração 2700: perda do treino 1.8299, perda da validação 1.8255\n",
      "iteração 2800: perda do treino 1.8183, perda da validação 1.8148\n",
      "iteração 2900: perda do treino 1.8089, perda da validação 1.8248\n",
      "iteração 3000: perda do treino 1.7939, perda da validação 1.8031\n",
      "iteração 3100: perda do treino 1.7916, perda da validação 1.7996\n",
      "iteração 3200: perda do treino 1.7795, perda da validação 1.8071\n",
      "iteração 3300: perda do treino 1.7689, perda da validação 1.7844\n",
      "iteração 3400: perda do treino 1.7705, perda da validação 1.7861\n",
      "iteração 3500: perda do treino 1.7658, perda da validação 1.7718\n",
      "iteração 3600: perda do treino 1.7596, perda da validação 1.7682\n",
      "iteração 3700: perda do treino 1.7483, perda da validação 1.7526\n",
      "iteração 3800: perda do treino 1.7293, perda da validação 1.7472\n",
      "iteração 3900: perda do treino 1.7307, perda da validação 1.7468\n",
      "iteração 4000: perda do treino 1.7247, perda da validação 1.7497\n",
      "iteração 4100: perda do treino 1.7307, perda da validação 1.7474\n",
      "iteração 4200: perda do treino 1.7290, perda da validação 1.7366\n",
      "iteração 4300: perda do treino 1.7085, perda da validação 1.7383\n",
      "iteração 4400: perda do treino 1.6906, perda da validação 1.7182\n",
      "iteração 4500: perda do treino 1.6999, perda da validação 1.7317\n",
      "iteração 4600: perda do treino 1.7001, perda da validação 1.7413\n",
      "iteração 4700: perda do treino 1.7027, perda da validação 1.7169\n",
      "iteração 4800: perda do treino 1.6998, perda da validação 1.7074\n",
      "iteração 4900: perda do treino 1.6821, perda da validação 1.7052\n",
      "iteração 4999: perda do treino 1.6781, perda da validação 1.7080\n",
      "\n",
      "nova no mal?\n",
      "\n",
      "MENCRICAIDA- O na não vossaquecidariconte, a falar o malivou e momens de Eduardo.\n",
      "\n",
      "SENHAMLET: Que não quando teus conformos que eu pribir. \n",
      " \n",
      "CONTE- Ingualte meus idicos aqui império, sabato o belou céu, dias do sonhecedo à bolona; nobre assista mortos honem sim: olharidos aí, por como a mim corria! \n",
      "\n",
      "RegoNa! E Wai, mas\n",
      "espadia cuindo alma.\n",
      "Rodeiro, \n",
      "É senhor, a sim eles; for Divereirar tem Ispriminho de entrábio. Por mais de sareme,\n",
      "sabeiro mesma franca de mim, essa sou que \n",
      "amberçam e conjua uração de vossar mal, ela não na mais adeu nobe a Dor Voucura me trobinha, o que pode exos que hor nos a vivaldou. \n",
      " \n",
      "IAGO \n",
      "(que o teu pére\n",
      "sei muura que esmo, assudo \n",
      "ele que perte de do teu condendei enquanto como Jeitarei.\n",
      "\n",
      "RIVERNDO III (Rei Rssclor. E  \n",
      "E apasou odio. Guilha alsa havaro desse-me lobe meu te forme a\n",
      "piderá o desejo. Que o receio ludençamos serialinos deste não me não que conferei. \n",
      " \n",
      "FREI LOURENÇO \n",
      "Assim: E tenha tens seglido. \n",
      " \n",
      "Imento de amarguão quanto quero.\n",
      "\n",
      "FRANTIN: Ses mundo, este que noite? \n",
      "\n",
      "HORÁCIO: Não, via vissa hor pra volúzeiro. \n",
      "Olá o pode os conham.\n",
      "\n",
      "DEURIDANDEN- Essalianto, não suas feito a começam o conferentizão de começo as cedim semprir \n",
      "por pode há douvesto pais de Dizença mesmos e \n",
      "arrança me ulastreza qualerna, estar-se tímioso eitai-amelência? Ele, que a orque\n",
      "na dor ar outrência a deter for em tempo que e eu teu próprio certos o crialseto. Mas que mate a \n",
      "nembo! não à\n",
      "flania que\n",
      "rez Polônios do te em teria. \n",
      "\n",
      "POLÔNIO: O se em desador, que o seus cermelo; o meu põemina, podem, iné, todo luz de diixa de Ficai em.  \n",
      " \n",
      "ROMEU \n",
      "A não elogo, meu planuncar em tudo ser irmã turma ser meu grande. \n",
      "\n",
      "RAINHA: Então; senhor. Há cim encôndio, Julicar agramosar-te que não do edia. \n",
      "Meu vos aqui, a impor à que \n",
      "Em esconvernirioso! \n",
      "  \n",
      "\n",
      " \n",
      "JULIETA \n",
      "E que aconsantez em\n",
      "amanha que desmos a renter Loucais atinha, de\n",
      "Sonde  atrepar-lhe vos a\n",
      "sada por nossa sente unrir a munis honesto de livre, de as \n",
      "pauro que eu sagrente, todos emprosos um qual informa se cubrenda a munhas.\n",
      "E \n",
      "Ricarrançanchado.\n",
      "\n",
      "JULIETA \n",
      "Vou contal impre vós de dicas mundo. Vossa o luanda que razã. o que, pela outro que pois que. Que, equentes seu estistão aqueu a caporiança de poder para mal\n",
      "Que tenho \n",
      "De matar o o\n",
      "convexobre soladou na mais obércelo de Sua fhliment; no concentário \n",
      "Tonhe aquela de modurada dor se depocônio mal na discretimente. É o Deus! conhece-ados que no fambo. (Sae me Senhor porque, ele eu digo. \n",
      " e o avos vlorosa e var para essa agrando que o que eu frogaste assim. \n",
      " \n",
      "BENVÓLIO \n",
      "Dicam-lhe de servino. Em ele digo asso melho reainhona nenhum. O insto; de matrelo. Por \n",
      "é de as baproída, estande luz bomba! Não os em negrei da flegra a de coruja que a seu nenhula \n",
      "Cons espara, mesmos.. \n",
      " \n",
      "IAGO \n",
      "Pois achamos tolos já senha graduíro de fidante, malhão sal dizer-vos dissa, de meus eu contravamos frado \n",
      "Se di o sentram-\n",
      "res à sentrio. \n",
      " \n",
      "ISABEL- Que a horrea.\n",
      "\n",
      "ISABEL- Quistiosos, pois vingado de para rolônio próprias armunha. \n",
      " \n",
      "POLÔNIO \n",
      "E foratave com o que fora d\n",
      "\n",
      "ORTINBIV- Bem-se receus. Páreberterno, o cateu a minha degura, onde todoe. Onde em muito antes falande a teu dalmarã-\n",
      "ne todo à todo quando seu sábira \n",
      "guarda sugra delparianta por e suas cargarei golaridos! \n",
      " \n",
      "PULETES \n",
      "É pai decor, os navas pelos ou tal criego: nã à sem confeito; ambrita, acarca legai que ou.\n",
      "\n",
      "RATCÓLIFE- Elem; o é o em dele repas contros vezes sução mais rei, e pento obramos.\n",
      "\n",
      "ISAGEIRES: Em Riclar-te)  \n",
      "    \n",
      " \n",
      "\n",
      " \n",
      "JAh! A Dua árca da babres seu\n",
      "de uma muito favel: \n",
      " o Lemostre. Quereis e meuum teu olar proso, enforam porte em meu obre de prudenfer a rechedir — não guarda pra almade a tade isso, mal o quando e intfende sago estigo.\n",
      "\n",
      "BERNÚCIO \n",
      "EsCas precime em acéus, com chuvo e que envedenderia Hrichrávos. Na inte Reisck- Para ambravos madia?\n",
      "\n",
      "RICARDO (Duque de Gloucester)- Em apode, apaixar-me aí mim Põem, vílida e consimento \n",
      "presa ter vos dobri a parai; é se eu moto e ferinição de polove, um fá tem me vingandito, meu que nas. Pai conforidos, parência retua \n",
      "retura muis como o\n",
      "contar intendente. E ao mundo e\n",
      "\n",
      "Própria nela. \n",
      "\n",
      "Cluà para o amar jura \n",
      "Teus, amais ele? \n",
      " \n",
      "Imento sua vós por que não destembersa e inmão, então dela linga ele a Seja agraçade do teus aprendido e é outros em ela posto.\n",
      "\n",
      "IRENCRANTZ: Pois, de mas meu\n",
      "espera à tua diva que \n",
      "a vírido no jogargo que pencem do despres suas olha muito aras rescreces e fortes abençage as príncipe sua vida muito?\n",
      "\n",
      "IRA LEIRO \n",
      "Sainai.\n",
      "\n",
      "RICRY- Senhor. (Romi)\n",
      "\n",
      " VEGNDO COVEIRO: É bentes desses precede em\n",
      "casna dor. De mim desposo deu \n",
      "crea muxar a vonte: Não venencar? \n",
      " \n",
      "BRICARDO (Dai)- Vingaro de Dor de Norfériga de seu à volter sonhas aí condes de nossa\n",
      "de ter nogra, duqueA \n",
      "Suja \n",
      " amigos; que oraí a teu páxico bem a ao tudo deito, a. \n",
      " \n",
      "OMA \n",
      "Onde este configo — está vá o vermos Jullarvos de amaçãos. \n",
      "Isso peris uspos aqui tal de príncipe. Quazes penais puja quando o  \n",
      "Reu como de conério. Inglaterralente que em tim, na relende e já inde, \n",
      "Pois longue, muito a mais que javalmente \n",
      "eu noite imigo da\n",
      "DeuSTINbO \n",
      "Mento. \n",
      "  \n",
      "POLÔNIO: Venho a torre\n",
      "da pai, em não de muletece! Enteste de teus vivesses  as então para lembara teu venturado persa.\n",
      "\n",
      "RICARDO (Duqueta a Groge, e conde: sante! \n",
      " profere mas oficar que em \n",
      "apantor um cumi caparede na incomo o na mesma \n",
      "embria da essor guardo Redonde, O Rei Já untir his! Párcham; o todo, manduram-tosas vou teu apreguir\n",
      "quis o que prão mais tão qualer-vos ainda.\n",
      "\n",
      "PULETO\n",
      "\n",
      "São o caso, puntelos agora fazer precade à sapaz ous um aputar assantras mo julágo. Aó rebalhar, assaria que\n",
      "o boi o olho dor reeria, contra te viêndei-lo. É o almo noute de faziu e, que desparei vossajáveram. (Quento ouvis? Oh, “pela, se não nigo Gronhe, \n",
      "— um percago; não o mais então lançament\n",
      "retinfer-ia! \n",
      "\n",
      "HORÁCIO: Na sua addentou senhor, sem tá astistado. Fuintão: me á? \n",
      " \n",
      "RODRIGO \n",
      "Gompólio, com vido ne! Tum ordem, e por\n",
      "despereção, repor tais em noúve, os noites ograçador emeu, não fonte o pouco?\n",
      "\n",
      "ISABO- Venenei à pouxões tem almigor. \n",
      "\n",
      "HORÁCIO: \n",
      "Ah! Derebre o hábir-me não crelho.\n",
      "\n",
      "RIVERCONO- Adeus resposação, nasas bente, anto o messe polugro por está em se não um me tuo anos melhos o\n",
      "minha adugardecar de as me almarca, porque home \n",
      "nenhum de perforbação o suanhas tal castendo o. \n",
      "\n",
      "ÓLAERTES: Bormigo.) Pára diança também! Sã todos em meariano quem o Abonder sangue o já alm-\n",
      "to? \n",
      " \n",
      "OTELO \n",
      "Tomo que o é promes jouro. Em Romeu ferido de dalando-me. \n",
      " \n",
      "ROANTINDO \n",
      "Vou vos amadeiro ao franim ent\n",
      "o satrum bondera lógos, sera cuja obre aparcade a em a revou ar nem meus. \n",
      "\n",
      "ÓLIAAos que te outera em morte sente, meu conconsantumbra tempo é velha dirmito, agora tem teu balar em bem tão \n",
      "conterrido, ah! \n",
      " que eu amarbeç o meu pensurno de como vém me ninum todo me do medinado? \n",
      " \n",
      "(Entrem Romeu e bongando. \n",
      "Derte) Oliderei comoste. \n",
      "O jogo! Senhores espaz, em Honr e inferir a mal coração. Rometinha, o \n",
      "inferno, porvada o sente perter mentão e falavre, palando em nós torno o me oço \n",
      "nífurta de míulo. Vou, \n",
      "Quem onde grava atestesejam do bom melha, se hanho em anta-vir. \n",
      "  \n",
      "PHoi! \n",
      " um o feço mim tormemo meu panto moça volta Fumbla do cornegarem ente. \n",
      "Que todos dele. Ó se dia, me enarecidam-ha anão no Rosto, ou ficarm e nos “nenteira, ao que ela escombar dia! \n",
      "Minúcio, em a tão, as comedas phas, \n",
      "Suas o puleto. Estrado é da vingirmador de polor frazente a nossa faz já notra tal, aí verem alma recupára na hora. \n",
      " \n",
      "OTELO: Sem car o empor há é que conferiu do destame les encrevintão não de vintocado haver-lo \n",
      "Hassim,\n",
      "morte, peus mãos sanguarado. \n",
      " bornes aminha não pena. \n",
      " \n",
      "MERICO \n",
      "Adeus o tra, corte; aos maindeiros — dia, de ser o dela de teu prester ela pergante e capressa. \n",
      "Fiona se alvos que afunçarem e as adnaida. Os inpõesonos e, sem vos ajrmavo-em; a a não neita reterem delou para mulhen sim, aconsidade, que do Ela luher trillia Rei Glançãe a nussivido, e cima só pobeste: morte\n",
      "bondiria; aímis de pazar-me homem a aparem agora a num senhor. Duas grassentes estão ouvido o só ponto terlure metemoedos de seu\n",
      "ceridade, \n",
      "Se elmo esse plogo o furo nenho de tescarnal! E o minha na céu os sofimos \n",
      "Entõo á o tenho caber me que vixão, vou \n",
      "vinde o seu já, inanla na por do dele coisar que eles e teu agoram os deixar atraçam mal não inscrém a do meu te por aposto antecente o me meu freal outra os se \n",
      "vidos \n",
      "Tenho o \n",
      "Vai-lhe à feupcianda as nassires tuas ouras. Então e olen! E que a dalia, tudo \n",
      "meu teme houvem. Díio! Senhoria desse, ca a teu \n",
      "Em amalvento em nunchidado: é, não flores, aceu, sobro e o mais Meu santo ulina, \n",
      "Era morte a ter. Noros mais fazer dante. \n",
      "\n",
      "O morte é coronar, e teu alma entre e verda teu gofico \n",
      "à pela. Vou-te quem enhoros seguento arma feitar-vos que \n",
      "o damar já eleia. Não\n",
      "em anteça a \n",
      "exarterngam de dor? Oh, sonho do \n",
      "Romeu. Alendio, não, senhor meu sonhos pão. \n",
      "Lamo me \n",
      "Assente matornado, e volônio mas são tracúnia. Um tão de da minha termino é a ficardeiro o coração, meu mundo é o que minha dor-\n",
      "que eu\n",
      "ceito teçam as condentar-se e não farores são de \n",
      "carga. \n",
      " \n",
      "Ima de a toutes, bem olos confantam ao metrono? \n",
      "Venem vai este de vende não é escustes par de conjanto, nois falar-me estim. \n",
      "Guém as de tudo nenhos pados. \n",
      " \n",
      "SENHORENT: E quando \n",
      "a via de de sou. por este e pouco mais; que nofiorou, curgar de conuntida disse do\n",
      "meio condenho? \n",
      " \n",
      "DESDURENÇO \n",
      "E cara semi! Que damação apar, da encortado que os codes inciência teu filho tua embéria para outrosa escusa. \n",
      "Não próxina acudam tratados celou \n",
      "por estem ferem muitioso cuurado dimind esse de própria de maninha-\n",
      "reste. Deus amaranha ter dotento a porém, a mida \n",
      "páris antes sorrelosam Bente juxar cormigo.   \n",
      " \n",
      "Rete por de escervido, coisa, \n",
      "sivlo da colônia o luzê-lo, não pode que se como \n",
      "abreantivo o espo sangua ha santo, de \n",
      "estão pra mele. \n",
      "\n",
      "REI: A Jaca e má bom bem em qual. \n",
      "  \n",
      "BENVÓLIO \n",
      "Elce só o, tua pode adugor nesbo. \n",
      "Por muito-me infloria. Fogo estrado. \n",
      " \n",
      "JULIETA \n",
      "Camente Strugamos, no bondes outros vento Humlmerbo. E não vos penas aos distos que em está profundo Capente? Por alalmente \n",
      "E que na Nalavros ne que eu ficante dar de esse e \n",
      "grandes na nestirável. Que esperavo, mais vossas nas \n",
      "servela. \n",
      "\n",
      "HAMLET: Nonjeito, tão, muitam do mulho de \n",
      "prea crudime morte presa que se pernarei, é propo; \n",
      "(Entre Cássio de Raverem. Um a bani belos do noite. Do Vou Raurono, Etertou de tudo, o Deus \n",
      "granteza, que o que prato; repeijo e camigo, é nuncas que eu novente?  \n",
      " \n",
      "\n",
      "OSENÓLIO \n",
      "Pois viu condemondique o espavantova. \n",
      " \n",
      "POLÔNIO: Deseste-me que vossas livfessas recunado à\n",
      "artecas me própria Se ser\n",
      "vou vinda Deuxar-te. \n",
      "\n",
      "REI LOURENÇO \n",
      "Qual diz que o deu chegos que é tem amigo de a. A recou dizo; me pouvo aí de tua as \n",
      "arreminhar. Usa a víi, rem hons isso asso conhece. Bou mais um lonjo, o o meu sogo, de certo: o mandar o estermem\n",
      "só nos felida dor pra pache, me aqui para tem a antes meu? loucunhado bossença-o geu ou já froma ela as costaga do, sob recider vosso é tampo o\n",
      "jique dia hei o teus prestou precimenos de perde quanta-do venho fam atrato mansundeiro? Em mim amor a quantar-me\n",
      "mu — Meu\n",
      "chamadar de assim seu obredes. Vou sou nátil que a viveria. \n",
      " \n",
      "BENVÚCIO \n",
      "Quem ohar, em a penuncia me o\n",
      "fai pendivo chipdido? \n",
      " \n",
      "JULIETA \n",
      "Bomenber eu, meu gal. Sua não brego trado o mim, amh de Dorquançar)\n",
      "\n",
      "RICARDO III (Rei)- Uma agraendenar. Eu surtando dervaçã. \n",
      " \n",
      "PRIMEIRO COVOURENÇO \n",
      "Bicarquece com o minha meu olher \n",
      "te, estão o revereginante o que teu provado pouver nos mau\n",
      "alma o de tnorna hei?\n",
      "\n",
      "RIVESENBEL- O meu.  inhao! Não monhas tal, Bonbe) Rei Asse Isa! Outua \n",
      "sonhes diridades \n",
      "emalançarminado nem ficarsugados da livra o \n",
      "pudida,\n",
      "\n",
      "REI (Duque\n",
      "Glairo, I tá vos sua beixa eu dembora, nouve-me \n",
      "Que tranquazes o nõem louce sangue seu exerido. \n",
      "Pedio livido e siá suspões ofamaçõo-me. \n",
      " \n",
      "\n",
      "OTELO \n",
      "À obílaremo ruús torremunha os moçãos chama da encondireiro. O loutas ordeiros um \n",
      "parte contz um estão para de morte nós \n",
      "Parepamentursar-tes do luar carmi. Ah lugrente a vossa a prólia. \n",
      "\n",
      "HAMLET: Sem, da saxenção\n",
      "me ama vi, ter por mente a teu senhor. \n",
      "Pois como míreio aparte. Então força que tal \n",
      "íeremos demeu. \n",
      " \n",
      "POLÔNIO: E o se \n",
      "E não dianto — de meana mim am olha tem falavraeza que do me teredo pode, seu outrou perquece o hançado, dolinar-no- Ricausinar em forção. \n",
      " \n",
      "OTELO \n",
      "Em o tem perniu orde, seu senhor, filha assim as apenas tormadoreira, olhander que que sodo que o Cante. Eu guei-me dia deplete eu..... \n",
      " \n",
      "JULIETA \n",
      "Coram ao o\n",
      "sentravar de molhior da pinte de tudos me nos do funfro que contra rainha-a \n",
      "vibal tempo e a suplipida: meu conrido, sangue a mim, ainhais, \n",
      "Venenho deso amigo. \n",
      " \n",
      "FREI LOURENÇO \n",
      "E pecida pois é agora cavalheiramonanham. Já nosses conde o\n",
      "me trei? \n",
      " \n",
      " \n",
      "JULIETA \n",
      "COlhe en estãos irmãos teus próprios vim boa “Oino e o quívelo) Que consém, não do caloa, calmundo, senhora, meu \n",
      "infem meu calou estemponhante com do java todas nos abeça sim eu plado. Umo em combado todo sangue dia nemos quave amar do seu que é de seu bendoal, Ricardo, encuberam sas Guas que eu oté tela embém.\n",
      "\n",
      "ANCONTEM- Não, desperta uma de embreque chanta, \n",
      "A da morte, pero ante for nenclite a poder seres e perdem. Sol, eu parendo antragua.. \n",
      " \n",
      "Duque de conhecistam? São vavas as e víngo de alem. que rande um confantundo a ferros has\n",
      "gramente eu assim dispo atrível-te supa,  ser abento fora e a nonginar-me profia de, matésforntar-\n",
      "so nisso ser o numplheiro de morrércedendo, a porquança, para mede tenham e verdar,\n",
      "serã: d. Parte o vento. Vou tente a grante teu e leito. \n",
      " \n",
      "(Entram Párcheiro-voros e que em floga do emtra irmão ao \n",
      "Umonqüem o arrei” mim enchácia; pode. Nóondio a trrangida, seu convendena, ao meu ódio desejüemomem crunca a ter dentrever no amor. Venha mam na mozinha, ar, de insulste. \n",
      "E, por olosos pode  no digimavos seu irre eu não é orreguradeir.)\n",
      "\n",
      "RIVERCO: Meu pouco-me a cobeção me e supos emos. Tura, ou à mi fanto, ao quem um luz atro me seu o \n",
      "com deus ao Beruríro de uma tema \n",
      "tão brado eu o respensaparei.. \n",
      "Verigo! \n",
      " \n",
      "HORÁCIO: Em o faz lavado dia manis cavo morto.\n",
      "\n",
      "BUCKINGHAM- Cecemo. Mas emplemo. \n",
      " \n",
      "BUCKINDO (Saer do Ber, que então. Pois que impacarção o nova que teste boai-luete luz de périvo altar o que o que eu guantiço mutou da me própria dele os crimecinosara \n",
      "esparece prendime cumpado que incusarem virivos prainegar de da prenou a oo de vir; senho, nenhuma livnea. De revinanIo Rainh, Hamrvente, pode, perguardo esporos!\n",
      "\n",
      "RICARDO (Duquei-o) Estam \n",
      "E a aóbsia, a mas a consimentos)\n",
      "\n",
      "RYRn; Está intoguém pai! \n",
      "\n",
      "RICARDO II (Das São respoupo, estambalho, o converngando altisa \n",
      "assim apropre o poucarte pena, o crivia procusa joga-a,\n",
      "Saldereiraça de agor ana a muito a \n",
      "alma isparecito sever que vos osso elet. \n",
      "\n",
      "PORINHA: Tudo, todo está mua talgo e a deste.\n",
      "\n",
      "JULIETA \n",
      "Ela perena, vou que alava a caminha. O língra de nova de Gloucester)\n",
      "\n",
      "ISABEL- O sanam páluque, daqueu. (Sam Tour-laria \n",
      "Norpeita feliz o tambéria morce,\n",
      "para da você o príncuro até muda para e\n",
      "vima o ou? \n",
      "Esperadona julhchas da havos desas pensas minha apinhar teus \n",
      "cassarendo de lhe isso. Euvero amor sais de melhor te há se de na \n",
      "soltras a\n",
      "antes de intermãos. Nortau. Rei a bela, onde da ter \n",
      "Ronde, atoço, corau o inde meito no sua nobressão, de não é clunde o \n",
      "talal, por diançam este umnsugar perguino é, inimmação Chup turra \n",
      "do nanguééro o pelante. \n",
      "\n",
      "OTELO Ouveria-pre triste músvermo\n",
      "Ricaria delado! (Desai-me divo o Retir.) \n",
      "\n",
      "GURENDO \n",
      "O que teria essa e \n",
      "\n",
      "RODRIGO \n",
      "Qualo fim, que a tanto. \n",
      " \n",
      "Imatão por me mail.. Não, então batar nuúgim o nos oltras \n",
      "seguência! \n",
      "Céu a repugo na vós onde a rovagilar, todo alá. \n",
      "\n",
      "REI LOURENÇO \n",
      "O velha que são manha nobre já e que absor \n",
      "O da quesse presentes para o monhece aprou-\n",
      "de \n",
      "convorem menhumento junto de meu recormo e morte ente do \n",
      "andeconda. \n",
      " \n",
      "REINALTES: Eu para dizen qualhes. Isastura a deperarem o me o amos nofinendo-\n",
      "me da hora anda malheira de tuída veda mais retar ela, a\n",
      "almundito, e e o coração, seu vor em condívei  \n",
      "\n",
      " Já tenho vinta se que o é te está eu padade que de\n",
      "muu amigo.\n",
      "\n",
      "RIVE- Ah, compo, profinde? \n",
      "\n",
      "STAPULETO \n",
      "Muicheiro, quandos o volgondinado agrancariar da da mersa e doce de a parque me presutão que me famplido,\n",
      "perceta, entrantam escutaremo, te cunio bido recontudo, dé ama, prumim assas não. \n",
      " \n",
      "IAGO \n",
      "A qual o fulsado um pra fratédicante de conhica assim um com que ele mor a do mai! \n",
      "\n",
      "BENVÓLIO \n",
      "A me eu que como jam fará o nova o. Primos as cavolosar? O me onde, o cuvo forem \n",
      "via. \n",
      "(Ente-me Hamlitar-am atir e o é sepular estada mor astalos fruncimento armi dispente.  \n",
      "Semento própria do confinde. Canda encondia o solável. Que hor é que cessa lação te \n",
      "ol um sevido do imei o grandero suas fizeras da cuidas corredeiro, amigor. Que que chece em pedernosáveis uma outra \n",
      "izade. \n",
      " \n",
      "\n",
      "HORÁCIO:. O vou bolo, está sangua tira vendeis e mais dizigos, seus. \n",
      " \n",
      "HAMLET: Não hora é própla parte sua fruvargume\n",
      "nestes almanosos. Pelo que um am de que facor? Dizendo, que a o imbrarrou em céus vos na enfalmadoreira-\n",
      "cida. Ouve nenTe, manlhe a na afarda aquelento.\n",
      "\n",
      "ISABEL- Oi-me Sangralma, Para Rei; que ele coragência nenhum à ente aquezã. \n",
      "Rate cergo) Unoi em\n",
      "timitida, fante o alodecam. É que agüer)- Servol não em tau voca mal todos pestas na pergumento ses formos sonha \n",
      "gua tua senhos em bestados lugars \n",
      "Horás\n",
      "inidado. \n",
      " \n",
      "SEGURENDO ISE LORANTO: De e devoce está invernitam as matritai essa guas dia beder no cardeção de iinqueto estará méuulo, de todo, me que devedo esseste Ratdando)  \n",
      " \n",
      "RODRIDO com da mesma \n",
      "Que apor ara. Um em porfel e por brida os destimegados seles pergosos pode? HajemorfHlícioso, ao ide estuúdo. \n",
      " \n",
      "IAGO \n",
      "Oo perdido.\n",
      "Não não chonte,\n",
      "ama, reste! Então o em depinou do Rirmeu curgar na Tandeia e \n",
      "Dá nos víriosos de nossa amudalheirado que contador, que temos, nis, que me mesma \n",
      "tim, que am a qualde do emprendiro, ódio contrem que Que eu serres agras ser intermiros? Vou \n",
      "pirfesse, elo por vossas mostros dois o argora \n",
      "Fora de prova funaldens; acande, \n",
      "tento lembnou; o formundo de o que \n",
      "eu, falar samos falsante te, pois damas prestes de Hoglet. O \n",
      "Em Ida. Nade exa parocurento, desse as pra \n",
      "E verdades se da aparem e seu sereiro! Pois falanto gerososam a em alsim de formicidade. Tluta ver parrecer o virtal e o mal conte. Mundo Riconde. De Guorndo-\n",
      "pretes vermos não nocuno, ter \n",
      "aí-te-te Pesso orrevos nanguém, como formos ao que teu maldinado; sentemos a guarro\n",
      "fildo. Então me favor os olábIs o meu for momens, anto \n",
      " amigor a concular e o vento oxtarelor eu párida e razão é que subcisses pode-me de lugrosar ou não de dar da Ricardo, se ata. Ei-la dor vos enfirmadar\n",
      "partes \n",
      "Vidade. Ao então\n",
      "ser andecer do \n",
      "pude ouverso. Por, sou pois pelos, ao mal retraz, eu e à otenhece: o é de ajua me deparece e; namos \n",
      "A pode que eu iuso dia de tenouna regra peçanheiros de foura, por o nevido que contem torna o mexincutando esse usso esparente; que seu melá \n",
      "amigo. \n",
      "E esse minha. \n",
      "\n",
      "BALTARDO (Duque tudo do Rebonado em vardeal teme desres olho senhor, meu\n",
      "serir estrode sonheiro, a nobrendo-meu, em para dizer \n",
      "ReI que eu ouviso. \n",
      " \n",
      "AMA \n",
      "Não que eles nobos outraste mim?\n",
      "\n",
      "HASTINGS A que um morréde. Emora recês. \n",
      " \n",
      "SENA \n",
      "FREI LOURENÇO  (Rodes seus pontori que \n",
      "sangúoios. \n",
      " \n",
      "AMA \n",
      "Tonho insfróeste a senhor (Salante, aqui à Salgrair. \n",
      "Jouta ao te há a maio \n",
      "amigor. É pudia sogrante, apara são na do diecester na nossos perdim. Efande qual, conteguo na antia. Deus um grainhos cortesãos,\n",
      "o atímundo há grocar e Rulcura então Ricargado\n",
      "um em Rosencro)   \n",
      "  \n",
      "FREI LOURENÇO \n",
      "Tocutalmento socunca conto: gue eu e recándo louco dinou sembraícia. Quano por de prego culco.\n",
      "\n",
      "IVERDO: O parsas a conda. O marcam ou bom coração, a périfor você Da; e pouca malhei dianes da mulher. \n",
      "Ratcliste, diz? \n",
      " \n",
      "SENGRA MASTINGO \n",
      "Comi-voro,\n",
      "minha que por o que primedia. \n",
      " \n",
      "FREI LOURENÇO \n",
      "Romeu pesa não casa me rei, por direito. \n",
      "Não, penos seu, como por\n",
      "me não diluz, o destro apulível e voclurás tudo onhum ofenço, o Horável olágora; rapunha então? \n",
      " \n",
      "GREI LOURENÇO \n",
      "Que me tão. \n",
      " \n",
      "IAGO \n",
      "É outro eleste. \n",
      "Filamento em fingéncia mesmo impara assim vento. Voa-de.\n",
      "\n",
      "BUCKINGS- Perim\n",
      "oucida. \n",
      " \n",
      "Esperantar. Vos agento mesmago. O mim a, mas esse voços ao tudo \n",
      "eu adente. E ao porém sedo mente se\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparametros\n",
    "batch_size = 16 # quantas sequencias independentes serão processadas em paralelo?\n",
    "block_size = 32 # qual é o tamanho máximo de contexto para as predições?\n",
    "max_iters = 30000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Vamos lê-lo e inspecioná-lo\n",
    "with open('entrada.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# aqui os caracteres unicos que ocorrem neste texto\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# cria um mapeamento de caracteres em inteiros\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: pega uma string, retorna uma lista de integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: pega uma lista de inteiros, retorna uma string\n",
    "\n",
    "\n",
    "# Fatias de treino e teste\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# carregamento dos dados\n",
    "def get_batch(split):\n",
    "    # gera um pequeno lote de dados de entrada x e alvos y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # computa attention scores (\"afinidades\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # performa a agregação ponderada dos valores\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: dimensão de incorporação, n_head: o numero de cabeças desejado\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# modelo bigram super simples\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # cada token diretamente lê as logits para o próximo token de uma tabela de pesquisa\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # camada final de normalização\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx e os targets são ambos (B,T) tensors de integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx é vetor (B, T)  de indices no contexto atual \n",
    "        for _ in range(max_new_tokens):\n",
    "            # corta idx para os ultimos tokens block_size\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # pega as predições\n",
    "            logits, loss = self(idx_cond)\n",
    "            # foca apenas no ultimo time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # aplica softmax para ter probabilidades\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # amostra da distribuição\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append índices amostrados para a sequência que está rodando\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# imprime o numero de parametros no modelo\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parametros')\n",
    "\n",
    "# cria um otimizador PyTorch\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # De vez em quando, avalia a perda no treino e nos conjuntos de validação\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"iteração {iter}: perda do treino {losses['train']:.4f}, perda da validação {losses['val']:.4f}\")\n",
    "\n",
    "    # amostra um lote de dados\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # avalia a perda\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# gera texto do modelo\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=20000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjjvMifYZf7x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
